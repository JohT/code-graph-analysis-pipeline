{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0eabc4",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Isolation Forest - Manual Exploration\n",
    "\n",
    "This notebook demonstrates anomaly detection with Isolation Forest for static code analysis gathered by using jQAssistant and Neo4j. Detecting anomalies in the data can be useful for identifying potential issues or areas for improvement in the codebase. To explain the results, we use SHAP (SHapley Additive exPlanations) to provide insights into the feature importances and how they contribute to the anomaly scores.\n",
    "\n",
    "<br>  \n",
    "\n",
    "### References\n",
    "- [jqassistant](https://jqassistant.org)\n",
    "- [Neo4j Python Driver](https://neo4j.com/docs/api/python-driver/current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1e4fb",
   "metadata": {},
   "source": [
    "## Features overview\n",
    "\n",
    "| **Feature**                      | **Type**           | **What it Measures**                        | **Why Itâ€™s Useful**                         |\n",
    "| -------------------------------- | ------------------ | ------------------------------------------- | ------------------------------------------- |\n",
    "| `PageRank`                       | Centrality         | Popularity / referenced code                | High = many dependents                      |\n",
    "| `ArticleRank`                    | Centrality         | How much the code depends on others         | High = high dependency                      |\n",
    "| `PageRank - ArticleRank`         | Relative Rank      | Role inversion / architectural layering     | Highlights mismatches                       |\n",
    "| `Betweenness Centrality`         | Centrality         | Bridge or control nodes                     | High = structural chokepoints               |\n",
    "| `Local Clustering Coefficient`   | Structural         | Local cohesion / modularity                 | Low = isolated node in a clique-like region |\n",
    "| `Degree` (Total and In)         | Structural         | Connectivity                                | Raw values may dominate                     |\n",
    "| `Node Embedding` (PCA reduced)   | Latent             | Structural and semantic similarity          | Captures latent position in graph           |\n",
    "| `Normalized Cluster Distance`    | Geometric          | Relative to cluster radius                  | Adds context to position                    |\n",
    "| `1.0 - HDBSCAN membership probability` | Cluster Confidence | How confidently HDBSCAN clustered this node, 1-x inverted | High score = likely anomaly                   |\n",
    "| `Average Cluster Radius`          | Cluster Context    | How tight or spread out the cluster is         | Highly spread clusters may be a less meaningful one   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy.typing as numpy_typing\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from optuna.importance import get_param_importances, MeanDecreaseImpurityImportanceEvaluator\n",
    "from optuna.trial import TrialState\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna import Study, create_study\n",
    "\n",
    "import shap # Explainable AI tool\n",
    "\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell uses the build-in %html \"magic\" to override the CSS style for tables to a much smaller size.\n",
    "#This is especially needed for PDF export of tables with multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS style for smaller dataframe tables. */\n",
    ".dataframe th {\n",
    "    font-size: 8px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 8px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07319282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Colormap\n",
    "# main_color_map = 'nipy_spectral'\n",
    "main_color_map = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version as python_version\n",
    "print('Python version: {}'.format(python_version))\n",
    "\n",
    "from numpy import __version__ as numpy_version\n",
    "print('numpy version: {}'.format(numpy_version))\n",
    "\n",
    "from pandas import __version__ as pandas_version\n",
    "print('pandas version: {}'.format(pandas_version))\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('sklearn version: {}'.format(sklearn_version))\n",
    "\n",
    "from matplotlib import __version__ as matplotlib_version\n",
    "print('matplotlib version: {}'.format(matplotlib_version))\n",
    "\n",
    "from neo4j import __version__ as neo4j_version\n",
    "print('neo4j version: {}'.format(neo4j_version))\n",
    "\n",
    "from optuna import __version__ as optuna_version\n",
    "print('optuna version: {}'.format(optuna_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the environment variable \"NEO4J_INITIAL_PASSWORD\" in your shell \n",
    "# before starting jupyter notebook to provide the password for the user \"neo4j\". \n",
    "# It is not recommended to hardcode the password into jupyter notebook for security reasons.\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\n",
    "    uri=\"bolt://localhost:7687\", \n",
    "    auth=(\"neo4j\", os.environ.get(\"NEO4J_INITIAL_PASSWORD\"))\n",
    ")\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cypher_to_data_frame(query: typing.LiteralString, parameters: typing.Optional[typing.Dict[str, typing.Any]] = None):\n",
    "    records, summary, keys = driver.execute_query(query, parameters_=parameters)\n",
    "    return pd.DataFrame([record.values() for record in records], columns=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7656bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotation_style: dict = {\n",
    "    'textcoords': 'offset points',\n",
    "    'arrowprops': dict(arrowstyle='->', color='black', alpha=0.3),\n",
    "    'fontsize': 6,\n",
    "    'backgroundcolor': 'white',\n",
    "    'bbox': dict(boxstyle='round,pad=0.4',\n",
    "                    edgecolor='silver',\n",
    "                    facecolor='whitesmoke',\n",
    "                    alpha=1\n",
    "                )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dde866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Settings\n",
    "\n",
    "# -> Feature \"outgoingDependencies\" is left in even though it is highly correlated with the feature \"degree\" and isn't important for anomaly decisions according to SHAP. Still, the F1 score of the model is improved by it. \n",
    "# -> Feature \"articleRank\" is left in even though it is highly correlated with the feature \"incomingDependencies\". It is important for anomaly decisions according to SHAP and improves the F1 score. \n",
    "features_for_visualization_excluded_from_training: typing.List[str] = [\n",
    "    'codeUnitName',\n",
    "    'shortCodeUnitName',\n",
    "    'projectName',\n",
    "    'clusterLabel',\n",
    "    'clusterSize',\n",
    "    'clusterMedoid',\n",
    "    'clusterNoise', # highly correlated with \"clusterApproximateOutlierScore\". doesn't improve F1 score of proxy model.\n",
    "    'embeddingVisualizationX',\n",
    "    'embeddingVisualizationY',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68aa20",
   "metadata": {},
   "source": [
    "## 1. Java Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927388f",
   "metadata": {},
   "source": [
    "### 1.1 Query Features\n",
    "\n",
    "Query all features that are relevant for anomaly detection. Some of them come from precalculated clustering (HDBSCAN), node embeddings (Fast Random Projection), community detection algorithms (Leiden, Local Clustering Coefficient), centrality algorithms (Page Rank, Article Rank, Betweenness) and classical metrics like the in-/out-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Package)\n",
    "    WHERE (codeUnit.incomingDependencies IS NOT NULL OR codeUnit.outgoingDependencies IS NOT NULL)\n",
    "      AND codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "      AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationX       IS NOT NULL\n",
    "      AND codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationY       IS NOT NULL\n",
    "     WITH * \n",
    "         ,coalesce(codeUnit.incomingDependencies, 0) AS incomingDependencies\n",
    "         ,coalesce(codeUnit.outgoingDependencies, 0) AS outgoingDependencies\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                                  AS codeUnitName\n",
    "        ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "        ,artifact.name                                                 AS projectName\n",
    "        ,incomingDependencies\n",
    "        ,outgoingDependencies\n",
    "        ,incomingDependencies + outgoingDependencies                   AS degree\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "        ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "        ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "        ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "        ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "        ,codeUnit.communityLocalClusteringCoefficient                  AS locallusteringCoefficient\n",
    "        ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "        ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "        ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "        ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "        ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "        ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "        ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationX          AS embeddingVisualizationX\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationY          AS embeddingVisualizationY\n",
    "\"\"\"\n",
    "\n",
    "java_package_anomaly_detection_features = query_cypher_to_data_frame(java_package_anomaly_detection_features_query)\n",
    "java_package_features_to_standardize = java_package_anomaly_detection_features.columns.drop(features_for_visualization_excluded_from_training + ['embedding']).to_list()\n",
    "\n",
    "display(java_package_anomaly_detection_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f1415",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "Prepare the data by standardizing numeric fields and reducing the dimensionality of the node embeddings to not dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(features: pd.DataFrame) -> None:\n",
    "    if features.empty:\n",
    "        print(\"Data Validation Info: No data\")\n",
    "\n",
    "    if features.isnull().values.any():\n",
    "        raise RuntimeError(\"Data Validation Error: Some values are null. Fix the wrong values or filter them out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between features for debugging, troubleshooting, and detailed analysis.\n",
    "\n",
    "def plot_feature_correlation_matrix(features: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plots the correlation matrix of the features in the DataFrame.\n",
    "    \n",
    "    :param java_package_anomaly_detection_features: DataFrame containing the features.\n",
    "    :param java_package_features_to_standardize: List of feature names to include in the correlation matrix.\n",
    "    \"\"\"\n",
    "    correlation_matrix = features.corr()\n",
    "\n",
    "    figure, axis = plot.subplots(figsize=(8, 6))\n",
    "    color_axis = axis.matshow(correlation_matrix, cmap=\"coolwarm\")\n",
    "    figure.colorbar(color_axis)\n",
    "    axis.set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axis.set_yticks(range(len(correlation_matrix.index)))\n",
    "    axis.set_xticklabels(correlation_matrix.columns, rotation=90)\n",
    "    axis.set_yticklabels(correlation_matrix.index)\n",
    "    for (i, j), correlation_value in np.ndenumerate(correlation_matrix.values):\n",
    "        axis.text(j, i, f\"{correlation_value:.2f}\", ha='center', va='center', color='black', bbox=dict(facecolor='white', alpha=0.3, edgecolor='none'))\n",
    "    plot.title(\"Feature Correlation Matrix (excluding embeddings)\", fontsize=10)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a289a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_correlation_matrix(java_package_anomaly_detection_features[java_package_features_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(features: pd.DataFrame, feature_list: list[str]) -> numpy_typing.NDArray:\n",
    "    features_to_scale = features[feature_list]\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_standardized = standardize_features(java_package_anomaly_detection_features, java_package_features_to_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality_of_node_embeddings(\n",
    "        features: pd.DataFrame, \n",
    "        min_dimensions: int = 20, \n",
    "        max_dimensions: int = 40, \n",
    "        target_variance: float = 0.90,\n",
    "        embedding_column_name: str = 'embedding'\n",
    ") -> numpy_typing.NDArray:\n",
    "    \"\"\"\n",
    "    Automatically reduce the dimensionality of node embeddings using Principal Component Analysis (PCA)\n",
    "    to reach a target explained variance ratio with the lowest possible number of components (output dimensions).\n",
    "\n",
    "    Parameters:\n",
    "    - features (pd.DataFrame) with a column 'embedding', where every value contains a float array with original dimensions.\n",
    "    - min_dimensions: Even if possible with the given variance, don't go below this number of dimensions for the output\n",
    "    - max_dimensions: Return at most the max number of dimensions, even if that means, that the target variance can't be met.\n",
    "    - target_variance (float): Cumulative variance threshold (default: 0.90)\n",
    "    - embedding_column_name (string): Defaults to 'embedding'\n",
    "\n",
    "    Returns: Reduced embeddings as an numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the input and get the original dimension\n",
    "    embeddings = np.stack(features[embedding_column_name].apply(np.array).tolist())\n",
    "    original_dimension = embeddings.shape[1]\n",
    "\n",
    "    # Fit PCA without dimensionality reduction to get explained variance\n",
    "    full_principal_component_analysis_without_reduction = PCA()\n",
    "    full_principal_component_analysis_without_reduction.fit(embeddings)\n",
    "\n",
    "    # Find smallest number of components to reach target variance\n",
    "    cumulative_variance = np.cumsum(full_principal_component_analysis_without_reduction.explained_variance_ratio_)\n",
    "    best_n_components = np.searchsorted(cumulative_variance, target_variance) + 1\n",
    "    best_n_components = max(best_n_components, min_dimensions) # Use at least min_dimensions\n",
    "    best_n_components = min(best_n_components, max_dimensions) # Use at most max_dimensions\n",
    "\n",
    "    # Apply PCA with optimal number of components\n",
    "    principal_component_analysis = PCA(n_components=best_n_components)\n",
    "    principal_component_analysis_results = principal_component_analysis.fit_transform(embeddings)\n",
    "\n",
    "    explained_variance_ratio_sum = sum(principal_component_analysis.explained_variance_ratio_)\n",
    "    print(f\"Dimensionality reduction from {original_dimension} to {best_n_components} (min {min_dimensions}, max {max_dimensions}) of node embeddings using Principal Component Analysis (PCA): Explained variance is {explained_variance_ratio_sum:.4f}.\")\n",
    "\n",
    "    return principal_component_analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a33f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_node_embeddings_reduced = reduce_dimensionality_of_node_embeddings(java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_prepared = np.hstack([java_package_anomaly_detection_features_standardized, java_package_anomaly_detection_node_embeddings_reduced])\n",
    "java_package_anomaly_detection_feature_names = list(java_package_features_to_standardize) + [f'pca_{i}' for i in range(java_package_anomaly_detection_node_embeddings_reduced.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e72e7",
   "metadata": {},
   "source": [
    "### 1.3 List the top 10 anomalies found using Isolation Forest\n",
    "\n",
    "> The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_optuna_tuning_results(optimized_study: Study, name_of_the_optimized_algorithm: str):\n",
    "\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} parameters (Optuna):\", optimized_study.best_params)\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} score:\", optimized_study.best_value)\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} parameter influence:\", get_param_importances(optimized_study, evaluator=MeanDecreaseImpurityImportanceEvaluator()))\n",
    "    \n",
    "    valid_trials = [trial for trial in optimized_study.trials if trial.value is not None and trial.state == TrialState.COMPLETE]\n",
    "    top_trials = sorted(valid_trials, key=lambda t: typing.cast(float, t.value), reverse=True)[:10]\n",
    "    for i, trial in enumerate(top_trials):\n",
    "        print(f\"Best {name_of_the_optimized_algorithm} parameter rank: {i+1}, trial: {trial.number}, Value = {trial.value:.6f}, Params: {trial.params}\")\n",
    "\n",
    "\n",
    "class AnomalyDetectionResults:\n",
    "    def __init__(self, \n",
    "                 anomaly_labels: np.ndarray, \n",
    "                 anomaly_scores: np.ndarray, \n",
    "                 random_forest_classifier: RandomForestClassifier,\n",
    "                 feature_importances: np.ndarray\n",
    "                 ):\n",
    "        self.anomaly_labels = anomaly_labels\n",
    "        self.anomaly_scores = anomaly_scores\n",
    "        self.random_forest_classifier = random_forest_classifier\n",
    "        self.feature_importances = feature_importances\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f\"AnomalyDetectionResults(anomaly_labels={self.anomaly_labels.shape}, \"\n",
    "                f\"anomaly_scores={self.anomaly_scores.shape}, \"\n",
    "                f\"random_forest_classifier={type(self.random_forest_classifier).__name__}, \"\n",
    "                f\"feature_importances={self.feature_importances.shape})\")\n",
    "\n",
    "\n",
    "def tune_anomaly_detection_models(\n",
    "    feature_matrix: np.ndarray,\n",
    "    contamination: float | typing.Literal[\"auto\"] = 0.05,\n",
    "    random_seed: int = 42,\n",
    "    number_of_trials: int = 20,\n",
    "    optimization_timeout_in_seconds: int = 40,\n",
    "    number_of_cross_validation_folds: int = 3,\n",
    ") -> AnomalyDetectionResults:\n",
    "    \"\"\"\n",
    "    Tunes both Isolation Forest and a proxy Random Forest using Optuna, maximizing the F1 score\n",
    "    between Isolation Forest pseudo-labels and proxy predictions. The proxy model mimics the behavior of the Isolation Forest \n",
    "    and is mainly used to provide feature importances and explainability using SHAP values later.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_matrix: np.ndarray of shape (n_samples, n_features), preprocessed input features.\n",
    "    - contamination: favor only a few suspicious cases with a fixed percentage\n",
    "    - random_seed: seed for reproducibility.\n",
    "    - number_of_trials: number of Optuna optimization trials.\n",
    "    - number_of_cross_validation_folds: number of cross validation (CV) folds for proxy model validation.\n",
    "\n",
    "    Returns:\n",
    "    - AnomalyDetectionResults containing:\n",
    "        - anomaly_labels: np.ndarray of shape (n_samples,), binary labels indicating anomalies (1) or normal (0).\n",
    "        - anomaly_scores: np.ndarray of shape (n_samples,), anomaly scores where higher values indicate more anomalous instances.\n",
    "        - random_forest_classifier: trained Random Forest classifier that mimics the Isolation Forest behavior.\n",
    "        - feature_importances: np.ndarray of shape (n_features,), feature importances from the Random Forest classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial) -> float:\n",
    "        # Isolation Forest hyperparameters\n",
    "        isolation_forest = IsolationForest(\n",
    "            max_samples=trial.suggest_float(\"isolation_max_samples\", 0.1, 1.0),\n",
    "            contamination=contamination, \n",
    "            n_estimators=trial.suggest_int(\"isolation_n_estimators\", 100, 400),\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        # Train Isolation Forest\n",
    "        isolation_forest.fit(feature_matrix)\n",
    "\n",
    "        # Generate pseudo-labels: 1 = anomaly, 0 = normal\n",
    "        pseudo_labels = (isolation_forest.predict(feature_matrix) == -1).astype(int)\n",
    "\n",
    "        if len(np.unique(pseudo_labels)) < 2:\n",
    "            print(\"tunedAnomalyDetectionExplained: Warning: Isolation Forest did not detect any anomalies. Returning a low score.\")\n",
    "            return 0.0\n",
    "        \n",
    "        # Proxy Random Forest hyperparameters\n",
    "        proxy_random_forest = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"proxy_n_estimators\", 100, 500),\n",
    "            max_depth=trial.suggest_int(\"proxy_max_depth\", 3, 20),\n",
    "            # class_weight=\"balanced\",\n",
    "            random_state=random_seed,\n",
    "        )\n",
    "\n",
    "        # Train proxy model\n",
    "        # Use cross-validation to get robust F1 score\n",
    "        f1_scores = cross_val_score(\n",
    "            proxy_random_forest,\n",
    "            feature_matrix,\n",
    "            pseudo_labels,\n",
    "            cv=number_of_cross_validation_folds,\n",
    "            scoring=\"f1\",\n",
    "        )\n",
    "        return float(np.mean(f1_scores))\n",
    "\n",
    "\n",
    "    # Print the number of samples and features in the feature matrix\n",
    "    n_samples = feature_matrix.shape[0]\n",
    "    print(f\"Tuned Anomaly Detection: Number of samples: {n_samples}, Number of features: {feature_matrix.shape[1]}, Number of trials: {number_of_trials}\")\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = create_study(direction=\"maximize\", sampler=TPESampler(seed=random_seed), study_name=\"AnomalyDetection_Tuning\")\n",
    "\n",
    "    # Try (enqueue) default settings\n",
    "    study.enqueue_trial({'isolation_max_samples': min(256, n_samples) / n_samples, 'isolation_n_estimators': 100, 'proxy_n_estimators': 100})\n",
    "    # Try (enqueue) some specific settings that led to good results during experiments\n",
    "    study.enqueue_trial({'isolation_max_samples': 0.5492229999946834, 'isolation_n_estimators': 162, 'proxy_n_estimators': 153, 'proxy_max_depth': 10})\n",
    "    study.enqueue_trial({'isolation_max_samples': 0.29110519961044856, 'isolation_n_estimators': 136, 'proxy_n_estimators': 77, 'proxy_max_depth': 8})\n",
    "    \n",
    "    study.enqueue_trial({'isolation_max_samples': 0.11350593116659227, 'isolation_n_estimators': 215, 'proxy_n_estimators': 112, 'proxy_max_depth': 15})\n",
    "    study.enqueue_trial({'isolation_max_samples': 0.2646104863448817, 'isolation_n_estimators': 185, 'proxy_n_estimators': 109, 'proxy_max_depth': 8})\n",
    "    \n",
    "    study.enqueue_trial({'isolation_max_samples': 0.1170151656515088, 'isolation_n_estimators': 299, 'proxy_n_estimators': 241, 'proxy_max_depth': 18})\n",
    "\n",
    "    study.optimize(objective, n_trials=number_of_trials, timeout=optimization_timeout_in_seconds)\n",
    "    output_optuna_tuning_results(study, study.study_name)\n",
    "\n",
    "    # Train best models from best params\n",
    "    best_params = study.best_params\n",
    "\n",
    "    best_isolation_forest = IsolationForest(\n",
    "        n_estimators=best_params[\"isolation_n_estimators\"],\n",
    "        max_samples=best_params[\"isolation_max_samples\"],\n",
    "        contamination=contamination,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    anomaly_detection_results = best_isolation_forest.fit_predict(feature_matrix)\n",
    "    anomaly_labels = (anomaly_detection_results == -1).astype(int) # 1 = anomaly, 0 = normal\n",
    "    anomaly_scores = best_isolation_forest.decision_function(feature_matrix) * -1  # higher = more anomalous\n",
    "\n",
    "    best_proxy_random_forest = RandomForestClassifier(\n",
    "        n_estimators=best_params[\"proxy_n_estimators\"],\n",
    "        max_depth=best_params[\"proxy_max_depth\"],\n",
    "        # class_weight=\"balanced\",\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    best_proxy_random_forest.fit(feature_matrix, anomaly_detection_results)\n",
    "\n",
    "    return AnomalyDetectionResults(anomaly_labels, anomaly_scores, best_proxy_random_forest, best_proxy_random_forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff20e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_anomaly_detection_results_to_features(\n",
    "    features: pd.DataFrame,\n",
    "    anomaly_detection_results: AnomalyDetectionResults,\n",
    "    anomaly_label_column: str = 'anomalyLabel',\n",
    "    anomaly_score_column: str = 'anomalyScore',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds anomaly detection results to the feature and returns the updated dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - features: pd.DataFrame of shape (n_samples, n_features).\n",
    "    - anomaly_detection_results: AnomalyDetectionResults object containing labels and scores.\n",
    "    - anomaly_label_column: Name for the anomaly label column.\n",
    "    - anomaly_score_column: Name for the anomaly score column.\n",
    "\n",
    "    Returns:\n",
    "    - Updated feature dataframe with anomaly labels and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add anomaly labels and scores to the feature matrix\n",
    "    features[anomaly_label_column] = anomaly_detection_results.anomaly_labels\n",
    "    features[anomaly_score_column] = anomaly_detection_results.anomaly_scores\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_anomalies(\n",
    "        anomaly_detected_features: pd.DataFrame, \n",
    "        anomaly_label_column: str = \"anomalyLabel\",\n",
    "        anomaly_score_column: str = \"anomalyScore\"\n",
    ") -> pd.DataFrame:\n",
    "    anomalies = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] == 1]\n",
    "    return anomalies.sort_values(by=anomaly_score_column, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_results = tune_anomaly_detection_models(java_package_anomaly_detection_features_prepared)\n",
    "java_package_anomaly_detection_features = add_anomaly_detection_results_to_features(java_package_anomaly_detection_features, java_package_anomaly_detection_results)\n",
    "display(get_top_10_anomalies(java_package_anomaly_detection_features).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfcc56",
   "metadata": {},
   "source": [
    "#### 1.3b List the top 10 anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10830e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_embedding_anomaly_detection_features = java_package_anomaly_detection_features[features_for_visualization_excluded_from_training + ['embedding', 'pageRank', 'articleRank']].copy()\n",
    "java_package_embedding_anomaly_detection_input = reduce_dimensionality_of_node_embeddings(java_package_embedding_anomaly_detection_features, max_dimensions=60, target_variance=0.95)\n",
    "java_package_embedding_anomaly_detection_feature_names = embedding_feature_names = [f'pca_{i}' for i in range(java_package_embedding_anomaly_detection_input.shape[1])]\n",
    "java_package_embedding_anomaly_detection_result = tune_anomaly_detection_models(java_package_embedding_anomaly_detection_input, contamination=\"auto\")\n",
    "java_package_embedding_anomaly_detection_features = add_anomaly_detection_results_to_features(java_package_embedding_anomaly_detection_features, java_package_embedding_anomaly_detection_result, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore')\n",
    "\n",
    "display(get_top_10_anomalies(java_package_embedding_anomaly_detection_features, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6ea49",
   "metadata": {},
   "source": [
    "#### 1.3c List the the top (most normal) non-anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68621d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_non_anomalies(\n",
    "        anomaly_detected_features: pd.DataFrame, \n",
    "        anomaly_label_column: str = \"anomalyLabel\",\n",
    "        anomaly_score_column: str = \"anomalyScore\"\n",
    ") -> pd.DataFrame:\n",
    "    anomalies = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] != 1]\n",
    "    return anomalies.sort_values(by=anomaly_score_column, ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_top_10_non_anomalies(java_package_anomaly_detection_features).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e083c1f",
   "metadata": {},
   "source": [
    "#### 1.3d List the the top (most normal) non-anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_top_10_non_anomalies(java_package_embedding_anomaly_detection_features, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed28c9",
   "metadata": {},
   "source": [
    "#### 1.3e Plot the distribution of the anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d051b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_score_distribution(\n",
    "        anomaly_detected_features: pd.DataFrame, \n",
    "        anomaly_label_column: str = \"anomalyLabel\",\n",
    "        anomaly_score_column: str = \"anomalyScore\",\n",
    "        title_prefix: str = \"\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the distribution of anomaly scores in the feature matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - anomaly_detected_features: pd.DataFrame containing anomaly labels and scores.\n",
    "    - anomaly_label_column: Name of the column containing anomaly labels.\n",
    "    - anomaly_score_column: Name of the column containing anomaly scores.\n",
    "    \"\"\"\n",
    "    plot.figure(figsize=(12, 6))\n",
    "    plot.hist(anomaly_detected_features[anomaly_score_column], bins=50, color='blue', alpha=0.7)\n",
    "    plot.title(f\"{title_prefix} Anomaly Score Distribution\")\n",
    "    plot.xlabel('Anomaly Score')\n",
    "    plot.ylabel('Frequency')\n",
    "    plot.grid(True)\n",
    "\n",
    "    # Add vertical lines for anomaly thresholds\n",
    "    anomaly_threshold = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] == 1][anomaly_score_column].min()\n",
    "    plot.axvline(anomaly_threshold, color='red', linestyle='dashed', linewidth=1, label='Anomaly Threshold')\n",
    "    plot.legend()\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cebd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomaly_score_distribution(java_package_anomaly_detection_features, title_prefix=\"Java Package\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6274214",
   "metadata": {},
   "source": [
    "#### 1.3f Plot the distribution of the anomaly scores solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2beda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomaly_score_distribution(\n",
    "    java_package_embedding_anomaly_detection_features, \n",
    "    anomaly_label_column='anomalyOfEmbeddingLabel',\n",
    "    anomaly_score_column='anomalyOfEmbeddingScore',\n",
    "    title_prefix=\"Java Package Embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3936d79",
   "metadata": {},
   "source": [
    "### 1.4 Plot anomalies\n",
    "\n",
    "Plots clustered nodes and highlights anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5604735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    title_prefix: str,\n",
    "    code_unit_column: str = \"shortCodeUnitName\",\n",
    "    cluster_label_column: str = \"clusterLabel\",\n",
    "    cluster_medoid_column: str = \"clusterMedoid\",\n",
    "    cluster_size_column: str = \"clusterSize\",\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    "    anomaly_score_column: str = \"anomalyScore\",\n",
    "    size_column: str = \"articleRank\",\n",
    "    x_position_column: str = 'embeddingVisualizationX',\n",
    "    y_position_column: str = 'embeddingVisualizationY',\n",
    "    annotate_top_n_anomalies: int = 10,\n",
    "    annotate_top_n_non_anomalies: int = 5,\n",
    "    annotate_top_n_clusters: int = 20,\n",
    ") -> None:\n",
    "    \n",
    "    if clustering_visualization_dataframe.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "    \n",
    "    def truncate(text: str, max_length: int = 22):\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length - 3] + \"...\"\n",
    "    \n",
    "    # Function to compute 2D Euclidean distance from center\n",
    "    def calculate_distances_to_center(data: pd.DataFrame, x_position_column: str, y_position_column: str):\n",
    "        center_x = data[x_position_column].mean()\n",
    "        center_y = data[y_position_column].mean()\n",
    "        return np.sqrt((data[x_position_column] - center_x)**2 + (data[y_position_column] - center_y)**2)\n",
    "\n",
    "    def zoom_into_center_while_preserving_top_anomalies(\n",
    "            data: pd.DataFrame,\n",
    "            distances_to_center: np.ndarray,\n",
    "            anomaly_score_column: str,\n",
    "            percentile_of_distance_to_center: float = 0.8,\n",
    "            top_n_anomalies: int = annotate_top_n_anomalies,\n",
    "    ) -> pd.DataFrame:\n",
    "        # Keep points within a distance percentile (e.g., 80%)\n",
    "        distance_to_center_threshold = np.quantile(distances_to_center, percentile_of_distance_to_center)\n",
    "        # Additionally, keep points that contain the Get the top_n_anomalies\n",
    "        anomaly_score_threshold = data[anomaly_score_column].nlargest(top_n_anomalies).iloc[-1]\n",
    "        return data[(distances_to_center <= distance_to_center_threshold) | (data[anomaly_score_column] >= anomaly_score_threshold)]\n",
    "\n",
    "    distances_to_center = calculate_distances_to_center(clustering_visualization_dataframe, x_position_column, y_position_column)\n",
    "    clustering_visualization_dataframe_zoomed = zoom_into_center_while_preserving_top_anomalies(clustering_visualization_dataframe, distances_to_center, anomaly_score_column)\n",
    "\n",
    "    cluster_anomalies = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[anomaly_label_column] == 1]\n",
    "    cluster_without_anomalies = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[anomaly_label_column] != 1]\n",
    "    cluster_noise = cluster_without_anomalies[cluster_without_anomalies[cluster_label_column] == -1]\n",
    "    cluster_non_noise = cluster_without_anomalies[cluster_without_anomalies[cluster_label_column] != -1]\n",
    "\n",
    "    plot.figure(figsize=(10, 10))\n",
    "    plot.title(f\"{title_prefix} (size={size_column}, main-color=cluster, red=anomaly, green=non-anomaly)\", pad=20)\n",
    "\n",
    "    # Plot noise (from clustering)\n",
    "    plot.scatter(\n",
    "        x=cluster_noise[x_position_column],\n",
    "        y=cluster_noise[y_position_column],\n",
    "        s=cluster_noise[size_column] * 60 + 2,\n",
    "        color='lightgrey',\n",
    "        alpha=0.4,\n",
    "        label='Noise'\n",
    "    )\n",
    "\n",
    "    # Plot clusters\n",
    "    plot.scatter(\n",
    "        x=cluster_non_noise[x_position_column],\n",
    "        y=cluster_non_noise[y_position_column],\n",
    "        s=cluster_non_noise[size_column] * 60 + 2,\n",
    "        c=cluster_non_noise[cluster_label_column],\n",
    "        cmap='tab20',\n",
    "        alpha=0.7,\n",
    "        label='Clusters'\n",
    "    )\n",
    "\n",
    "    # Plot anomalies\n",
    "    plot.scatter(\n",
    "        x=cluster_anomalies[x_position_column],\n",
    "        y=cluster_anomalies[y_position_column],\n",
    "        s=cluster_anomalies[size_column] * 60 + 10,\n",
    "        c=cluster_anomalies[anomaly_score_column],\n",
    "        cmap=\"Reds\",\n",
    "        alpha=0.9,\n",
    "        label='Anomaly'\n",
    "    )\n",
    "\n",
    "    # Annotate medoids of the cluster\n",
    "    cluster_medoids = cluster_non_noise[cluster_non_noise[cluster_medoid_column] == 1].sort_values(by=cluster_size_column, ascending=False).head(annotate_top_n_clusters)\n",
    "    for index, row in cluster_medoids.iterrows():\n",
    "        plot.annotate(\n",
    "            text=f\"{truncate(row[code_unit_column])} (cluster {row[cluster_label_column]})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5),\n",
    "            alpha=0.4,\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    # Annotate top non-anomalies\n",
    "    non_anomalies = cluster_without_anomalies.sort_values(by=anomaly_score_column, ascending=True).reset_index(drop=True).head(annotate_top_n_non_anomalies)\n",
    "    non_anomalies_in_reversed_order = non_anomalies.iloc[::-1] # plot most important annotations last to overlap less important ones\n",
    "    for dataframe_index, row in non_anomalies_in_reversed_order.iterrows():\n",
    "        index = typing.cast(int, dataframe_index)\n",
    "        plot.annotate(\n",
    "            text=f\"#{index + 1}: {truncate(row[code_unit_column])} ({row[anomaly_score_column]:.3f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5 + (index % 5) * 10),\n",
    "            color='green',\n",
    "            alpha=0.7,\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    # Annotate top anomalies\n",
    "    anomalies = cluster_anomalies.sort_values(by=anomaly_score_column, ascending=False).reset_index(drop=True).head(annotate_top_n_anomalies)\n",
    "    anomalies_in_reversed_order = anomalies.iloc[::-1] # plot most important annotations last to overlap less important ones\n",
    "    for dataframe_index, row in anomalies_in_reversed_order.iterrows():\n",
    "        index = typing.cast(int, dataframe_index)\n",
    "        plot.annotate(\n",
    "            text=f\"#{index + 1}: {truncate(row[code_unit_column])} ({row[anomaly_score_column]:.3f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5 + (index % 5) * 10),\n",
    "            color='red',\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(java_package_anomaly_detection_features, title_prefix=\"Java Package Anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b08b6",
   "metadata": {},
   "source": [
    "#### 1.4b Plot anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c08da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(\n",
    "    java_package_embedding_anomaly_detection_features, \n",
    "    title_prefix=\"Java Package Anomalies (Embeddings Only)\",\n",
    "    anomaly_label_column='anomalyOfEmbeddingLabel',\n",
    "    anomaly_score_column='anomalyOfEmbeddingScore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa822ca",
   "metadata": {},
   "source": [
    "### 1.5 Print the 20 most influential features (no explainable AI yet)\n",
    "\n",
    "Use Random Forest as a proxy to estimate the importance of each feature contributing to the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b21d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_importances_series = pd.Series(java_package_anomaly_detection_results.feature_importances, index=java_package_anomaly_detection_feature_names).sort_values(ascending=False)\n",
    "print(java_package_anomaly_detection_importances_series.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03216e",
   "metadata": {},
   "source": [
    "### 1.6 Use SHAP to explain the detected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "DType = typing.TypeVar(\"DType\", bound=np.generic)\n",
    "Numpy_Array = numpy_typing.NDArray[DType]\n",
    "Two_Dimensional_Vector = typing.Annotated[Numpy_Array, typing.Literal[2]]\n",
    "\n",
    "class AnomaliesExplanationResults:\n",
    "    def __init__(self, \n",
    "                 shap_anomaly_values: Numpy_Array, \n",
    "                 shap_expected_anomaly_value: float, \n",
    "                 ):\n",
    "        assert shap_anomaly_values.ndim == 2, \"The 'shap_anomaly_values' must be a 2D numpy array.\"\n",
    "        self.shap_anomaly_values = shap_anomaly_values\n",
    "        self.shap_expected_anomaly_value = shap_expected_anomaly_value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"AnomaliesExplanationResults(shap_anomaly_values shape={self.shap_anomaly_values.shape}, \"\n",
    "                f\"shap_expected_anomaly_value shape={self.shap_expected_anomaly_value})\")\n",
    "\n",
    "\n",
    "def explain_anomalies_with_shap(\n",
    "        random_forest_model: RandomForestClassifier,\n",
    "        prepared_features: Numpy_Array\n",
    ") -> AnomaliesExplanationResults:\n",
    "    \"\"\"\n",
    "    Use SHAP to explain the detected anomalies.\n",
    "    \"\"\"\n",
    "    if not isinstance(prepared_features, np.ndarray) or len(prepared_features.shape) != 2:\n",
    "        raise ValueError(\"Prepared_features must be a 2D numpy array.\")\n",
    "    \n",
    "    # Use TreeExplainer on Random Forest Proxy Model\n",
    "    # This is necessary because Isolation Forest does not support SHAP explanations directly.\n",
    "    explainer = shap.TreeExplainer(random_forest_model)\n",
    "    shap_values = explainer.shap_values(prepared_features)\n",
    "    shap_anomaly_values = shap_values[:, :, 1] # Class 1 = anomaly\n",
    "    \n",
    "    print(f\"Explain Anomaly Decision with SHAP: features shape={prepared_features.shape}\")\n",
    "    print(f\"Explain Anomaly Decision with SHAP: values shape={np.shape(shap_values)}\")\n",
    "    print(f\"Explain Anomaly Decision with SHAP: anomaly values shape={shap_anomaly_values.shape}\")\n",
    "    print(f\"Explain Anomaly Decision with SHAP: expected_value shape={np.shape(typing.cast(np.ndarray, explainer.expected_value))}\")\n",
    "    print(f\"Explain Anomaly Decision with SHAP: expected_value type={type(explainer.expected_value)}\")\n",
    "    \n",
    "    shap_expected_anomaly_value = typing.cast(Numpy_Array, explainer.expected_value)[1]  # Class 1 = anomaly\n",
    "    \n",
    "    return AnomaliesExplanationResults(shap_anomaly_values, shap_expected_anomaly_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72d1c1",
   "metadata": {},
   "source": [
    "#### 1.6a Plot global feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explained_global_feature_importance(\n",
    "    shap_anomaly_values: numpy_typing.NDArray,\n",
    "    anomaly_detected_features: pd.DataFrame,\n",
    "    prepared_features: numpy_typing.NDArray,\n",
    "    feature_names: list[str],\n",
    "    title_prefix: str = \"\",\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Explain anomalies using SHAP values and plot the global feature importance.\n",
    "    This function uses the SHAP library to visualize the impact of features on the model's predictions\n",
    "    for anomalies detected by the Isolation Forest model.\n",
    "    It generates a bar plot showing the most influential features for the anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    anomaly_rows = anomaly_detected_features[anomaly_label_column] == 1 # Filter anomalies\n",
    "    shap.summary_plot(\n",
    "        shap_anomaly_values[anomaly_rows, :],  # Class 1 = anomaly\n",
    "        prepared_features[anomaly_rows],\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        max_display=20,\n",
    "        plot_size=(12, 6),  # (width, height) in inches\n",
    "        show=False\n",
    "    )\n",
    "    plot.title(f\"{title_prefix}: Which features contribute most to the anomaly score? (SHAP explained)\", fontsize=14)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d671e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomalies_explanation_results = explain_anomalies_with_shap(\n",
    "    random_forest_model=java_package_anomaly_detection_results.random_forest_classifier,\n",
    "    prepared_features=java_package_anomaly_detection_features_prepared\n",
    ")\n",
    "plot_shap_explained_global_feature_importance(\n",
    "    shap_anomaly_values=java_package_anomalies_explanation_results.shap_anomaly_values,\n",
    "    anomaly_detected_features=java_package_anomaly_detection_features, \n",
    "    prepared_features=java_package_anomaly_detection_features_prepared,\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98e1b1",
   "metadata": {},
   "source": [
    "#### 1.6b Plot global feature importance including direction\n",
    "\n",
    "* **Best for:** Global understanding of which features drive anomalies.\n",
    "* **Adds:** Direction of impact (color shows feature value).\n",
    "* **Why:** Useful when you want to see how values push predictions toward normal or anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explained_beeswarm(\n",
    "    shap_anomaly_values: np.ndarray,\n",
    "    prepared_features: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    title_prefix: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Uses the SHAP values for anomalies to visaulize the global feature importance in a beeswarm plot.\n",
    "    Best for global understanding of which features drive anomalies.\n",
    "    Adds direction of impact (color shows feature value).\n",
    "    Useful when you want to see how values push predictions toward normal or anomalous.\n",
    "    \"\"\"\n",
    "\n",
    "    shap.summary_plot(\n",
    "        shap_anomaly_values,\n",
    "        prepared_features,\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"dot\",\n",
    "        max_display=20,\n",
    "        plot_size=(12, 6),  # (width, height) in inches\n",
    "        show=False\n",
    "    )\n",
    "    plot.title(f\"How {title_prefix} features drive anomalies globally (beeswarm plot)\", fontsize=12)\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6676c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_beeswarm(\n",
    "    shap_anomaly_values=java_package_anomalies_explanation_results.shap_anomaly_values,\n",
    "    prepared_features=java_package_anomaly_detection_features_prepared,\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42495f",
   "metadata": {},
   "source": [
    "#### 1.6c Plot global feature importance including direction for solely embeddings\n",
    "\n",
    "As in 1.6b we use the beeswarm plot to show the global feature importance including direction, but this time for the anomaly detection solely based on embeddings and not any other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a781b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_embedding_anomalies_explanation_results = explain_anomalies_with_shap(\n",
    "    random_forest_model=java_package_embedding_anomaly_detection_result.random_forest_classifier,\n",
    "    prepared_features=java_package_embedding_anomaly_detection_input\n",
    ")\n",
    "plot_shap_explained_beeswarm(\n",
    "    shap_anomaly_values=java_package_embedding_anomalies_explanation_results.shap_anomaly_values,\n",
    "    prepared_features=java_package_embedding_anomaly_detection_input,\n",
    "    feature_names=java_package_embedding_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package Embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b82b5e",
   "metadata": {},
   "source": [
    "#### 1.6d Plot local feature importance for every of the top 10 anomalies\n",
    "\n",
    "* **Best for:** Explaining *why a specific data point* is anomalous.\n",
    "* **Adds:** Visual breakdown of how each feature contributes to the score.\n",
    "* **Why:** Highly interpretable for debugging single nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explained_local_feature_importance(\n",
    "    index_to_explain,\n",
    "    anomalies_explanation_results: AnomaliesExplanationResults,\n",
    "    prepared_features: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    title: str = \"\",\n",
    "    rounding_precision: int = 4,\n",
    "):\n",
    "    \"\"\"    \n",
    "    Uses the SHAP values for anomalies to visualize the local feature importance for a specific anomaly.\n",
    "    This function generates a force plot showing how each feature contributes to the anomaly score for a specific anomaly instance.\n",
    "    The force plot is a powerful visualization that helps to understand the impact of each feature for each as anomaly classified data point.\n",
    "    Visual breakdown of how each feature contributes to the score.\n",
    "    Highly interpretable for debugging single nodes.\n",
    "    \"\"\"\n",
    "    shap_anomaly_values = anomalies_explanation_results.shap_anomaly_values\n",
    "    expected_anomaly_value = anomalies_explanation_results.shap_expected_anomaly_value\n",
    "\n",
    "    shap_values_rounded = np.round(shap_anomaly_values[index_to_explain], rounding_precision)\n",
    "    prepared_features_rounded = prepared_features[index_to_explain].round(rounding_precision)\n",
    "    base_value_rounded = np.round(expected_anomaly_value, rounding_precision)\n",
    "\n",
    "    shap.force_plot(\n",
    "        base_value_rounded,\n",
    "        shap_values_rounded,\n",
    "        prepared_features_rounded,\n",
    "        feature_names=feature_names,\n",
    "        matplotlib=True,\n",
    "        show=False,\n",
    "        contribution_threshold=0.06\n",
    "    )\n",
    "    current_figure = plot.gcf()\n",
    "\n",
    "    # Resize fonts manually (best effort, affects all text)\n",
    "    for text in current_figure.findobj(match=plot.Text):\n",
    "        text.set_fontsize(10)  # Set smaller font\n",
    "\n",
    "    if title.strip():\n",
    "        plot.title(title, fontsize=16, loc='left', y=0.05)\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_top_10_anomalies = get_top_10_anomalies(java_package_anomaly_detection_features)\n",
    "\n",
    "index=0\n",
    "for row_index, row in java_package_top_10_anomalies.iterrows():\n",
    "    row_index = typing.cast(int, row_index)\n",
    "    index=index+1\n",
    "    plot_shap_explained_local_feature_importance(\n",
    "        index_to_explain=row_index,\n",
    "        anomalies_explanation_results=java_package_anomalies_explanation_results,\n",
    "        prepared_features=java_package_anomaly_detection_features_prepared,\n",
    "        feature_names=java_package_anomaly_detection_feature_names,\n",
    "        title=f\"Java Package \\\"{row['shortCodeUnitName']}\\\" anomaly #{index} explained\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446a0f3",
   "metadata": {},
   "source": [
    "#### 1.6e Plot feature dependence for each of top 10 important features\n",
    "\n",
    "* **Best for:** Understanding how *one feature* affects anomaly scores.\n",
    "* **Adds:** Color can show interaction with another feature.\n",
    "* **Why:** Helps discover *nonlinear effects or interaction terms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explained_top_10_feature_dependence(\n",
    "    shap_anomaly_values: np.ndarray,\n",
    "    prepared_features: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses the SHAP values for anomalies to visualize the top 10 feature dependence plots.\n",
    "    This function generates dependence plots for the top 10 features that contribute most to the anomaly score\n",
    "    based on the mean absolute SHAP values across all anomalies.\n",
    "    Dependence plots show how the SHAP value of a feature varies with its value, helping to understand the relationship between feature values and their impact on the anomaly score.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_shap_anomaly_value = np.abs(shap_anomaly_values).mean(axis=0)\n",
    "    sorted_by_mean_ascending = np.argsort(mean_shap_anomaly_value)\n",
    "    top_10_indices_ascending = sorted_by_mean_ascending[-10:]\n",
    "    top_10_indices_descending = top_10_indices_ascending[::-1]  # Reverse to get descending order\n",
    "    top_feature_names = [feature_names[i] for i in top_10_indices_descending]  # Get names of top features\n",
    "    \n",
    "    figure, axes = plot.subplots(5, 2, figsize=(15, 20))  # 5 rows x 2 columns\n",
    "    figure.suptitle(f\"{title_prefix} Anomalies: Top 10 feature dependence plots\", fontsize=16)\n",
    "    axes = axes.flatten()  # Flatten for easy iteration\n",
    "\n",
    "    for index, feature in enumerate(top_feature_names):\n",
    "        shap.dependence_plot(\n",
    "            ind=feature,  # Feature name or index\n",
    "            shap_values=shap_anomaly_values,\n",
    "            features=prepared_features,\n",
    "            feature_names=feature_names,\n",
    "            interaction_index=None,  # Set to a feature name/index to see interactions\n",
    "            show=False,\n",
    "            ax=axes[index]\n",
    "        )\n",
    "\n",
    "    plot.tight_layout(rect=(0.0, 0.02, 1.0, 0.98))\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_top_10_feature_dependence(\n",
    "    shap_anomaly_values=java_package_anomalies_explanation_results.shap_anomaly_values,\n",
    "    prepared_features=java_package_anomaly_detection_features_prepared,\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba6ac1",
   "metadata": {},
   "source": [
    "#### 1.6f Plot interaction values for each of top 10 important features\n",
    "\n",
    "* **Best for:** Tracing how the model arrives at a decision.\n",
    "* **Adds:** Shows cumulative impact of features.\n",
    "* **Why:** Good for **comparing multiple instances** and identifying tipping-point features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explained_decision(\n",
    "    anomalies_explanation_results: AnomaliesExplanationResults,\n",
    "    prepared_features: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    title_prefix: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses the SHAP values for anomalies to visualize the top 10 feature dependence plots.\n",
    "    This function generates dependence plots for the top 10 features that contribute most to the anomaly score\n",
    "    based on the mean absolute SHAP values across all anomalies.\n",
    "    Dependence plots show how the SHAP value of a feature varies with its value, helping to understand the relationship between feature values and their impact on the anomaly score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get indices of the top 10 samples with the highest anomaly scores\n",
    "    top_anomaly_indices = np.argsort(anomalies_explanation_results.shap_anomaly_values.mean(axis=1))[-10:]\n",
    "\n",
    "    plot.figure(figsize=(12, 8))\n",
    "    shap.decision_plot(\n",
    "        base_value=anomalies_explanation_results.shap_expected_anomaly_value,\n",
    "        shap_values=anomalies_explanation_results.shap_anomaly_values[top_anomaly_indices],\n",
    "        feature_order='importance',\n",
    "        highlight=None,  # No specific highlight\n",
    "        features=prepared_features[top_anomaly_indices],\n",
    "        feature_names=feature_names,\n",
    "        link='identity',\n",
    "        new_base_value=None,\n",
    "        plot_color='Spectral',\n",
    "        auto_size_plot=False,\n",
    "        show=False,\n",
    "    )\n",
    "\n",
    "    plot.title(f\"{title_prefix} Anomalies: How were the decisions made for the top 10 anomalies?\", fontsize=14)\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_decision(\n",
    "    anomalies_explanation_results=java_package_anomalies_explanation_results,\n",
    "    prepared_features=java_package_anomaly_detection_features_prepared,\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33560",
   "metadata": {},
   "source": [
    "### 1.7 Add the top SHAP features and their scores to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_shap_features_to_anomalies(\n",
    "    shap_anomaly_values: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    anomaly_detected_features: pd.DataFrame,\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    "    top_n: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds top-N SHAP features and their SHAP values for each anomaly in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - shap_values_array: SHAP values array with shape (n_samples, n_features).\n",
    "    - feature_names: List of names corresponding to the features.\n",
    "    - anomaly_detected_features: Original DataFrame containing anomaly labels.\n",
    "    - anomaly_label_column: Name of the column indicating anomalies (1 = anomaly).\n",
    "    - top_n: Number of top influential features to extract.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with additional columns:\n",
    "      anomalyTopFeature_1, ..., topFeature_N\n",
    "      anomalyTopFeatureSHAPValue_, ..., topFeatureValue_N\n",
    "    \"\"\"\n",
    "    # Convert SHAP values to DataFrame for easier processing\n",
    "    shap_dataframe = pd.DataFrame(shap_anomaly_values, columns=feature_names)\n",
    "\n",
    "    # Get indices of rows marked as anomalies\n",
    "    anomaly_indices = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] == 1].index\n",
    "\n",
    "    # Initialize result columns\n",
    "    for rank in range(1, top_n + 1):\n",
    "        anomaly_detected_features[f\"anomalyTopFeature_{rank}\"] = \"\"\n",
    "        anomaly_detected_features[f\"anomalyTopFeatureSHAPValue_{rank}\"] = 0.0\n",
    "\n",
    "    # Loop over each anomaly and assign top features + SHAP values\n",
    "    for index in anomaly_indices:\n",
    "        row_values = shap_dataframe.loc[index]\n",
    "        top_features = row_values.abs().sort_values(ascending=False).head(top_n)\n",
    "        for rank, (feature_name, shap_value) in enumerate(row_values[top_features.index].items(), 1):\n",
    "            anomaly_detected_features.at[index, f\"anomalyTopFeature_{rank}\"] = feature_name\n",
    "            anomaly_detected_features.at[index, f\"anomalyTopFeatureSHAPValue_{rank}\"] = shap_value\n",
    "\n",
    "    return anomaly_detected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_top_shap_features_to_anomalies(\n",
    "    shap_anomaly_values=java_package_anomalies_explanation_results.shap_anomaly_values,\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    anomaly_detected_features=java_package_anomaly_detection_features\n",
    ")\n",
    "display(java_package_anomaly_detection_features[java_package_anomaly_detection_features[\"anomalyLabel\"] == 1].sort_values(by='anomalyScore', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682bb64",
   "metadata": {},
   "source": [
    "## 2. Java Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25370d7f",
   "metadata": {},
   "source": [
    "### 2.1 Query Features\n",
    "\n",
    "Query all features that are relevant for anomaly detection. Some of them come from precalculated clustering (HDBSCAN), node embeddings (Fast Random Projection), community detection algorithms (Leiden, Local Clustering Coefficient), centrality algorithms (Page Rank, Article Rank, Betweenness) and classical metrics like the in-/out-degree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Type)\n",
    "    WHERE (codeUnit.incomingDependencies IS NOT NULL OR codeUnit.outgoingDependencies IS NOT NULL)\n",
    "      AND codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "      AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationX       IS NOT NULL\n",
    "      AND codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationY       IS NOT NULL\n",
    "     WITH * \n",
    "         ,coalesce(codeUnit.incomingDependencies, 0) AS incomingDependencies\n",
    "         ,coalesce(codeUnit.outgoingDependencies, 0) AS outgoingDependencies\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                                  AS codeUnitName\n",
    "        ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "        ,artifact.name                                                 AS projectName\n",
    "        ,incomingDependencies\n",
    "        ,outgoingDependencies\n",
    "        ,incomingDependencies + outgoingDependencies                   AS degree\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "        ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "        ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "        ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "        ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "        ,codeUnit.communityLocalClusteringCoefficient                  AS locallusteringCoefficient\n",
    "        ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "        ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "        ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "        ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "        ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "        ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "        ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationX          AS embeddingVisualizationX\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationY          AS embeddingVisualizationY\n",
    "\"\"\"\n",
    "\n",
    "java_type_anomaly_detection_features = query_cypher_to_data_frame(java_type_anomaly_detection_features_query)\n",
    "java_type_features_to_standardize = java_type_anomaly_detection_features.columns.drop(features_for_visualization_excluded_from_training + ['embedding']).to_list()\n",
    "\n",
    "display(java_type_anomaly_detection_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c67ed0",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "Prepare the data by standardizing numeric fields and reducing the dimensionality of the node embeddings to not dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681090a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(java_type_anomaly_detection_features)\n",
    "java_type_anomaly_detection_features_standardized = standardize_features(java_type_anomaly_detection_features, java_type_features_to_standardize)\n",
    "java_type_anomaly_detection_node_embeddings_reduced = reduce_dimensionality_of_node_embeddings(java_type_anomaly_detection_features)\n",
    "\n",
    "java_type_anomaly_detection_features_prepared = np.hstack([java_type_anomaly_detection_features_standardized, java_type_anomaly_detection_node_embeddings_reduced])\n",
    "java_type_anomaly_detection_feature_names = list(java_type_features_to_standardize) + [f'pca_{i}' for i in range(java_type_anomaly_detection_node_embeddings_reduced.shape[1])]\n",
    "\n",
    "plot_feature_correlation_matrix(java_type_anomaly_detection_features[java_type_features_to_standardize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7ac1b",
   "metadata": {},
   "source": [
    "### 2.3 List the top 10 anomalies found using Isolation Forest\n",
    "\n",
    "> The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_results = tune_anomaly_detection_models(java_type_anomaly_detection_features_prepared)\n",
    "java_type_anomaly_detection_features = add_anomaly_detection_results_to_features(java_type_anomaly_detection_features, java_type_anomaly_detection_results)\n",
    "display(get_top_10_anomalies(java_type_anomaly_detection_features).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314821d",
   "metadata": {},
   "source": [
    "#### 2.3b List the top 10 anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab174c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_embedding_anomaly_detection_features = java_type_anomaly_detection_features[features_for_visualization_excluded_from_training + ['embedding', 'pageRank', 'articleRank']].copy()\n",
    "java_type_embedding_anomaly_detection_input = reduce_dimensionality_of_node_embeddings(java_type_embedding_anomaly_detection_features, max_dimensions=60, target_variance=0.95)\n",
    "java_type_embedding_anomaly_detection_feature_names = embedding_feature_names = [f'pca_{i}' for i in range(java_type_embedding_anomaly_detection_input.shape[1])]\n",
    "java_type_embedding_anomaly_detection_result = tune_anomaly_detection_models(java_type_embedding_anomaly_detection_input, contamination=\"auto\")\n",
    "java_type_embedding_anomaly_detection_features = add_anomaly_detection_results_to_features(java_type_embedding_anomaly_detection_features, java_type_embedding_anomaly_detection_result, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore')\n",
    "\n",
    "display(get_top_10_anomalies(java_type_embedding_anomaly_detection_features, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac51eb",
   "metadata": {},
   "source": [
    "#### 1.3c List the top (most normal) non-anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_top_10_non_anomalies(java_type_anomaly_detection_features).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52ffa2",
   "metadata": {},
   "source": [
    "#### 1.3d List the top (most normal) non-anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_top_10_non_anomalies(java_type_embedding_anomaly_detection_features, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b635a0e",
   "metadata": {},
   "source": [
    "#### 1.3e Plot the distribution of the anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomaly_score_distribution(java_type_anomaly_detection_features, title_prefix=\"Java Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269582b",
   "metadata": {},
   "source": [
    "#### 1.3f Plot the distribution of the anomaly scores solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomaly_score_distribution(\n",
    "    java_type_embedding_anomaly_detection_features, \n",
    "    anomaly_label_column='anomalyOfEmbeddingLabel',\n",
    "    anomaly_score_column='anomalyOfEmbeddingScore',\n",
    "    title_prefix=\"Java Type Embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a00628",
   "metadata": {},
   "source": [
    "### 2.4. Plot anomalies\n",
    "\n",
    "Plots clustered nodes and highlights anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(java_type_anomaly_detection_features, title_prefix=\"Java Type Anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05275be7",
   "metadata": {},
   "source": [
    "#### 2.4.b Plot anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c98022",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(\n",
    "    java_type_embedding_anomaly_detection_features, \n",
    "    title_prefix=\"Java Type Anomalies (Embeddings Only)\",\n",
    "    anomaly_label_column='anomalyOfEmbeddingLabel',\n",
    "    anomaly_score_column='anomalyOfEmbeddingScore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e565f84",
   "metadata": {},
   "source": [
    "### 2.5 Print the 20 most influential features (no explainable AI yet)\n",
    "\n",
    "Use Random Forest as a proxy to estimate the importance of each feature contributing to the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86945e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_importances_series = pd.Series(java_type_anomaly_detection_results.feature_importances, index=java_type_anomaly_detection_feature_names).sort_values(ascending=False)\n",
    "print(java_type_anomaly_detection_importances_series.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a0379",
   "metadata": {},
   "source": [
    "### 2.6 Use SHAP to explain the detected anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomalies_explanation_results = explain_anomalies_with_shap(\n",
    "    random_forest_model=java_type_anomaly_detection_results.random_forest_classifier,\n",
    "    prepared_features=java_type_anomaly_detection_features_prepared\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f324a38",
   "metadata": {},
   "source": [
    "#### 2.6a Plot global feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ead283",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_global_feature_importance(\n",
    "    shap_anomaly_values=java_type_anomalies_explanation_results.shap_anomaly_values,\n",
    "    anomaly_detected_features=java_type_anomaly_detection_features, \n",
    "    prepared_features=java_type_anomaly_detection_features_prepared,\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452ef0a",
   "metadata": {},
   "source": [
    "#### 2.6b Plot global feature importance including direction\n",
    "\n",
    "* **Best for:** Global understanding of which features drive anomalies.\n",
    "* **Adds:** Direction of impact (color shows feature value).\n",
    "* **Why:** Useful when you want to see how values push predictions toward normal or anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ae841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_beeswarm(\n",
    "    shap_anomaly_values=java_type_anomalies_explanation_results.shap_anomaly_values,\n",
    "    prepared_features=java_type_anomaly_detection_features_prepared,\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8daa48",
   "metadata": {},
   "source": [
    "#### 2.6c Plot global feature importance including direction for solely embeddings\n",
    "\n",
    "As in 1.6b we use the beeswarm plot to show the global feature importance including direction, but this time for the anomaly detection solely based on embeddings and not any other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_embedding_anomalies_explanation_results = explain_anomalies_with_shap(\n",
    "    random_forest_model=java_type_embedding_anomaly_detection_result.random_forest_classifier,\n",
    "    prepared_features=java_type_embedding_anomaly_detection_input\n",
    ")\n",
    "plot_shap_explained_beeswarm(\n",
    "    shap_anomaly_values=java_type_embedding_anomalies_explanation_results.shap_anomaly_values,\n",
    "    prepared_features=java_type_embedding_anomaly_detection_input,\n",
    "    feature_names=java_type_embedding_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Type Embeddings-Only\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b51de",
   "metadata": {},
   "source": [
    "#### 2.6d Plot local feature importance for every of the top 10 anomalies\n",
    "\n",
    "* **Best for:** Explaining *why a specific data point* is anomalous.\n",
    "* **Adds:** Visual breakdown of how each feature contributes to the score.\n",
    "* **Why:** Highly interpretable for debugging single nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_top_10_anomalies = get_top_10_anomalies(java_type_anomaly_detection_features)\n",
    "\n",
    "index=0\n",
    "for row_index, row in java_type_top_10_anomalies.iterrows():\n",
    "    row_index = typing.cast(int, row_index)\n",
    "    index=index+1\n",
    "    plot_shap_explained_local_feature_importance(\n",
    "        index_to_explain=row_index,\n",
    "        anomalies_explanation_results=java_type_anomalies_explanation_results,\n",
    "        prepared_features=java_type_anomaly_detection_features_prepared,\n",
    "        feature_names=java_type_anomaly_detection_feature_names,\n",
    "        title=f\"Java Type \\\"{row['shortCodeUnitName']}\\\" anomaly #{index} explained\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725718d",
   "metadata": {},
   "source": [
    "#### 2.6e Plot feature dependence for each of top 10 important features\n",
    "\n",
    "* **Best for:** Understanding how *one feature* affects anomaly scores.\n",
    "* **Adds:** Color can show interaction with another feature.\n",
    "* **Why:** Helps discover *nonlinear effects or interaction terms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce899258",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_top_10_feature_dependence(\n",
    "    shap_anomaly_values=java_type_anomalies_explanation_results.shap_anomaly_values,\n",
    "    prepared_features=java_type_anomaly_detection_features_prepared,\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a606cc2",
   "metadata": {},
   "source": [
    "#### 2.6f Plot interaction values for each of top 10 important features\n",
    "\n",
    "* **Best for:** Tracing how the model arrives at a decision.\n",
    "* **Adds:** Shows cumulative impact of features.\n",
    "* **Why:** Good for **comparing multiple instances** and identifying tipping-point features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_explained_decision(\n",
    "    anomalies_explanation_results=java_type_anomalies_explanation_results,\n",
    "    prepared_features=java_type_anomaly_detection_features_prepared,\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a55b29",
   "metadata": {},
   "source": [
    "### 2.7 Add the top SHAP features and their scores to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_top_shap_features_to_anomalies(\n",
    "    shap_anomaly_values=java_type_anomalies_explanation_results.shap_anomaly_values,\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    anomaly_detected_features=java_type_anomaly_detection_features\n",
    ")\n",
    "display(java_type_anomaly_detection_features[java_type_anomaly_detection_features[\"anomalyLabel\"] == 1].sort_values(by='anomalyScore', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "JohT"
   }
  ],
  "code_graph_analysis_pipeline_data_validation": "ValidateAlwaysFalse",
  "kernelspec": {
   "display_name": "codegraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Anomaly Detection - Manual Exploration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
