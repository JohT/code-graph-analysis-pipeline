{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0eabc4",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Isolation Forest - Manual Exploration\n",
    "\n",
    "This notebook demonstrates anomaly detection with Isolation Forest for static code analysis gathered by using jQAssistant and Neo4j. Detecting anomalies in the data can be useful for identifying potential issues or areas for improvement in the codebase. To explain the results, we use SHAP (SHapley Additive exPlanations) to provide insights into the feature importances and how they contribute to the anomaly scores.\n",
    "\n",
    "<br>  \n",
    "\n",
    "### References\n",
    "- [jqassistant](https://jqassistant.org)\n",
    "- [Neo4j Python Driver](https://neo4j.com/docs/api/python-driver/current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1e4fb",
   "metadata": {},
   "source": [
    "## Features overview\n",
    "\n",
    "| **Feature**                      | **Type**           | **What it Measures**                        | **Why Itâ€™s Useful**                         |\n",
    "| -------------------------------- | ------------------ | ------------------------------------------- | ------------------------------------------- |\n",
    "| `PageRank`                       | Centrality         | Popularity / referenced code                | High = many dependents                      |\n",
    "| `ArticleRank`                    | Centrality         | How much the code depends on others         | High = high dependency                      |\n",
    "| `PageRank - ArticleRank`         | Relative Rank      | Role inversion / architectural layering     | Highlights mismatches                       |\n",
    "| `Betweenness Centrality`         | Centrality         | Bridge or control nodes                     | High = structural chokepoints               |\n",
    "| `Local Clustering Coefficient`   | Structural         | Local cohesion / modularity                 | Low = isolated node in a clique-like region |\n",
    "| `Degree` (Total and In)         | Structural         | Connectivity                                | Raw values may dominate                     |\n",
    "| `Node Embedding` (PCA reduced)   | Latent             | Structural and semantic similarity          | Captures latent position in graph           |\n",
    "| `Normalized Cluster Distance`    | Geometric          | Relative to cluster radius                  | Adds context to position                    |\n",
    "| `1.0 - HDBSCAN membership probability` | Cluster Confidence | How confidently HDBSCAN clustered this node, 1-x inverted | High score = likely anomaly                   |\n",
    "| `Average Cluster Radius`          | Cluster Context    | How tight or spread out the cluster is         | Highly spread clusters may be a less meaningful one   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy.typing as numpy_typing\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from optuna.importance import get_param_importances, MeanDecreaseImpurityImportanceEvaluator\n",
    "from optuna.trial import TrialState\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna import Study, create_study\n",
    "\n",
    "import shap # Explainable AI tool\n",
    "\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell uses the build-in %html \"magic\" to override the CSS style for tables to a much smaller size.\n",
    "#This is especially needed for PDF export of tables with multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS style for smaller dataframe tables. */\n",
    ".dataframe th {\n",
    "    font-size: 8px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 8px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07319282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Colormap\n",
    "# main_color_map = 'nipy_spectral'\n",
    "main_color_map = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version as python_version\n",
    "print('Python version: {}'.format(python_version))\n",
    "\n",
    "from numpy import __version__ as numpy_version\n",
    "print('numpy version: {}'.format(numpy_version))\n",
    "\n",
    "from pandas import __version__ as pandas_version\n",
    "print('pandas version: {}'.format(pandas_version))\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('sklearn version: {}'.format(sklearn_version))\n",
    "\n",
    "from matplotlib import __version__ as matplotlib_version\n",
    "print('matplotlib version: {}'.format(matplotlib_version))\n",
    "\n",
    "from neo4j import __version__ as neo4j_version\n",
    "print('neo4j version: {}'.format(neo4j_version))\n",
    "\n",
    "from optuna import __version__ as optuna_version\n",
    "print('optuna version: {}'.format(optuna_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the environment variable \"NEO4J_INITIAL_PASSWORD\" in your shell \n",
    "# before starting jupyter notebook to provide the password for the user \"neo4j\". \n",
    "# It is not recommended to hardcode the password into jupyter notebook for security reasons.\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\n",
    "    uri=\"bolt://localhost:7687\", \n",
    "    auth=(\"neo4j\", os.environ.get(\"NEO4J_INITIAL_PASSWORD\"))\n",
    ")\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cypher_to_data_frame(query: typing.LiteralString, parameters: typing.Optional[typing.Dict[str, typing.Any]] = None):\n",
    "    records, summary, keys = driver.execute_query(query, parameters_=parameters)\n",
    "    return pd.DataFrame([record.values() for record in records], columns=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7656bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotation_style: dict = {\n",
    "    'textcoords': 'offset points',\n",
    "    'arrowprops': dict(arrowstyle='->', color='black', alpha=0.3),\n",
    "    'fontsize': 6,\n",
    "    'backgroundcolor': 'white',\n",
    "    'bbox': dict(boxstyle='round,pad=0.4',\n",
    "                    edgecolor='silver',\n",
    "                    facecolor='whitesmoke',\n",
    "                    alpha=1\n",
    "                )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dde866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Settings\n",
    "\n",
    "# -> Feature \"outgoingDependencies\" is left in even though it is highly correlated with the feature \"degree\" and isn't important for anomaly decisions according to SHAP. Still, the F1 score of the model is improved by it. \n",
    "# -> Feature \"articleRank\" is left in even though it is highly correlated with the feature \"incomingDependencies\". It is important for anomaly decisions according to SHAP and improves the F1 score. \n",
    "features_for_visualization_excluded_from_training: typing.List[str] = [\n",
    "    'codeUnitName',\n",
    "    'shortCodeUnitName',\n",
    "    'projectName',\n",
    "    'clusterLabel',\n",
    "    'clusterSize',\n",
    "    'clusterMedoid',\n",
    "    'clusterNoise', # highly correlated with \"clusterApproximateOutlierScore\". doesn't improve F1 score of proxy model.\n",
    "    'embeddingVisualizationX',\n",
    "    'embeddingVisualizationY',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68aa20",
   "metadata": {},
   "source": [
    "## 1. Java Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927388f",
   "metadata": {},
   "source": [
    "### 1.1 Query Features\n",
    "\n",
    "Query all features that are relevant for anomaly detection. Some of them come from precalculated clustering (HDBSCAN), node embeddings (Fast Random Projection), community detection algorithms (Leiden, Local Clustering Coefficient), centrality algorithms (Page Rank, Article Rank, Betweenness) and classical metrics like the in-/out-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Package)\n",
    "    WHERE codeUnit.incomingDependencies                              IS NOT NULL\n",
    "      AND codeUnit.outgoingDependencies                              IS NOT NULL\n",
    "      and codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "      AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationX       IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationY       IS NOT NULL\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                                  AS codeUnitName\n",
    "        ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "        ,artifact.name                                                 AS projectName\n",
    "        ,codeUnit.incomingDependencies                                 AS incomingDependencies\n",
    "        ,codeUnit.outgoingDependencies                                 AS outgoingDependencies\n",
    "        ,codeUnit.incomingDependencies + codeUnit.outgoingDependencies AS degree\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "        ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "        ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "        ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "        ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "        ,codeUnit.communityLocalClusteringCoefficient                  AS locallusteringCoefficient\n",
    "        ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "        ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "        ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "        ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "        ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "        ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "        ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationX          AS embeddingVisualizationX\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationY          AS embeddingVisualizationY\n",
    "\"\"\"\n",
    "\n",
    "java_package_anomaly_detection_features = query_cypher_to_data_frame(java_package_anomaly_detection_features_query)\n",
    "java_package_features_to_standardize = java_package_anomaly_detection_features.columns.drop(features_for_visualization_excluded_from_training + ['embedding']).to_list()\n",
    "\n",
    "display(java_package_anomaly_detection_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f1415",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "Prepare the data by standardizing numeric fields and reducing the dimensionality of the node embeddings to not dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(features: pd.DataFrame) -> None:\n",
    "    if features.empty:\n",
    "        print(\"Data Validation Info: No data\")\n",
    "\n",
    "    if features.isnull().values.any():\n",
    "        raise RuntimeError(\"Data Validation Error: Some values are null. Fix the wrong values or filter them out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between features for debugging, troubleshooting, and detailed analysis.\n",
    "\n",
    "def plot_feature_correlation_matrix(features: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plots the correlation matrix of the features in the DataFrame.\n",
    "    \n",
    "    :param java_package_anomaly_detection_features: DataFrame containing the features.\n",
    "    :param java_package_features_to_standardize: List of feature names to include in the correlation matrix.\n",
    "    \"\"\"\n",
    "    correlation_matrix = features.corr()\n",
    "\n",
    "    figure, axis = plot.subplots(figsize=(8, 6))\n",
    "    color_axis = axis.matshow(correlation_matrix, cmap=\"coolwarm\")\n",
    "    figure.colorbar(color_axis)\n",
    "    axis.set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axis.set_yticks(range(len(correlation_matrix.index)))\n",
    "    axis.set_xticklabels(correlation_matrix.columns, rotation=90)\n",
    "    axis.set_yticklabels(correlation_matrix.index)\n",
    "    for (i, j), correlation_value in np.ndenumerate(correlation_matrix.values):\n",
    "        axis.text(j, i, f\"{correlation_value:.2f}\", ha='center', va='center', color='black', bbox=dict(facecolor='white', alpha=0.3, edgecolor='none'))\n",
    "    plot.title(\"Feature Correlation Matrix (excluding embeddings)\", fontsize=10)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a289a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_correlation_matrix(java_package_anomaly_detection_features[java_package_features_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(features: pd.DataFrame, feature_list: list[str]) -> numpy_typing.NDArray:\n",
    "    features_to_scale = features[feature_list]\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_standardized = standardize_features(java_package_anomaly_detection_features, java_package_features_to_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality_of_node_embeddings(\n",
    "        features: pd.DataFrame, \n",
    "        min_dimensions: int = 20, \n",
    "        max_dimensions: int = 40, \n",
    "        target_variance: float = 0.90,\n",
    "        embedding_column_name: str = 'embedding'\n",
    ") -> numpy_typing.NDArray:\n",
    "    \"\"\"\n",
    "    Automatically reduce the dimensionality of node embeddings using Principal Component Analysis (PCA)\n",
    "    to reach a target explained variance ratio with the lowest possible number of components (output dimensions).\n",
    "\n",
    "    Parameters:\n",
    "    - features (pd.DataFrame) with a column 'embedding', where every value contains a float array with original dimensions.\n",
    "    - min_dimensions: Even if possible with the given variance, don't go below this number of dimensions for the output\n",
    "    - max_dimensions: Return at most the max number of dimensions, even if that means, that the target variance can't be met.\n",
    "    - target_variance (float): Cumulative variance threshold (default: 0.90)\n",
    "    - embedding_column_name (string): Defaults to 'embedding'\n",
    "\n",
    "    Returns: Reduced embeddings as an numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the input and get the original dimension\n",
    "    embeddings = np.stack(features[embedding_column_name].apply(np.array).tolist())\n",
    "    original_dimension = embeddings.shape[1]\n",
    "\n",
    "    # Fit PCA without dimensionality reduction to get explained variance\n",
    "    full_principal_component_analysis_without_reduction = PCA()\n",
    "    full_principal_component_analysis_without_reduction.fit(embeddings)\n",
    "\n",
    "    # Find smallest number of components to reach target variance\n",
    "    cumulative_variance = np.cumsum(full_principal_component_analysis_without_reduction.explained_variance_ratio_)\n",
    "    best_n_components = np.searchsorted(cumulative_variance, target_variance) + 1\n",
    "    best_n_components = max(best_n_components, min_dimensions) # Use at least min_dimensions\n",
    "    best_n_components = min(best_n_components, max_dimensions) # Use at most max_dimensions\n",
    "\n",
    "    # Apply PCA with optimal number of components\n",
    "    principal_component_analysis = PCA(n_components=best_n_components)\n",
    "    java_type_anomaly_detection_node_embeddings_reduced = principal_component_analysis.fit_transform(embeddings)\n",
    "\n",
    "    explained_variance_ratio_sum = sum(principal_component_analysis.explained_variance_ratio_)\n",
    "    print(f\"Dimensionality reduction from {original_dimension} to {best_n_components} (min {min_dimensions}, max {max_dimensions}) of node embeddings using Principal Component Analysis (PCA): Explained variance is {explained_variance_ratio_sum:.4f}.\")\n",
    "\n",
    "    return java_type_anomaly_detection_node_embeddings_reduced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a33f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_node_embeddings_reduced = reduce_dimensionality_of_node_embeddings(java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_prepared = np.hstack([java_package_anomaly_detection_features_standardized, java_package_anomaly_detection_node_embeddings_reduced])\n",
    "java_package_anomaly_detection_feature_names = list(java_package_features_to_standardize) + [f'pca_{i}' for i in range(java_package_anomaly_detection_node_embeddings_reduced.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e72e7",
   "metadata": {},
   "source": [
    "### 1.3 List the top 10 anomalies found using Isolation Forest\n",
    "\n",
    "> The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_optuna_tuning_results(optimized_study: Study, name_of_the_optimized_algorithm: str):\n",
    "\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} parameters (Optuna):\", optimized_study.best_params)\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} score:\", optimized_study.best_value)\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} parameter influence:\", get_param_importances(optimized_study, evaluator=MeanDecreaseImpurityImportanceEvaluator()))\n",
    "    \n",
    "    valid_trials = [trial for trial in optimized_study.trials if trial.value is not None and trial.state == TrialState.COMPLETE]\n",
    "    top_trials = sorted(valid_trials, key=lambda t: typing.cast(float, t.value), reverse=True)[:10]\n",
    "    for i, trial in enumerate(top_trials):\n",
    "        print(f\"Best {name_of_the_optimized_algorithm} parameter rank: {i+1}, trial: {trial.number}, Value = {trial.value:.6f}, Params: {trial.params}\")\n",
    "\n",
    "\n",
    "class AnomalyDetectionResults:\n",
    "    def __init__(self, \n",
    "                 anomaly_labels: np.ndarray, \n",
    "                 anomaly_scores: np.ndarray, \n",
    "                 random_forest_classifier: RandomForestClassifier,\n",
    "                 feature_importances: np.ndarray\n",
    "                 ):\n",
    "        self.anomaly_labels = anomaly_labels\n",
    "        self.anomaly_scores = anomaly_scores\n",
    "        self.random_forest_classifier = random_forest_classifier\n",
    "        self.feature_importances = feature_importances\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f\"AnomalyDetectionResults(anomaly_labels={self.anomaly_labels.shape}, \"\n",
    "                f\"anomaly_scores={self.anomaly_scores.shape}, \"\n",
    "                f\"random_forest_classifier={type(self.random_forest_classifier).__name__}, \"\n",
    "                f\"feature_importances={self.feature_importances.shape})\")\n",
    "\n",
    "\n",
    "def tune_anomaly_detection_models(\n",
    "    feature_matrix: np.ndarray,\n",
    "    contamination: float | typing.Literal[\"auto\"] = 0.05,\n",
    "    random_seed: int = 42,\n",
    "    number_of_trials: int = 20,\n",
    "    optimization_timeout_in_seconds: int = 40,\n",
    "    number_of_cross_validation_folds: int = 3,\n",
    ") -> AnomalyDetectionResults:\n",
    "    \"\"\"\n",
    "    Tunes both Isolation Forest and a proxy Random Forest using Optuna, maximizing the F1 score\n",
    "    between Isolation Forest pseudo-labels and proxy predictions. The proxy model mimics the behavior of the Isolation Forest \n",
    "    and is mainly used to provide feature importances and explainability using SHAP values later.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_matrix: np.ndarray of shape (n_samples, n_features), preprocessed input features.\n",
    "    - contamination: favor only a few suspicious cases with a fixed percentage\n",
    "    - random_seed: seed for reproducibility.\n",
    "    - number_of_trials: number of Optuna optimization trials.\n",
    "    - number_of_cross_validation_folds: number of cross validation (CV) folds for proxy model validation.\n",
    "\n",
    "    Returns:\n",
    "    - AnomalyDetectionResults containing:\n",
    "        - anomaly_labels: np.ndarray of shape (n_samples,), binary labels indicating anomalies (1) or normal (0).\n",
    "        - anomaly_scores: np.ndarray of shape (n_samples,), anomaly scores where higher values indicate more anomalous instances.\n",
    "        - random_forest_classifier: trained Random Forest classifier that mimics the Isolation Forest behavior.\n",
    "        - feature_importances: np.ndarray of shape (n_features,), feature importances from the Random Forest classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def objective(trial) -> float:\n",
    "        # Isolation Forest hyperparameters\n",
    "        isolation_forest = IsolationForest(\n",
    "            max_samples=trial.suggest_float(\"isolation_max_samples\", 0.1, 1.0),\n",
    "            contamination=contamination, \n",
    "            n_estimators=trial.suggest_int(\"isolation_n_estimators\", 100, 400),\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        # Train Isolation Forest\n",
    "        isolation_forest.fit(feature_matrix)\n",
    "\n",
    "        # Generate pseudo-labels: 1 = anomaly, 0 = normal\n",
    "        pseudo_labels = (isolation_forest.predict(feature_matrix) == -1).astype(int)\n",
    "\n",
    "        # Proxy Random Forest hyperparameters\n",
    "        proxy_random_forest = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"proxy_n_estimators\", 50, 300),\n",
    "            max_depth=trial.suggest_int(\"proxy_max_depth\", 3, 20),\n",
    "            random_state=random_seed,\n",
    "        )\n",
    "\n",
    "        # Train proxy model\n",
    "        # Use cross-validation to get robust F1 score\n",
    "        f1_scores = cross_val_score(\n",
    "            proxy_random_forest,\n",
    "            feature_matrix,\n",
    "            pseudo_labels,\n",
    "            cv=number_of_cross_validation_folds,\n",
    "            scoring=\"f1\",\n",
    "        )\n",
    "        return float(np.mean(f1_scores))\n",
    "\n",
    "\n",
    "    # Print the number of samples and features in the feature matrix\n",
    "    n_samples = feature_matrix.shape[0]\n",
    "    print(f\"Tuned Anomaly Detection: Number of samples: {n_samples}, Number of features: {feature_matrix.shape[1]}, Number of trials: {number_of_trials}\")\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = create_study(direction=\"maximize\", sampler=TPESampler(seed=random_seed), study_name=\"AnomalyDetection_Tuning\")\n",
    "\n",
    "    # Try (enqueue) default settings\n",
    "    study.enqueue_trial({'isolation_max_samples': min(256, n_samples) / n_samples, 'isolation_n_estimators': 100, 'proxy_n_estimators': 100})\n",
    "    # Try (enqueue) some specific settings that led to good results during experiments\n",
    "    study.enqueue_trial({'isolation_max_samples': 0.5492229999946834, 'isolation_n_estimators': 162, 'proxy_n_estimators': 153, 'proxy_max_depth': 10})\n",
    "    study.enqueue_trial({'isolation_max_samples': 0.29110519961044856, 'isolation_n_estimators': 136, 'proxy_n_estimators': 77, 'proxy_max_depth': 8})\n",
    "    \n",
    "    study.enqueue_trial({'isolation_max_samples': 0.11350593116659227, 'isolation_n_estimators': 215, 'proxy_n_estimators': 112, 'proxy_max_depth': 15})\n",
    "    study.enqueue_trial({'isolation_max_samples': 0.2646104863448817, 'isolation_n_estimators': 185, 'proxy_n_estimators': 109, 'proxy_max_depth': 8})\n",
    "\n",
    "    study.optimize(objective, n_trials=number_of_trials, timeout=optimization_timeout_in_seconds)\n",
    "    output_optuna_tuning_results(study, study.study_name)\n",
    "\n",
    "    # Train best models from best params\n",
    "    best_params = study.best_params\n",
    "\n",
    "    best_isolation_forest = IsolationForest(\n",
    "        n_estimators=best_params[\"isolation_n_estimators\"],\n",
    "        max_samples=best_params[\"isolation_max_samples\"],\n",
    "        contamination=contamination,\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    anomaly_detection_results = best_isolation_forest.fit_predict(feature_matrix)\n",
    "    anomaly_labels = (anomaly_detection_results == -1).astype(int) # 1 = anomaly, 0 = normal\n",
    "    anomaly_scores = best_isolation_forest.decision_function(feature_matrix) * -1  # higher = more anomalous\n",
    "\n",
    "    best_proxy_random_forest = RandomForestClassifier(\n",
    "        n_estimators=best_params[\"proxy_n_estimators\"],\n",
    "        max_depth=best_params[\"proxy_max_depth\"],\n",
    "        random_state=random_seed\n",
    "    )\n",
    "    best_proxy_random_forest.fit(feature_matrix, anomaly_detection_results)\n",
    "\n",
    "    return AnomalyDetectionResults(anomaly_labels, anomaly_scores, best_proxy_random_forest, best_proxy_random_forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff20e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_anomaly_detection_results_to_features(\n",
    "    features: pd.DataFrame,\n",
    "    anomaly_detection_results: AnomalyDetectionResults,\n",
    "    anomaly_label_column: str = 'anomalyLabel',\n",
    "    anomaly_score_column: str = 'anomalyScore',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds anomaly detection results to the feature and returns the updated dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - features: pd.DataFrame of shape (n_samples, n_features).\n",
    "    - anomaly_detection_results: AnomalyDetectionResults object containing labels and scores.\n",
    "    - anomaly_label_column: Name for the anomaly label column.\n",
    "    - anomaly_score_column: Name for the anomaly score column.\n",
    "\n",
    "    Returns:\n",
    "    - Updated feature dataframe with anomaly labels and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add anomaly labels and scores to the feature matrix\n",
    "    features[anomaly_label_column] = anomaly_detection_results.anomaly_labels\n",
    "    features[anomaly_score_column] = anomaly_detection_results.anomaly_scores\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_anomalies(\n",
    "        anomaly_detected_features: pd.DataFrame, \n",
    "        anomaly_label_column: str = \"anomalyLabel\",\n",
    "        anomaly_score_column: str = \"anomalyScore\"\n",
    ") -> pd.DataFrame:\n",
    "    anomalies = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] == 1]\n",
    "    return anomalies.sort_values(by=anomaly_score_column, ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_results = tune_anomaly_detection_models(java_package_anomaly_detection_features_prepared)\n",
    "java_package_anomaly_detection_features = add_anomaly_detection_results_to_features(java_package_anomaly_detection_features, java_package_anomaly_detection_results)\n",
    "display(get_top_10_anomalies(java_package_anomaly_detection_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfcc56",
   "metadata": {},
   "source": [
    "#### 1.3.b List the top 10 anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10830e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_embedding_anomaly_detection_features = java_package_anomaly_detection_features[features_for_visualization_excluded_from_training + ['embedding', 'pageRank']].copy()\n",
    "java_package_embedding_anomaly_detection_input = reduce_dimensionality_of_node_embeddings(java_package_embedding_anomaly_detection_features, max_dimensions=60, target_variance=0.95)\n",
    "java_package_embedding_anomaly_detection_feature_names = embedding_feature_names = [f'pca_{i}' for i in range(java_package_embedding_anomaly_detection_input.shape[1])]\n",
    "java_package_embedding_anomaly_detection_result = tune_anomaly_detection_models(java_package_embedding_anomaly_detection_input, contamination=\"auto\")\n",
    "java_package_embedding_anomaly_detection_features = add_anomaly_detection_results_to_features(java_package_embedding_anomaly_detection_features, java_package_embedding_anomaly_detection_result, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore')\n",
    "\n",
    "display(get_top_10_anomalies(java_package_embedding_anomaly_detection_features, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3936d79",
   "metadata": {},
   "source": [
    "### 1.4 Plot anomalies\n",
    "\n",
    "Plots clustered nodes and highlights anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5604735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    title_prefix: str,\n",
    "    code_unit_column: str = \"shortCodeUnitName\",\n",
    "    cluster_label_column: str = \"clusterLabel\",\n",
    "    cluster_medoid_column: str = \"clusterMedoid\",\n",
    "    cluster_size_column: str = \"clusterSize\",\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    "    anomaly_score_column: str = \"anomalyScore\",\n",
    "    page_rank_column: str = \"pageRank\",\n",
    "    x_position_column: str = 'embeddingVisualizationX',\n",
    "    y_position_column: str = 'embeddingVisualizationY',\n",
    ") -> None:\n",
    "    \n",
    "    if clustering_visualization_dataframe.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "    \n",
    "    def truncate(text: str, max_length: int):\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length - 3] + \"...\"\n",
    "    \n",
    "    cluster_anomalies = clustering_visualization_dataframe[clustering_visualization_dataframe[anomaly_label_column] == 1]\n",
    "    cluster_without_anomalies = clustering_visualization_dataframe[clustering_visualization_dataframe[anomaly_label_column] != 1]\n",
    "    cluster_noise = cluster_without_anomalies[cluster_without_anomalies[cluster_label_column] == -1]\n",
    "    cluster_non_noise = cluster_without_anomalies[cluster_without_anomalies[cluster_label_column] != -1]\n",
    "\n",
    "    plot.figure(figsize=(10, 10))\n",
    "    plot.title(title_prefix + ' (size=PageRank, color=ClusterLabel, red=Anomaly)')\n",
    "\n",
    "    # Plot noise\n",
    "    plot.scatter(\n",
    "        x=cluster_noise[x_position_column],\n",
    "        y=cluster_noise[y_position_column],\n",
    "        s=cluster_noise[page_rank_column] * 200 + 4,\n",
    "        color='lightgrey',\n",
    "        alpha=0.5,\n",
    "        label='Noise'\n",
    "    )\n",
    "\n",
    "    # Plot clusters\n",
    "    plot.scatter(\n",
    "        x=cluster_non_noise[x_position_column],\n",
    "        y=cluster_non_noise[y_position_column],\n",
    "        s=cluster_non_noise[page_rank_column] * 200 + 4,\n",
    "        c=cluster_non_noise[cluster_label_column],\n",
    "        cmap='tab20',\n",
    "        alpha=0.7,\n",
    "        label='Clusters'\n",
    "    )\n",
    "\n",
    "    # Plot anomalies\n",
    "    plot.scatter(\n",
    "        x=cluster_anomalies[x_position_column],\n",
    "        y=cluster_anomalies[y_position_column],\n",
    "        s=cluster_anomalies[page_rank_column] * 200 + 4,\n",
    "        c=cluster_anomalies[anomaly_score_column],\n",
    "        cmap=\"Reds\",\n",
    "        alpha=0.9,\n",
    "        label='Anomaly'\n",
    "    )\n",
    "\n",
    "    # Annotate medoids of the cluster\n",
    "    cluster_medoids = cluster_non_noise[cluster_non_noise[cluster_medoid_column] == 1].sort_values(by=cluster_size_column, ascending=False).head(20)\n",
    "    for index, row in cluster_medoids.iterrows():\n",
    "        plot.annotate(\n",
    "            text=f\"{row[cluster_label_column]}:{truncate(row[code_unit_column], 20)} ({row[anomaly_score_column]:.4f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5),\n",
    "            alpha=0.4,\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    anomalies = cluster_anomalies.sort_values(by=anomaly_score_column, ascending=False).reset_index(drop=True).head(10)\n",
    "    for dataframe_index, row in anomalies.iterrows():\n",
    "        index = typing.cast(int, dataframe_index)\n",
    "        plot.annotate(\n",
    "            text=f\"{row[cluster_label_column]}:{truncate(row[code_unit_column], 20)} ({row[anomaly_score_column]:.4f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5 + (index % 5) * 10),\n",
    "            color='red',\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(java_package_anomaly_detection_features, title_prefix=\"Java Package Anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b08b6",
   "metadata": {},
   "source": [
    "#### 1.4.b Plot anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c08da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(\n",
    "    java_package_embedding_anomaly_detection_features, \n",
    "    title_prefix=\"Java Package Anomalies (Embeddings Only)\",\n",
    "    anomaly_label_column='anomalyOfEmbeddingLabel',\n",
    "    anomaly_score_column='anomalyOfEmbeddingScore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa822ca",
   "metadata": {},
   "source": [
    "### 1.5 Print the 20 most influential features (no explainable AI yet)\n",
    "\n",
    "Use Random Forest as a proxy to estimate the importance of each feature contributing to the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b21d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_importances_series = pd.Series(java_package_anomaly_detection_results.feature_importances, index=java_package_anomaly_detection_feature_names).sort_values(ascending=False)\n",
    "print(java_package_anomaly_detection_importances_series.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03216e",
   "metadata": {},
   "source": [
    "### 1.6 Use SHAP to explain the Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_anomalies_with_shap(\n",
    "        random_forest_model: RandomForestClassifier,\n",
    "        prepared_features: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use SHAP to explain the detected anomalies.\n",
    "    \"\"\"\n",
    "    # Use TreeExplainer on Random Forest Proxy Model\n",
    "    # This is necessary because Isolation Forest does not support SHAP explanations directly.\n",
    "    explainer = shap.TreeExplainer(random_forest_model)\n",
    "    shap_values = explainer.shap_values(prepared_features)\n",
    "    print(f\"Input data with prepared features: shape={prepared_features.shape}\")\n",
    "    print(f\"Explainable AI SHAP result: shape={np.shape(shap_values)}\")\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_explained_global_feature_importance(\n",
    "    shap_values: numpy_typing.NDArray,\n",
    "    anomaly_detected_features: pd.DataFrame,\n",
    "    prepared_features: numpy_typing.NDArray,\n",
    "    feature_names: list[str],\n",
    "    title_prefix: str = \"\",\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Explain anomalies using SHAP values and plot the global feature importance.\n",
    "    This function uses the SHAP library to visualize the impact of features on the model's predictions\n",
    "    for anomalies detected by the Isolation Forest model.\n",
    "    It generates a bar plot showing the most influential features for the anomalies.\n",
    "    \"\"\"\n",
    "\n",
    "    anomaly_rows = anomaly_detected_features[anomaly_label_column] == 1 # Filter anomalies\n",
    "    shap.summary_plot(\n",
    "        shap_values[anomaly_rows, :, 1],  # Class 1 = anomaly\n",
    "        prepared_features[anomaly_rows],\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        title=f\"{title_prefix} Anomalies explained using SHAP\",\n",
    "        max_display=20,\n",
    "        plot_size=(12, 6)  # (width, height) in inches\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d671e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_shap_values = explain_anomalies_with_shap(\n",
    "    # random_forest_model=java_package_proxy_random_forest,\n",
    "    random_forest_model=java_package_anomaly_detection_results.random_forest_classifier,\n",
    "    prepared_features=java_package_anomaly_detection_features_prepared\n",
    ")\n",
    "plot_shap_explained_global_feature_importance(\n",
    "    shap_values=java_package_shap_values,\n",
    "    anomaly_detected_features=java_package_anomaly_detection_features, \n",
    "    prepared_features=java_package_anomaly_detection_features_prepared,\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33560",
   "metadata": {},
   "source": [
    "### 1.7 Add the top SHAP features and their scores to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_shap_features_to_anomalies(\n",
    "    shap_values_array: np.ndarray,\n",
    "    feature_names: list[str],\n",
    "    anomaly_detected_features: pd.DataFrame,\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    "    top_n: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds top-N SHAP features and their SHAP values for each anomaly in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - shap_values_array: SHAP values array with shape (n_samples, n_features).\n",
    "    - feature_names: List of names corresponding to the features.\n",
    "    - anomaly_detected_features: Original DataFrame containing anomaly labels.\n",
    "    - anomaly_label_column: Name of the column indicating anomalies (1 = anomaly).\n",
    "    - top_n: Number of top influential features to extract.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with additional columns:\n",
    "      anomalyTopFeature_1, ..., topFeature_N\n",
    "      anomalyTopFeatureSHAPValue_, ..., topFeatureValue_N\n",
    "    \"\"\"\n",
    "    # Convert SHAP values to DataFrame for easier processing\n",
    "    shap_dataframe = pd.DataFrame(shap_values_array, columns=feature_names)\n",
    "\n",
    "    # Get indices of rows marked as anomalies\n",
    "    anomaly_indices = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] == 1].index\n",
    "\n",
    "    # Initialize result columns\n",
    "    for rank in range(1, top_n + 1):\n",
    "        anomaly_detected_features[f\"anomalyTopFeature_{rank}\"] = \"\"\n",
    "        anomaly_detected_features[f\"anomalyTopFeatureSHAPValue_{rank}\"] = 0.0\n",
    "\n",
    "    # Loop over each anomaly and assign top features + SHAP values\n",
    "    for index in anomaly_indices:\n",
    "        row_values = shap_dataframe.loc[index]\n",
    "        top_features = row_values.abs().sort_values(ascending=False).head(top_n)\n",
    "        for rank, (feature_name, shap_value) in enumerate(row_values[top_features.index].items(), 1):\n",
    "            anomaly_detected_features.at[index, f\"anomalyTopFeature_{rank}\"] = feature_name\n",
    "            anomaly_detected_features.at[index, f\"anomalyTopFeatureSHAPValue_{rank}\"] = shap_value\n",
    "\n",
    "    return anomaly_detected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_top_shap_features_to_anomalies(\n",
    "    shap_values_array=java_package_shap_values[:, :, 1],\n",
    "    feature_names=java_package_anomaly_detection_feature_names,\n",
    "    anomaly_detected_features=java_package_anomaly_detection_features\n",
    ")\n",
    "display(java_package_anomaly_detection_features[java_package_anomaly_detection_features[\"anomalyLabel\"] == 1].sort_values(by='anomalyScore', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682bb64",
   "metadata": {},
   "source": [
    "## 2. Java Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25370d7f",
   "metadata": {},
   "source": [
    "### 2.1 Query Features\n",
    "\n",
    "Query all features that are relevant for anomaly detection. Some of them come from precalculated clustering (HDBSCAN), node embeddings (Fast Random Projection), community detection algorithms (Leiden, Local Clustering Coefficient), centrality algorithms (Page Rank, Article Rank, Betweenness) and classical metrics like the in-/out-degree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Type)\n",
    "    WHERE codeUnit.incomingDependencies                              IS NOT NULL\n",
    "      AND codeUnit.outgoingDependencies                              IS NOT NULL\n",
    "      and codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "      AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationX       IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationY       IS NOT NULL\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                                  AS codeUnitName\n",
    "        ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "        ,artifact.name                                                 AS projectName\n",
    "        ,codeUnit.incomingDependencies                                 AS incomingDependencies\n",
    "        ,codeUnit.outgoingDependencies                                 AS outgoingDependencies\n",
    "        ,codeUnit.incomingDependencies + codeUnit.outgoingDependencies AS degree\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "        ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "        ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "        ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "        ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "        ,codeUnit.communityLocalClusteringCoefficient                  AS locallusteringCoefficient\n",
    "        ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "        ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "        ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "        ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "        ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "        ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "        ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationX          AS embeddingVisualizationX\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationY          AS embeddingVisualizationY\n",
    "\"\"\"\n",
    "\n",
    "java_type_anomaly_detection_features = query_cypher_to_data_frame(java_type_anomaly_detection_features_query)\n",
    "java_type_features_to_standardize = java_type_anomaly_detection_features.columns.drop(features_for_visualization_excluded_from_training + ['embedding']).to_list()\n",
    "\n",
    "display(java_type_anomaly_detection_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c67ed0",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "Prepare the data by standardizing numeric fields and reducing the dimensionality of the node embeddings to not dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681090a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(java_type_anomaly_detection_features)\n",
    "java_type_anomaly_detection_features_standardized = standardize_features(java_type_anomaly_detection_features, java_type_features_to_standardize)\n",
    "java_type_anomaly_detection_node_embeddings_reduced = reduce_dimensionality_of_node_embeddings(java_type_anomaly_detection_features)\n",
    "\n",
    "java_type_anomaly_detection_features_prepared = np.hstack([java_type_anomaly_detection_features_standardized, java_type_anomaly_detection_node_embeddings_reduced])\n",
    "java_type_anomaly_detection_feature_names = list(java_type_features_to_standardize) + [f'pca_{i}' for i in range(java_type_anomaly_detection_node_embeddings_reduced.shape[1])]\n",
    "\n",
    "plot_feature_correlation_matrix(java_type_anomaly_detection_features[java_type_features_to_standardize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7ac1b",
   "metadata": {},
   "source": [
    "### 2.3 List the top 10 anomalies found using Isolation Forest\n",
    "\n",
    "> The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_results = tune_anomaly_detection_models(java_type_anomaly_detection_features_prepared)\n",
    "java_type_anomaly_detection_features = add_anomaly_detection_results_to_features(java_type_anomaly_detection_features, java_type_anomaly_detection_results)\n",
    "display(get_top_10_anomalies(java_type_anomaly_detection_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314821d",
   "metadata": {},
   "source": [
    "#### 2.3.b List the top 10 anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab174c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_embedding_anomaly_detection_features = java_type_anomaly_detection_features[features_for_visualization_excluded_from_training + ['embedding', 'pageRank']].copy()\n",
    "java_type_embedding_anomaly_detection_input = reduce_dimensionality_of_node_embeddings(java_type_embedding_anomaly_detection_features, max_dimensions=60, target_variance=0.95)\n",
    "java_type_embedding_anomaly_detection_feature_names = embedding_feature_names = [f'pca_{i}' for i in range(java_type_embedding_anomaly_detection_input.shape[1])]\n",
    "java_type_embedding_anomaly_detection_result = tune_anomaly_detection_models(java_type_embedding_anomaly_detection_input, contamination=\"auto\")\n",
    "java_type_embedding_anomaly_detection_features = add_anomaly_detection_results_to_features(java_type_embedding_anomaly_detection_features, java_type_embedding_anomaly_detection_result, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore')\n",
    "\n",
    "display(get_top_10_anomalies(java_type_embedding_anomaly_detection_features, anomaly_label_column='anomalyOfEmbeddingLabel', anomaly_score_column='anomalyOfEmbeddingScore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a00628",
   "metadata": {},
   "source": [
    "### 2.4. Plot anomalies\n",
    "\n",
    "Plots clustered nodes and highlights anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(java_type_anomaly_detection_features, title_prefix=\"Java Type Anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05275be7",
   "metadata": {},
   "source": [
    "#### 2.4.b Plot anomalies solely based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c98022",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(\n",
    "    java_type_embedding_anomaly_detection_features, \n",
    "    title_prefix=\"Java Type Anomalies (Embeddings Only)\",\n",
    "    anomaly_label_column='anomalyOfEmbeddingLabel',\n",
    "    anomaly_score_column='anomalyOfEmbeddingScore'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e565f84",
   "metadata": {},
   "source": [
    "### 2.5 Print the 20 most influential features (no explainable AI yet)\n",
    "\n",
    "Use Random Forest as a proxy to estimate the importance of each feature contributing to the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86945e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_importances_series = pd.Series(java_type_anomaly_detection_results.feature_importances, index=java_type_anomaly_detection_feature_names).sort_values(ascending=False)\n",
    "print(java_type_anomaly_detection_importances_series.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a0379",
   "metadata": {},
   "source": [
    "### 2.6 Use SHAP to explain the Isolation Forest Model via the Random Forest Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ead283",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_shap_values = explain_anomalies_with_shap(\n",
    "    random_forest_model=java_type_anomaly_detection_results.random_forest_classifier,\n",
    "    prepared_features=java_type_anomaly_detection_features_prepared\n",
    ")\n",
    "plot_shap_explained_global_feature_importance(\n",
    "    shap_values=java_type_shap_values,\n",
    "    anomaly_detected_features=java_type_anomaly_detection_features, \n",
    "    prepared_features=java_type_anomaly_detection_features_prepared,\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    title_prefix=\"Java Package\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a55b29",
   "metadata": {},
   "source": [
    "### 2.7 Add the top SHAP features and their scores to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_top_shap_features_to_anomalies(\n",
    "    shap_values_array=java_type_shap_values[:, :, 1],\n",
    "    feature_names=java_type_anomaly_detection_feature_names,\n",
    "    anomaly_detected_features=java_type_anomaly_detection_features\n",
    ")\n",
    "display(java_type_anomaly_detection_features[java_type_anomaly_detection_features[\"anomalyLabel\"] == 1].sort_values(by='anomalyScore', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "JohT"
   }
  ],
  "code_graph_analysis_pipeline_data_validation": "ValidateAlwaysFalse",
  "kernelspec": {
   "display_name": "codegraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Anomaly Detection - Manual Exploration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
