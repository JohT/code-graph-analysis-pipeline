{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0eabc4",
   "metadata": {},
   "source": [
    "# Anomaly Detection with Isolation Forest - Manual Exploration\n",
    "\n",
    "This notebook demonstrates anomaly detection with Isolation Forest for static code analysis gathered by using jQAssistant and Neo4j. The focus is on detecting anomalies in the data, which can be useful for identifying potential issues or areas for improvement in the codebase.\n",
    "\n",
    "<br>  \n",
    "\n",
    "### References\n",
    "- [jqassistant](https://jqassistant.org)\n",
    "- [Neo4j Python Driver](https://neo4j.com/docs/api/python-driver/current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1e4fb",
   "metadata": {},
   "source": [
    "## Features overview\n",
    "\n",
    "| **Feature**                      | **Type**           | **What it Measures**                        | **Why Itâ€™s Useful**                         |\n",
    "| -------------------------------- | ------------------ | ------------------------------------------- | ------------------------------------------- |\n",
    "| `PageRank`                       | Centrality         | Popularity / referenced code                | High = many dependents                      |\n",
    "| `ArticleRank`                    | Centrality         | How much the code depends on others         | High = high dependency                      |\n",
    "| `PageRank - ArticleRank`         | Relative Rank      | Role inversion / architectural layering     | Highlights mismatches                       |\n",
    "| `Betweenness Centrality`         | Centrality         | Bridge or control nodes                     | High = structural chokepoints               |\n",
    "| `Local Clustering Coefficient`   | Structural         | Local cohesion / modularity                 | Low = isolated node in a clique-like region |\n",
    "| `Degree` (Total and In/Out)      | Structural         | Connectivity                                | Raw values may dominate                     |\n",
    "| `Node Embedding` (PCA reduced)   | Latent             | Structural and semantic similarity          | Captures latent position in graph           |\n",
    "| `Normalized Cluster Distance`    | Geometric          | Relative to cluster radius                  | Adds context to position                    |\n",
    "| `1.0 - HDBSCAN membership probability` | Cluster Confidence | How confidently HDBSCAN clustered this node, 1-x inverted | High score = likely anomaly                   |\n",
    "| `Average Cluster Radius`          | Cluster Context    | How tight or spread out the cluster is         | Highly spread clusters may be a less meaningful one   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy.typing as numpy_typing\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell uses the build-in %html \"magic\" to override the CSS style for tables to a much smaller size.\n",
    "#This is especially needed for PDF export of tables with multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS style for smaller dataframe tables. */\n",
    ".dataframe th {\n",
    "    font-size: 8px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 8px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07319282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Colormap\n",
    "# main_color_map = 'nipy_spectral'\n",
    "main_color_map = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version as python_version\n",
    "print('Python version: {}'.format(python_version))\n",
    "\n",
    "from numpy import __version__ as numpy_version\n",
    "print('numpy version: {}'.format(numpy_version))\n",
    "\n",
    "from pandas import __version__ as pandas_version\n",
    "print('pandas version: {}'.format(pandas_version))\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('sklearn version: {}'.format(sklearn_version))\n",
    "\n",
    "from matplotlib import __version__ as matplotlib_version\n",
    "print('matplotlib version: {}'.format(matplotlib_version))\n",
    "\n",
    "from neo4j import __version__ as neo4j_version\n",
    "print('neo4j version: {}'.format(neo4j_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the environment variable \"NEO4J_INITIAL_PASSWORD\" in your shell \n",
    "# before starting jupyter notebook to provide the password for the user \"neo4j\". \n",
    "# It is not recommended to hardcode the password into jupyter notebook for security reasons.\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\n",
    "    uri=\"bolt://localhost:7687\", \n",
    "    auth=(\"neo4j\", os.environ.get(\"NEO4J_INITIAL_PASSWORD\"))\n",
    ")\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cypher_to_data_frame(query: typing.LiteralString, parameters: typing.Optional[typing.Dict[str, typing.Any]] = None):\n",
    "    records, summary, keys = driver.execute_query(query, parameters_=parameters)\n",
    "    return pd.DataFrame([record.values() for record in records], columns=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7656bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotation_style: dict = {\n",
    "    'textcoords': 'offset points',\n",
    "    'arrowprops': dict(arrowstyle='->', color='black', alpha=0.3),\n",
    "    'fontsize': 6,\n",
    "    'backgroundcolor': 'white',\n",
    "    'bbox': dict(boxstyle='round,pad=0.4',\n",
    "                    edgecolor='silver',\n",
    "                    facecolor='whitesmoke',\n",
    "                    alpha=1\n",
    "                )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68aa20",
   "metadata": {},
   "source": [
    "## 1. Java Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927388f",
   "metadata": {},
   "source": [
    "### 1.1 Query Features\n",
    "\n",
    "Query all features that are relevant for anomaly detection. Some of them come from precalculated clustering (HDBSCAN), node embeddings (Fast Random Projection), community detection algorithms (Leiden, Local Clustering Coefficient), centrality algorithms (Page Rank, Article Rank, Betweenness) and classical metrics like the in-/out-degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Package)\n",
    "    WHERE codeUnit.incomingDependencies                              IS NOT NULL\n",
    "      AND codeUnit.outgoingDependencies                              IS NOT NULL\n",
    "      and codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "      AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationX       IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationY       IS NOT NULL\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                                  AS codeUnitName\n",
    "        ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "        ,artifact.name                                                 AS projectName\n",
    "        ,codeUnit.incomingDependencies                                 AS incomingDependencies\n",
    "        ,codeUnit.outgoingDependencies                                 AS outgoingDependencies\n",
    "        ,codeUnit.incomingDependencies + codeUnit.outgoingDependencies AS degree\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "        ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "        ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "        ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "        ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "        ,codeUnit.communityLocalClusteringCoefficient                  AS locallusteringCoefficient\n",
    "        ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "        ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "        ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "        ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "        ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "        ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "        ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationX          AS embeddingVisualizationX\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationY          AS embeddingVisualizationY\n",
    "\"\"\"\n",
    "\n",
    "java_package_anomaly_detection_features = query_cypher_to_data_frame(java_package_anomaly_detection_features_query)\n",
    "java_package_features_to_standardize = java_package_anomaly_detection_features.columns.drop(['codeUnitName', 'shortCodeUnitName', 'projectName', 'embedding', 'clusterLabel', 'clusterSize', 'clusterMedoid', 'embeddingVisualizationX', 'embeddingVisualizationY']).to_list()\n",
    "\n",
    "display(java_package_anomaly_detection_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f1415",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "Prepare the data by standardizing numeric fields and reducing the dimensionality of the node embeddings to not dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(features: pd.DataFrame) -> None:\n",
    "    if features.empty:\n",
    "        print(\"Data Validation Info: No data\")\n",
    "\n",
    "    if features.isnull().values.any():\n",
    "        raise RuntimeError(\"Data Validation Error: Some values are null. Fix the wrong values or filter them out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(features: pd.DataFrame, feature_list: list[str]) -> numpy_typing.NDArray:\n",
    "    features_to_scale = features[feature_list]\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_standardized = standardize_features(java_package_anomaly_detection_features, java_package_features_to_standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality_of_node_embeddings(\n",
    "        features: pd.DataFrame, \n",
    "        min_dimensions: int = 20, \n",
    "        max_dimensions: int = 40, \n",
    "        target_variance: float = 0.90,\n",
    "        embedding_column_name: str = 'embedding'\n",
    ") -> numpy_typing.NDArray:\n",
    "    \"\"\"\n",
    "    Automatically reduce the dimensionality of node embeddings using Principal Component Analysis (PCA)\n",
    "    to reach a target explained variance ratio with the lowest possible number of components (output dimensions).\n",
    "\n",
    "    Parameters:\n",
    "    - features (pd.DataFrame) with a column 'embedding', where every value contains a float array with original dimensions.\n",
    "    - min_dimensions: Even if possible with the given variance, don't go below this number of dimensions for the output\n",
    "    - max_dimensions: Return at most the max number of dimensions, even if that means, that the target variance can't be met.\n",
    "    - target_variance (float): Cumulative variance threshold (default: 0.90)\n",
    "    - embedding_column_name (string): Defaults to 'embedding'\n",
    "\n",
    "    Returns: Reduced embeddings as an numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the input and get the original dimension\n",
    "    embeddings = np.stack(features[embedding_column_name].apply(np.array).tolist())\n",
    "    original_dimension = embeddings.shape[1]\n",
    "\n",
    "    # Fit PCA without dimensionality reduction to get explained variance\n",
    "    full_principal_component_analysis_without_reduction = PCA()\n",
    "    full_principal_component_analysis_without_reduction.fit(embeddings)\n",
    "\n",
    "    # Find smallest number of components to reach target variance\n",
    "    cumulative_variance = np.cumsum(full_principal_component_analysis_without_reduction.explained_variance_ratio_)\n",
    "    best_n_components = np.searchsorted(cumulative_variance, target_variance) + 1\n",
    "    best_n_components = max(best_n_components, min_dimensions) # Use at least min_dimensions\n",
    "    best_n_components = min(best_n_components, max_dimensions) # Use at most max_dimensions\n",
    "\n",
    "    # Apply PCA with optimal number of components\n",
    "    principal_component_analysis = PCA(n_components=best_n_components)\n",
    "    java_type_anomaly_detection_node_embeddings_reduced = principal_component_analysis.fit_transform(embeddings)\n",
    "\n",
    "    explained_variance_ratio_sum = sum(principal_component_analysis.explained_variance_ratio_)\n",
    "    print(f\"Dimensionality reduction from {original_dimension} to {best_n_components} (min {min_dimensions}) of node embeddings using Principal Component Analysis (PCA): Explained variance is {explained_variance_ratio_sum:.4f}.\")\n",
    "\n",
    "    return java_type_anomaly_detection_node_embeddings_reduced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a33f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_node_embeddings_reduced = reduce_dimensionality_of_node_embeddings(java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features_prepared = np.hstack([java_package_anomaly_detection_features_standardized, java_package_anomaly_detection_node_embeddings_reduced])\n",
    "java_package_anomaly_detection_feature_names = list(java_package_features_to_standardize) + [f'pca_{i}' for i in range(java_package_anomaly_detection_node_embeddings_reduced.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e72e7",
   "metadata": {},
   "source": [
    "### 1.3 List the top 10 anomalies found using Isolation Forest\n",
    "\n",
    "> The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(\n",
    "        prepared_features: numpy_typing.NDArray, \n",
    "        original_features: pd.DataFrame,\n",
    "        anomaly_label_column: str = 'anomalyLabel',\n",
    "        anomaly_score_column: str = 'anomalyScore',\n",
    ") -> pd.DataFrame:\n",
    "    isolation_forest = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
    "    anomaly_score = isolation_forest.fit_predict(prepared_features)\n",
    "\n",
    "    original_features[anomaly_label_column] = anomaly_score * -1 # 1 = anomaly, 0 = no anomaly\n",
    "    original_features[anomaly_score_column] = isolation_forest.decision_function(prepared_features) * -1  # higher = more anomalous\n",
    "    return original_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_features = detect_anomalies(java_package_anomaly_detection_features_prepared, java_package_anomaly_detection_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_anomalies(\n",
    "        anomaly_detected_features: pd.DataFrame, \n",
    "        anomaly_label_column: str = \"anomalyLabel\",\n",
    "        anomaly_score_column: str = \"anomalyScore\"\n",
    ") -> pd.DataFrame:\n",
    "    anomalies = anomaly_detected_features[anomaly_detected_features[anomaly_label_column] == -1]\n",
    "    return anomalies.sort_values(by=anomaly_score_column, ascending=False).reset_index(drop=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b43abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_top_10_anomalies(java_package_anomaly_detection_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa822ca",
   "metadata": {},
   "source": [
    "### 1.4 Plot the 20 most influential features\n",
    "\n",
    "Use Random Forest as a proxy to estimate the importance of each feature contributing to the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24427977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(\n",
    "        anomaly_detected_features: pd.DataFrame, \n",
    "        prepared_features: numpy_typing.NDArray,\n",
    "        anomaly_label_column: str = \"anomalyLabel\",\n",
    ") -> numpy_typing.NDArray:\n",
    "    \"\"\"\n",
    "    Use Random Forest as a proxy model to find out which are the most important features for the anomaly detection model (Isolation Forest).\n",
    "    This helps to see if embedding components dominate (top 10 filled with them), and then tune accordingly.\n",
    "    \"\"\"\n",
    "    # Use IsolationForest labels as a \"pseudo ground truth\"\n",
    "    y_pseudo = (anomaly_detected_features[anomaly_label_column] == -1).astype(int)\n",
    "\n",
    "    # Fit classifier to match the IF model\n",
    "    proxy_random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    proxy_random_forest.fit(prepared_features, y_pseudo)\n",
    "\n",
    "    return proxy_random_forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b21d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_anomaly_detection_importances = get_feature_importances(java_package_anomaly_detection_features, java_package_anomaly_detection_features_prepared)\n",
    "java_package_anomaly_detection_importances_series = pd.Series(java_package_anomaly_detection_importances, index=java_package_anomaly_detection_feature_names).sort_values(ascending=False)\n",
    "#display(java_type_anomaly_detection_importances_series.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(feature_importances_series: pd.Series, title_prefix: str) -> None:\n",
    "    feature_importances_series.head(20).plot(\n",
    "        kind='barh',\n",
    "        figsize=(10, 6),\n",
    "        color='skyblue',\n",
    "        title=f\"{title_prefix}: Top 20 Feature Importances (Random Forest Proxy)\",\n",
    "        xlabel=\"Importance\"\n",
    "    )\n",
    "    plot.gca().invert_yaxis() # Most important feature at the top\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(java_package_anomaly_detection_importances_series, title_prefix='Java Packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd6246",
   "metadata": {},
   "source": [
    "### 1.5. Plot anomalies\n",
    "\n",
    "Plots clustered nodes and highlights anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    title_prefix: str,\n",
    "    code_unit_column: str = \"shortCodeUnitName\",\n",
    "    cluster_label_column: str = \"clusterLabel\",\n",
    "    cluster_medoid_column: str = \"clusterMedoid\",\n",
    "    cluster_size_column: str = \"clusterSize\",\n",
    "    anomaly_label_column: str = \"anomalyLabel\",\n",
    "    anomaly_score_column: str = \"anomalyScore\",\n",
    "    page_rank_column: str = \"pageRank\",\n",
    "    x_position_column: str = 'embeddingVisualizationX',\n",
    "    y_position_column: str = 'embeddingVisualizationY',\n",
    ") -> None:\n",
    "    \n",
    "    if clustering_visualization_dataframe.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "    \n",
    "    def truncate(text: str, max_length: int):\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length - 3] + \"...\"\n",
    "    \n",
    "    cluster_anomalies = clustering_visualization_dataframe[clustering_visualization_dataframe[anomaly_label_column] == 1]\n",
    "    cluster_without_anomalies = clustering_visualization_dataframe[clustering_visualization_dataframe[anomaly_label_column] != 1]\n",
    "    cluster_noise = cluster_without_anomalies[cluster_without_anomalies[cluster_label_column] == -1]\n",
    "    cluster_non_noise = cluster_without_anomalies[cluster_without_anomalies[cluster_label_column] != -1]\n",
    "\n",
    "    plot.figure(figsize=(10, 10))\n",
    "    plot.title(title_prefix + ' (size=PageRank, color=ClusterLabel, red=Anomaly)')\n",
    "\n",
    "    # Plot noise\n",
    "    plot.scatter(\n",
    "        x=cluster_noise[x_position_column],\n",
    "        y=cluster_noise[y_position_column],\n",
    "        s=cluster_noise[page_rank_column] * 200 + 4,\n",
    "        color='lightgrey',\n",
    "        alpha=0.5,\n",
    "        label='Noise'\n",
    "    )\n",
    "\n",
    "    # Plot clusters\n",
    "    plot.scatter(\n",
    "        x=cluster_non_noise[x_position_column],\n",
    "        y=cluster_non_noise[y_position_column],\n",
    "        s=cluster_non_noise[page_rank_column] * 200 + 4,\n",
    "        c=cluster_non_noise[cluster_label_column],\n",
    "        cmap='tab20',\n",
    "        alpha=0.7,\n",
    "        label='Clusters'\n",
    "    )\n",
    "\n",
    "    # Plot anomalies\n",
    "    plot.scatter(\n",
    "        x=cluster_anomalies[x_position_column],\n",
    "        y=cluster_anomalies[y_position_column],\n",
    "        s=cluster_anomalies[page_rank_column] * 200 + 4,\n",
    "        c=cluster_anomalies[anomaly_score_column],\n",
    "        cmap=\"Reds\",\n",
    "        alpha=0.9,\n",
    "        label='Anomaly'\n",
    "    )\n",
    "\n",
    "    # Annotate medoids of the cluster\n",
    "    cluster_medoids = cluster_non_noise[cluster_non_noise[cluster_medoid_column] == 1].sort_values(by=cluster_size_column, ascending=False).head(20)\n",
    "    for index, row in cluster_medoids.iterrows():\n",
    "        plot.annotate(\n",
    "            text=f\"{row[cluster_label_column]}:{truncate(row[code_unit_column], 20)} ({row[anomaly_score_column]:.4f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5),\n",
    "            alpha=0.4,\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    anomalies = cluster_anomalies.sort_values(by=anomaly_score_column, ascending=False).reset_index(drop=True).head(6)\n",
    "    for dataframe_index, row in anomalies.iterrows():\n",
    "        index = typing.cast(int, dataframe_index)\n",
    "        plot.annotate(\n",
    "            text=f\"{row[cluster_label_column]}:{truncate(row[code_unit_column], 20)} ({row[anomaly_score_column]:.4f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5 + (index % 5) * 10),\n",
    "            color='red',\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea29887",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(java_package_anomaly_detection_features, title_prefix=\"Java Package Anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682bb64",
   "metadata": {},
   "source": [
    "## 2. Java Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25370d7f",
   "metadata": {},
   "source": [
    "### 2.1 Query Features\n",
    "\n",
    "Query all features that are relevant for anomaly detection. Some of them come from precalculated clustering (HDBSCAN), node embeddings (Fast Random Projection), community detection algorithms (Leiden, Local Clustering Coefficient), centrality algorithms (Page Rank, Article Rank, Betweenness) and classical metrics like the in-/out-degree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Type)\n",
    "    WHERE codeUnit.incomingDependencies                              IS NOT NULL\n",
    "      AND codeUnit.outgoingDependencies                              IS NOT NULL\n",
    "      and codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "      AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "      AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationX       IS NOT NULL\n",
    "      AND codeUnit.embeddingFastRandomProjectionVisualizationY       IS NOT NULL\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                                  AS codeUnitName\n",
    "        ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "        ,artifact.name                                                 AS projectName\n",
    "        ,codeUnit.incomingDependencies                                 AS incomingDependencies\n",
    "        ,codeUnit.outgoingDependencies                                 AS outgoingDependencies\n",
    "        ,codeUnit.incomingDependencies + codeUnit.outgoingDependencies AS degree\n",
    "        ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "        ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "        ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "        ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "        ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "        ,codeUnit.communityLocalClusteringCoefficient                  AS locallusteringCoefficient\n",
    "        ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "        ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "        ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "        ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "        ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "        ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "        ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationX          AS embeddingVisualizationX\n",
    "        ,codeUnit.embeddingFastRandomProjectionVisualizationY          AS embeddingVisualizationY\n",
    "\"\"\"\n",
    "\n",
    "java_type_anomaly_detection_features = query_cypher_to_data_frame(java_type_anomaly_detection_features_query)\n",
    "java_type_features_to_standardize = java_type_anomaly_detection_features.columns.drop(['codeUnitName', 'shortCodeUnitName', 'projectName', 'embedding', 'clusterLabel', 'clusterSize', 'clusterMedoid', 'embeddingVisualizationX', 'embeddingVisualizationY']).to_list()\n",
    "\n",
    "display(java_type_anomaly_detection_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c67ed0",
   "metadata": {},
   "source": [
    "### 1.2 Data preparation\n",
    "\n",
    "Prepare the data by standardizing numeric fields and reducing the dimensionality of the node embeddings to not dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681090a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data(java_type_anomaly_detection_features)\n",
    "java_type_anomaly_detection_features_standardized = standardize_features(java_type_anomaly_detection_features, java_type_features_to_standardize)\n",
    "java_type_anomaly_detection_node_embeddings_reduced = reduce_dimensionality_of_node_embeddings(java_type_anomaly_detection_features)\n",
    "\n",
    "java_type_anomaly_detection_features_prepared = np.hstack([java_type_anomaly_detection_features_standardized, java_type_anomaly_detection_node_embeddings_reduced])\n",
    "java_type_anomaly_detection_feature_names = list(java_type_features_to_standardize) + [f'pca_{i}' for i in range(java_type_anomaly_detection_node_embeddings_reduced.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7ac1b",
   "metadata": {},
   "source": [
    "### 2.3 List the top 10 anomalies found using Isolation Forest\n",
    "\n",
    "> The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_features = detect_anomalies(java_type_anomaly_detection_features_prepared, java_type_anomaly_detection_features)\n",
    "display(get_top_10_anomalies(java_type_anomaly_detection_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e565f84",
   "metadata": {},
   "source": [
    "### 2.4 Plot the 20 most influential features\n",
    "\n",
    "Use Random Forest as a proxy to estimate the importance of each feature contributing to the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_importances = get_feature_importances(java_type_anomaly_detection_features, java_type_anomaly_detection_features_prepared)\n",
    "java_type_anomaly_detection_importances_series = pd.Series(java_type_anomaly_detection_importances, index=java_type_anomaly_detection_feature_names).sort_values(ascending=False)\n",
    "#display(java_type_anomaly_detection_importances_series.head(10))\n",
    "\n",
    "plot_feature_importances(java_type_anomaly_detection_importances_series, title_prefix='Java Types')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a00628",
   "metadata": {},
   "source": [
    "### 2.5. Plot anomalies\n",
    "\n",
    "Plots clustered nodes and highlights anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_anomalies(java_type_anomaly_detection_features, title_prefix=\"Java Type Anomalies\")"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "JohT"
   }
  ],
  "code_graph_analysis_pipeline_data_validation": "ValidateAlwaysFalse",
  "kernelspec": {
   "display_name": "codegraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Anomaly Detection - Manual Exploration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
