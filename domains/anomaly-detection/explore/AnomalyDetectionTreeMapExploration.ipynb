{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0eabc4",
   "metadata": {},
   "source": [
    "# Anomaly Detection - TreeMap Exploration\n",
    "\n",
    "This notebook demonstrates how to visualize anomalies with Treemap charts for static code analysis data using jQAssistant and Neo4j. \n",
    "\n",
    "<br>  \n",
    "\n",
    "### References\n",
    "- [jqassistant](https://jqassistant.org)\n",
    "- [Neo4j Python Driver](https://neo4j.com/docs/api/python-driver/current)\n",
    "- [Plotly Treemap Chart](https://plotly.com/python/treemaps/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from typing import List, Tuple\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from plotly import graph_objects as plotly_graph_objects\n",
    "import plotly.colors as plotly_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell uses the build-in %html \"magic\" to override the CSS style for tables to a much smaller size.\n",
    "#This is especially needed for PDF export of tables with multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS style for smaller dataframe tables. */\n",
    ".dataframe th {\n",
    "    font-size: 8px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 8px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame Display Configuration\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version as python_version\n",
    "print('Python version: {}'.format(python_version))\n",
    "\n",
    "from numpy import __version__ as numpy_version\n",
    "print('numpy version: {}'.format(numpy_version))\n",
    "\n",
    "from pandas import __version__ as pandas_version\n",
    "print('pandas version: {}'.format(pandas_version))\n",
    "\n",
    "from neo4j import __version__ as neo4j_version\n",
    "print('neo4j version: {}'.format(neo4j_version))\n",
    "\n",
    "from plotly import version as plotly_version\n",
    "print('plotly version: {}'.format(plotly_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the environment variable \"NEO4J_INITIAL_PASSWORD\" in your shell \n",
    "# before starting jupyter notebook to provide the password for the user \"neo4j\". \n",
    "# It is not recommended to hardcode the password into jupyter notebook for security reasons.\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "def get_graph_database_driver() -> Driver:\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri=\"bolt://localhost:7687\",\n",
    "        auth=(\"neo4j\", os.environ.get(\"NEO4J_INITIAL_PASSWORD\")) # type: ignore\n",
    "    )\n",
    "    driver.verify_connectivity()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cypher_to_data_frame(query: typing.LiteralString, parameters: typing.Optional[typing.Dict[str, typing.Any]] = None):\n",
    "    records, summary, keys = driver.execute_query(query, parameters_=parameters)\n",
    "    return pd.DataFrame([record.values() for record in records], columns=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base settings for Plotly Treemap\n",
    "\n",
    "plotly_main_layout_base_settings = {\n",
    "    \"margin\": {\"t\": 50, \"l\": 15, \"r\": 15, \"b\": 15},\n",
    "}\n",
    "plotly_treemap_layout_base_settings = dict(\n",
    "    **plotly_main_layout_base_settings\n",
    ")\n",
    "plotly_bar_layout_base_settings = dict(\n",
    "    **plotly_main_layout_base_settings\n",
    ")\n",
    "plotly_treemap_figure_show_settings = {\n",
    "    \"renderer\": None,\n",
    "    \"width\": 1080,\n",
    "    \"height\": 1080,\n",
    "}\n",
    "\n",
    "plotly_treemap_marker_base_style = {\n",
    "    \"cornerradius\": 5,\n",
    "}\n",
    "\n",
    "#  Hot_r, ice_r, Viridis_r, speed_r, haline_r, thermal_r, Plasma_r, solar_r, Electric_r, Blackbody_r, deep_r, Turbo_r, amp, Reds, Blackbody_r, RdGy_r, RdBu_r\n",
    "plotly_treemap_marker_base_color_scale = dict(\n",
    "    **plotly_treemap_marker_base_style,\n",
    "    colorscale='Hot_r',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_treemap_settings(data_frame: pd.DataFrame, element_path_column: str = 'elementPath', element_name_column: str = \"elementName\") -> plotly_graph_objects.Treemap:\n",
    "    \"\"\"\n",
    "    Creates a Plotly Treemap with the given settings and data frame.\n",
    "    data_frame : pd.DataFrame : The input data frame\n",
    "    return :plotly_graph_objects.Treemap : The prepared Plotly Treemap\n",
    "    \"\"\"\n",
    "    return plotly_graph_objects.Treemap(\n",
    "        labels=data_frame[element_name_column],\n",
    "        parents=data_frame['directoryParentPath'],\n",
    "        ids=data_frame[element_path_column],\n",
    "        customdata=data_frame[['fileCount', 'absoluteAnomalyScore', 'normalizedAuthorityRank', 'normalizedBottleneckRank', 'normalizedBridgeRank', 'normalizedHubRank', 'normalizedOutlierRank', 'elementPath']],\n",
    "        hovertemplate='<b>%{label}</b><br>Highlighted anomalies: %{customdata[0]}<br>Anomaly Score: %{customdata[1]:.4f}<br>Authority: %{customdata[2]}, Bottleneck: %{customdata[3]}, Bridge: %{customdata[4]}, Hub: %{customdata[5]}, Outlier: %{customdata[6]}<br>Path: %{customdata[7]}',\n",
    "        maxdepth=-1,\n",
    "        root_color=\"lightgrey\",\n",
    "        marker=dict(**plotly_treemap_marker_base_style),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_path_file_extension(file_path_elements: list) -> list:\n",
    "    \"\"\"\n",
    "    Removes the file extension of the last element of the file path so that only the file name remains.\n",
    "    file_path_elements : list : The list of file path elements where the last one contains the file name with extension\n",
    "    return : list : The list of the directories + the file name without extension as last element.\n",
    "    \"\"\"\n",
    "    if not file_path_elements:\n",
    "        return ['']\n",
    "    if len(file_path_elements) == 1:\n",
    "        return [os.path.splitext(file_path_elements[0])[0]]\n",
    "    return file_path_elements[:-1] + [os.path.splitext(file_path_elements[-1])[0]]\n",
    "\n",
    "def join_path_elements(file_path_elements: list) -> list:\n",
    "    \"\"\"\n",
    "    Joins the file path elements (and removes the file extension).\n",
    "    file_path_elements : list : The list of levels to convert\n",
    "    return : list : The list of directories\n",
    "    \"\"\"\n",
    "    prepared_path_elements = remove_last_path_file_extension(file_path_elements)\n",
    "    return ['/'.join(prepared_path_elements[:i+1]) for i in range(len(prepared_path_elements))]\n",
    "\n",
    "def add_element_path_column(input_dataframe: pd.DataFrame, file_path_column: str, element_path_column: str = 'elementPath'):\n",
    "    \"\"\"\n",
    "    Adds a directory column to the input DataFrame based on the file path column.\n",
    "    input_dataframe : pd.DataFrame : The input DataFrame\n",
    "    file_path_column : str : The name of the file path column\n",
    "    directory_column : str : The name of the directory column to be added\n",
    "    return : pd.DataFrame : The DataFrame with added directory column\n",
    "    \"\"\"\n",
    "    if element_path_column in input_dataframe.columns:\n",
    "        return input_dataframe # Column already exists\n",
    "    \n",
    "    input_dataframe.insert(0, element_path_column, input_dataframe[file_path_column].str.split('/').apply(join_path_elements))\n",
    "    input_dataframe = input_dataframe.explode(element_path_column)\n",
    "    return input_dataframe\n",
    "\n",
    "def add_element_name_column(input_dataframe: pd.DataFrame, element_path_column: str = 'elementPath', element_name_column: str = 'elementName'):\n",
    "    \"\"\"\n",
    "    Adds a directory name column to the input DataFrame based on the directory column.\n",
    "    input_dataframe : pd.DataFrame : The input DataFrame\n",
    "    directory_column : str : The name of the directory column\n",
    "    directory_name_column : str : The name of the directory name column to be added\n",
    "    return : pd.DataFrame : The DataFrame with added directory name column\n",
    "    \"\"\"\n",
    "    if element_name_column in input_dataframe.columns:\n",
    "        return input_dataframe # Column already exists\n",
    "    \n",
    "    splitted_directories = input_dataframe[element_path_column].str.rsplit('/', n=1)\n",
    "    input_dataframe.insert(1, element_name_column, splitted_directories.apply(lambda x: (x[-1])))\n",
    "    return input_dataframe\n",
    "\n",
    "def add_parent_directory_column(input_dataframe: pd.DataFrame, element_path_column: str = 'elementPath', directory_parent_column: str = 'directoryParentPath'):\n",
    "    \"\"\"\n",
    "    Adds a directory parent column to the input DataFrame based on the directory column.\n",
    "    input_dataframe : pd.DataFrame : The input DataFrame\n",
    "    directory_column : str : The name of the directory column\n",
    "    directory_parent_column : str : The name of the directory parent column to be added\n",
    "    return : pd.DataFrame : The DataFrame with added directory parent column\n",
    "    \"\"\"\n",
    "    if directory_parent_column in input_dataframe.columns:\n",
    "        return input_dataframe # Column already exists\n",
    "    \n",
    "    # Remove last path element from directory_column to get the directory_parent_column\n",
    "    splitted_directories = input_dataframe[element_path_column].str.rsplit('/', n=1)\n",
    "    input_dataframe.insert(1, directory_parent_column, splitted_directories.apply(lambda x: (x[0])))\n",
    "    \n",
    "    # Clear parent (set to empty string) when it equal to the directory\n",
    "    input_dataframe.loc[input_dataframe[directory_parent_column] == input_dataframe[element_path_column], directory_parent_column] = ''\n",
    "    return input_dataframe\n",
    "\n",
    "def count_unique_aggregated_values(values: pd.Series):\n",
    "    \"\"\"\n",
    "    Return the number of unique values from an array of array of strings.\n",
    "    Meant to be used as an aggregation function for dataframe grouping.\n",
    "    values : Series : The pandas Series of values\n",
    "    return : int : The number of files\n",
    "    \"\"\"\n",
    "    return len(np.unique(np.concatenate(values.to_list())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04166d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Archetypes = typing.Literal[\"Authority\", \"Bottleneck\", \"Bridge\", \"Hub\", \"Outlier\"]\n",
    "archetype_names: List[Archetypes] = [\"Authority\", \"Bottleneck\", \"Bridge\", \"Hub\", \"Outlier\"]\n",
    "\n",
    "def get_archetype_column_name(archetype: Archetypes) -> str:\n",
    "    \"\"\"\n",
    "    Returns the column name for the given archetype.\n",
    "    archetype : Archetypes : The archetype name\n",
    "    return : str : The column name for the given archetype\n",
    "    \"\"\"\n",
    "    return f\"normalized{archetype}Rank\"\n",
    "\n",
    "def get_archetype_index(archetype: Archetypes) -> int:\n",
    "    \"\"\"\n",
    "    Returns the index of the given archetype.\n",
    "    archetype : Archetypes : The archetype name\n",
    "    return : int : The index of the given archetype\n",
    "    \"\"\"\n",
    "    return archetype_names.index(archetype)\n",
    "\n",
    "archetype_columns = [get_archetype_column_name(name) for name in archetype_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4caf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data() -> pd.DataFrame:\n",
    "    query: typing.LiteralString = \"\"\"\n",
    "        MATCH (anomalyScoreStats:File&!Directory&!Archive)\n",
    "        WHERE anomalyScoreStats.anomalyScore < 0\n",
    "        ORDER BY anomalyScoreStats.anomalyScore ASCENDING\n",
    "        LIMIT 150 // n largest negative anomaly scores as threshold\n",
    "         WITH collect(anomalyScoreStats.anomalyScore)[-1] AS anomalyScoreThreshold\n",
    "        MATCH (anomalyRankStats:File&!Directory&!Archive)\n",
    "         WITH anomalyScoreThreshold\n",
    "             ,max(anomalyRankStats.anomalyAuthorityRank)  AS maxAnomalyAuthorityRank\n",
    "             ,max(anomalyRankStats.anomalyBottleneckRank) AS maxAnomalyBottleneckRank\n",
    "             ,max(anomalyRankStats.anomalyBridgeRank)     AS maxAnomalyBridgeRank\n",
    "             ,max(anomalyRankStats.anomalyHubRank)        AS maxAnomalyHubRank\n",
    "             ,max(anomalyRankStats.anomalyOutlierRank)    AS maxAnomalyOutlierRank\n",
    "        MATCH (anomalous:File&!Directory&!Archive)\n",
    "        WHERE (anomalous.anomalyScore < anomalyScoreThreshold\n",
    "           OR  anomalous.anomalyHubRank        IS NOT NULL\n",
    "           OR  anomalous.anomalyAuthorityRank  IS NOT NULL\n",
    "           OR  anomalous.anomalyBottleneckRank IS NOT NULL\n",
    "           OR  anomalous.anomalyOutlierRank    IS NOT NULL\n",
    "           OR  anomalous.anomalyBridgeRank     IS NOT NULL)\n",
    "        OPTIONAL MATCH (project:Artifact|Project)-[:CONTAINS]->(anomalous)\n",
    "          WITH *\n",
    "              ,coalesce(project.name + '/', '')                     AS projectName\n",
    "              ,coalesce(anomalous.fileName, anomalous.relativePath) AS fileName\n",
    "        RETURN replace(projectName + fileName, '//', '/')   AS filePath\n",
    "              ,CASE WHEN anomalous.anomalyScore < 0 THEN abs(anomalous.anomalyScore) ELSE 0 END AS absoluteAnomalyScore\n",
    "              ,coalesce(toFloat(anomalous.anomalyAuthorityRank) / maxAnomalyAuthorityRank, 0)   AS normalizedAuthorityRank\n",
    "              ,coalesce(toFloat(anomalous.anomalyBottleneckRank) / maxAnomalyBottleneckRank, 0) AS normalizedBottleneckRank\n",
    "              ,coalesce(toFloat(anomalous.anomalyBridgeRank) / maxAnomalyBridgeRank, 0)         AS normalizedBridgeRank\n",
    "              ,coalesce(toFloat(anomalous.anomalyHubRank / maxAnomalyHubRank), 0)               AS normalizedHubRank\n",
    "              ,coalesce(toFloat(anomalous.anomalyOutlierRank) / maxAnomalyOutlierRank, 0)       AS normalizedOutlierRank\n",
    "        ORDER BY filePath ASCENDING\n",
    "        \"\"\"\n",
    "    return query_cypher_to_data_frame(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e51a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_treemap(data: pd.DataFrame, debug: bool = False) -> pd.DataFrame:\n",
    "    if debug:\n",
    "        display(\"1. query result ---------------------\")\n",
    "        display(data)\n",
    "\n",
    "    # 3. Add multiple rows for each file path containing all its directories paths in the new column 'elementPath'\n",
    "    data = add_element_path_column(data, 'filePath', 'elementPath')\n",
    "\n",
    "    if debug:\n",
    "        display(\"3. added elementPath --------------\")\n",
    "        display(data)\n",
    "\n",
    "    # Group the files by their directory and count the number of files of each directory (across all levels).\n",
    "    common_named_aggregation = dict(\n",
    "        absoluteAnomalyScore=pd.NamedAgg(column=\"absoluteAnomalyScore\", aggfunc=\"mean\"),\n",
    "        normalizedAuthorityRank=pd.NamedAgg(column=\"normalizedAuthorityRank\", aggfunc=\"max\"),\n",
    "        normalizedBottleneckRank=pd.NamedAgg(column=\"normalizedBottleneckRank\", aggfunc=\"max\"),\n",
    "        normalizedBridgeRank=pd.NamedAgg(column=\"normalizedBridgeRank\", aggfunc=\"max\"),\n",
    "        normalizedHubRank=pd.NamedAgg(column=\"normalizedHubRank\", aggfunc=\"max\"),\n",
    "        normalizedOutlierRank=pd.NamedAgg(column=\"normalizedOutlierRank\", aggfunc=\"max\"),\n",
    "    )\n",
    "\n",
    "    data = data.groupby(['elementPath']).aggregate(\n",
    "        filePaths=pd.NamedAgg(column=\"filePath\", aggfunc=np.unique),\n",
    "        firstFile=pd.NamedAgg(column=\"filePath\", aggfunc=\"first\"),\n",
    "        maxAnomalyScore=pd.NamedAgg(column=\"absoluteAnomalyScore\", aggfunc=\"max\"),\n",
    "        **common_named_aggregation\n",
    "    )\n",
    "\n",
    "    # Sort the grouped and aggregated entries by the name of the directory ascending and the anomaly score descending.\n",
    "    # The author with the most commits will then be listed first for each directory.\n",
    "    data = data.sort_values(by=['elementPath', 'absoluteAnomalyScore'], ascending=[True, False])\n",
    "    data = data.reset_index()\n",
    "\n",
    "    if debug:\n",
    "        display(\"4. grouped by elementPath --------------\")\n",
    "        display(data)\n",
    "\n",
    "    # Group the entries again now only by their directory path to get the aggregated number of anomalies and ranks.\n",
    "    data = data.groupby('elementPath').aggregate(\n",
    "        fileCount=pd.NamedAgg(column=\"filePaths\", aggfunc=count_unique_aggregated_values),\n",
    "        firstFile=pd.NamedAgg(column=\"firstFile\", aggfunc=\"first\"),\n",
    "        maxAnomalyScore=pd.NamedAgg(column=\"maxAnomalyScore\", aggfunc=\"max\"),\n",
    "        **common_named_aggregation\n",
    "    )\n",
    "    data = data.reset_index()\n",
    "\n",
    "    if debug:\n",
    "        display(\"5. grouped by directory path --------------\")\n",
    "        display(data)\n",
    "\n",
    "    # Add the name of the directory (last '/' separated element) and the parent directory path to the table.\n",
    "    data = add_element_name_column(data, 'elementPath', 'elementName')\n",
    "    data = add_parent_directory_column(data, 'elementPath', 'directoryParentPath')\n",
    "\n",
    "    if debug:\n",
    "        display(\"6. added directory and parent name --------------\")\n",
    "        display(data)\n",
    "\n",
    "    # Group finally by all columns except for the directory name, parent and path (first 3 columns) and pick the longest (max) directory path in case there are multiple.\n",
    "    all_column_names_except_for_the_directory_path = data.columns.to_list()[3:]\n",
    "    data = data.groupby(all_column_names_except_for_the_directory_path).aggregate(\n",
    "        elementName=pd.NamedAgg(column=\"elementName\", aggfunc=lambda names: '/'.join(names)),\n",
    "        directoryParentPath=pd.NamedAgg(column=\"directoryParentPath\", aggfunc=\"first\"),\n",
    "        elementPath=pd.NamedAgg(column=\"elementPath\", aggfunc=\"last\"),\n",
    "    )\n",
    "\n",
    "    # Reorder the column positions so that the directory path is again the first column. \n",
    "    all_column_names_with_the_directory_path_first = ['elementPath', 'directoryParentPath', 'elementName'] + all_column_names_except_for_the_directory_path\n",
    "    data = data.reset_index()[all_column_names_with_the_directory_path_first]\n",
    "\n",
    "    if debug:\n",
    "        display(\"7. final grouping --------------\")\n",
    "        display(data)\n",
    "        display(\"Statistics --------------\")\n",
    "        data.describe()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68aa20",
   "metadata": {},
   "source": [
    "## 1. Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5222a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = get_graph_database_driver()\n",
    "anomaly_file_paths = query_data()\n",
    "anomaly_file_paths = prepare_data_for_treemap(anomaly_file_paths)\n",
    "display(anomaly_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa3a949",
   "metadata": {},
   "source": [
    "### 1.1 Average anomaly score per file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_display = anomaly_file_paths.copy()\n",
    "\n",
    "figure = plotly_graph_objects.Figure(plotly_graph_objects.Treemap(\n",
    "    create_treemap_settings(data_to_display),\n",
    "    marker=dict(\n",
    "        **plotly_treemap_marker_base_color_scale,\n",
    "        colors=data_to_display['absoluteAnomalyScore'], \n",
    "        colorbar={\"title\": \"score\"},\n",
    "    ),\n",
    "))\n",
    "figure.update_layout(\n",
    "    **plotly_treemap_layout_base_settings, # type: ignore\n",
    "    title='Average anomaly score per directory',\n",
    ")\n",
    "figure.show(**plotly_treemap_figure_show_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24341675",
   "metadata": {},
   "source": [
    "### 1.2 Overview of all anomaly archetypes per directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_exclusive_ranks(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Modifies the input data frame to ensure that only one archetype rank is non-zero per row.\n",
    "    The archetype with the highest normalized rank is retained, and others are set to zero.\n",
    "    data : pd.DataFrame : The input data frame\n",
    "    return : pd.DataFrame : The modified data frame with mutual exclusive ranks\n",
    "    \"\"\"\n",
    "    modified_data = data.copy()\n",
    "    \n",
    "    for dataframe_index, row in modified_data.iterrows():\n",
    "        index = typing.cast(int, dataframe_index)\n",
    "        max_rank_value = 0\n",
    "        max_rank_column = None\n",
    "        \n",
    "        for column in archetype_columns:\n",
    "            if row[column] > max_rank_value:\n",
    "                max_rank_value = row[column]\n",
    "                max_rank_column = column\n",
    "        \n",
    "        for column in archetype_columns:\n",
    "            if column != max_rank_column:\n",
    "                modified_data.at[index, column] = 0\n",
    "    \n",
    "    return modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_color(low: Tuple[int, int, int], high: Tuple[int, int, int], normalized_value: float) -> str:\n",
    "    \"\"\"Linear interpolation between two RGB tuples, returns rgba string.\"\"\"\n",
    "    \n",
    "    def linear_interpolation_of_color_component(color_component: int) -> int:\n",
    "        return int(low[color_component] + (high[color_component] - low[color_component]) * normalized_value)\n",
    "    \n",
    "    red = linear_interpolation_of_color_component(0)\n",
    "    green = linear_interpolation_of_color_component(1)\n",
    "    blue = linear_interpolation_of_color_component(2)\n",
    "    return f\"rgb({red},{green},{blue})\"\n",
    "\n",
    "\n",
    "def get_rank_color(rank: float, low: Tuple[int, int, int], high: Tuple[int, int, int]) -> str:\n",
    "    \"\"\"Return transparent if rank == 0, else interpolate between low and high.\"\"\"\n",
    "    if rank <= 0:\n",
    "        return \"rgb(255,255,255)\"\n",
    "    return interpolate_color(low, high, rank)\n",
    "\n",
    "\n",
    "def combine_rank_colors(\n",
    "    dataframe: pd.DataFrame,\n",
    "    rank_columns: List[str],\n",
    "    color_pairs: List[Tuple[Tuple[int, int, int], Tuple[int, int, int]]],\n",
    ") -> List[str]:\n",
    "    \"\"\"Combine multiple ranks, using the first nonzero value's color.\"\"\"\n",
    "    combined: List[str] = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        color = \"rgb(255,255,255)\"\n",
    "        for rank_col, (low, high) in zip(rank_columns, color_pairs):\n",
    "            rank = row[rank_col]\n",
    "            if rank > 0:\n",
    "                color = get_rank_color(rank, low, high)\n",
    "                break\n",
    "        combined.append(color)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def get_rank_color_for_archetype(dataframe: pd.DataFrame, archetype: Archetypes) -> List[str]:\n",
    "    \"\"\"Get combined rank colors for a specific archetype.\"\"\"\n",
    "    archetype_column_name = get_archetype_column_name(archetype)\n",
    "    coloring_pair = get_coloring_pairs()[archetype_names.index(archetype)]\n",
    "    return combine_rank_colors(dataframe, [archetype_column_name], [coloring_pair])\n",
    "\n",
    "\n",
    "def get_coloring_pairs() -> List[Tuple[Tuple[int, int, int], Tuple[int, int, int]]]:\n",
    "    \"\"\"Define the coloring scheme for each archetype.\"\"\"\n",
    "    assert len(archetype_names) == 5, \"Expected exactly 5 archetypes.\"\n",
    "    return [\n",
    "        ((222, 235, 247), (33, 113, 181)), # Authority, Red shades\n",
    "        ((254, 230, 206), (217, 72, 1)),   # Bottleneck, Green shades\n",
    "        ((239, 237, 245), (106, 81, 163)), # Bridge, Blue shades\n",
    "        ((254, 224, 210), (165,15,21)),    # Hub, Orange shades\n",
    "        ((240, 240, 240), (82, 82, 82)),   # Outlier, Purple shades\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_display = mutual_exclusive_ranks(anomaly_file_paths)\n",
    "# Optionally only keep rows where at least one archetype rank is greater than zero\n",
    "data_to_display = data_to_display[data_to_display[archetype_columns].sum(axis=1) > 0]\n",
    "\n",
    "coloring_pairs = get_coloring_pairs()\n",
    "combined_colors = combine_rank_colors(data_to_display, archetype_columns, coloring_pairs)\n",
    "\n",
    "figure = plotly_graph_objects.Figure()\n",
    "\n",
    "figure.add_trace(plotly_graph_objects.Treemap(\n",
    "    create_treemap_settings(data_to_display),\n",
    "    marker=dict(\n",
    "        **plotly_treemap_marker_base_style,\n",
    "        line=dict(width=1, color=\"black\"),\n",
    "        showscale=False,\n",
    "        colors=combined_colors,\n",
    "    ),\n",
    "    name=\"Anomalies\",\n",
    "    opacity=0.8\n",
    "))\n",
    "\n",
    "# Add dummy scatter traces for legend\n",
    "for name, (low, high) in zip(archetype_names, coloring_pairs):\n",
    "    bright_color = interpolate_color(low, high, 0.4)  # light tone for legend filling\n",
    "    dark_color = interpolate_color(low, high, 1.0)  # darkest tone for legend outline\n",
    "    figure.add_trace(plotly_graph_objects.Scatter(\n",
    "        x=[None],\n",
    "        y=[None],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=12, color=bright_color, line=dict(width=2, color=dark_color)),\n",
    "        name=name,\n",
    "        legendgroup=name,\n",
    "        showlegend=True,\n",
    "    ))\n",
    "\n",
    "figure.update_layout(\n",
    "    **plotly_treemap_layout_base_settings, # type: ignore\n",
    "    title='Overview of all anomaly archetypes per directory',\n",
    "    legend=dict(\n",
    "        orientation=\"h\", # horizontal legend (use \"v\" for vertical)\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.12,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    )\n",
    ")\n",
    "figure.update_xaxes(visible=False)\n",
    "figure.update_yaxes(visible=False)\n",
    "\n",
    "figure.show(**plotly_treemap_figure_show_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d568c",
   "metadata": {},
   "source": [
    "### 1.3a Archetype - Authority per directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_archetype_treemap(archetype: Archetypes, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots a treemap for the given archetype using the provided data.\n",
    "    archetype : Archetypes : The archetype to plot\n",
    "    data : pd.DataFrame : The input data frame\n",
    "    \"\"\"\n",
    "    data_to_display = data.copy()\n",
    "    data_to_display = data_to_display[data_to_display[archetype_columns].sum(axis=1) > 0]\n",
    "\n",
    "    archetype_column_name = get_archetype_column_name(archetype)\n",
    "    combined_colors = get_rank_color_for_archetype(data_to_display, archetype)\n",
    "\n",
    "    figure = plotly_graph_objects.Figure(plotly_graph_objects.Treemap(\n",
    "        create_treemap_settings(data_to_display),\n",
    "        marker=dict(\n",
    "            **plotly_treemap_marker_base_style,\n",
    "            colors=combined_colors,\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            colorbar={\"title\": \"rank\"},\n",
    "        ),\n",
    "    ))\n",
    "    figure.update_layout(\n",
    "        **plotly_treemap_layout_base_settings, # type: ignore\n",
    "        title=f'Archetype \"{archetype}\" per directory',\n",
    "    )\n",
    "    figure.show(**plotly_treemap_figure_show_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a497cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_archetype_treemap(\"Authority\", data_to_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4eb401",
   "metadata": {},
   "source": [
    "### 1.3b Archetype - Bottleneck per directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37656bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_archetype_treemap(\"Bottleneck\", anomaly_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06010d6d",
   "metadata": {},
   "source": [
    "### 1.3c Archetype - Bridge per directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_archetype_treemap(\"Bridge\", anomaly_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375f191",
   "metadata": {},
   "source": [
    "### 1.3d Archetype - Hub per directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_archetype_treemap(\"Hub\", anomaly_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d80b4",
   "metadata": {},
   "source": [
    "### 1.3e Archetype - Outlier per directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ac193",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_archetype_treemap(\"Outlier\", anomaly_file_paths)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "JohT"
   }
  ],
  "code_graph_analysis_pipeline_data_validation": "ValidateAlwaysFalse",
  "kernelspec": {
   "display_name": "codegraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Anomaly Detection - Manual Exploration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
