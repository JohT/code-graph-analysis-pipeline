{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0eabc4",
   "metadata": {},
   "source": [
    "# Anomaly Detection - Manual Exploration\n",
    "\n",
    "This notebook demonstrates different methods for anomaly detection for static code analysis data using jQAssistant and Neo4j. It plots results of different approaches from plain queries to statistical methods. The focus is on detecting anomalies in the data, which can be useful for identifying potential issues or areas for improvement in the codebase.\n",
    "\n",
    "<br>  \n",
    "\n",
    "### References\n",
    "- [jqassistant](https://jqassistant.org)\n",
    "- [Neo4j Python Driver](https://neo4j.com/docs/api/python-driver/current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell uses the build-in %html \"magic\" to override the CSS style for tables to a much smaller size.\n",
    "#This is especially needed for PDF export of tables with multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS style for smaller dataframe tables. */\n",
    ".dataframe th {\n",
    "    font-size: 8px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 8px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07319282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Colormap\n",
    "# main_color_map = 'nipy_spectral'\n",
    "main_color_map = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version as python_version\n",
    "print('Python version: {}'.format(python_version))\n",
    "\n",
    "from numpy import __version__ as numpy_version\n",
    "print('numpy version: {}'.format(numpy_version))\n",
    "\n",
    "from pandas import __version__ as pandas_version\n",
    "print('pandas version: {}'.format(pandas_version))\n",
    "\n",
    "from matplotlib import __version__ as matplotlib_version\n",
    "print('matplotlib version: {}'.format(matplotlib_version))\n",
    "\n",
    "from seaborn import __version__ as seaborn_version  # type: ignore\n",
    "print('seaborn version: {}'.format(seaborn_version))\n",
    "\n",
    "from neo4j import __version__ as neo4j_version\n",
    "print('neo4j version: {}'.format(neo4j_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the environment variable \"NEO4J_INITIAL_PASSWORD\" in your shell \n",
    "# before starting jupyter notebook to provide the password for the user \"neo4j\". \n",
    "# It is not recommended to hardcode the password into jupyter notebook for security reasons.\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\n",
    "    uri=\"bolt://localhost:7687\", \n",
    "    auth=(\"neo4j\", os.environ.get(\"NEO4J_INITIAL_PASSWORD\"))\n",
    ")\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cypher_to_data_frame(query: typing.LiteralString, parameters: typing.Optional[typing.Dict[str, typing.Any]] = None):\n",
    "    records, summary, keys = driver.execute_query(query, parameters_=parameters)\n",
    "    return pd.DataFrame([record.values() for record in records], columns=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4caf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(node_label: typing.Literal[\"Artifact\", \"Package\", \"Type\", \"Method\", \"Module\"]) -> pd.DataFrame:\n",
    "\n",
    "    query: typing.LiteralString = \"\"\"\n",
    "        MATCH (codeUnit)\n",
    "        WHERE $projection_node_label IN labels(codeUnit)\n",
    "          AND (codeUnit.incomingDependencies IS NOT NULL OR codeUnit.outgoingDependencies IS NOT NULL)\n",
    "          AND codeUnit.embeddingsFastRandomProjectionTunedForClustering  IS NOT NULL\n",
    "          AND codeUnit.centralityPageRank                                IS NOT NULL\n",
    "          AND codeUnit.centralityArticleRank                             IS NOT NULL\n",
    "          AND codeUnit.centralityBetweenness                             IS NOT NULL\n",
    "          AND codeUnit.communityLocalClusteringCoefficient               IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANProbability                      IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANNoise                            IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANMedoid                           IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANRadiusMax                        IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANRadiusAverage                    IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid       IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANSize                             IS NOT NULL\n",
    "          AND codeUnit.clusteringHDBSCANLabel                            IS NOT NULL\n",
    "          AND codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationX       IS NOT NULL\n",
    "          AND codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationY       IS NOT NULL\n",
    "        OPTIONAL MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit)\n",
    "            WITH *, artifact.name AS artifactName\n",
    "        OPTIONAL MATCH (projectRoot:Directory)<-[:HAS_ROOT]-(proj:TS:Project)-[:CONTAINS]->(codeUnit)\n",
    "            WITH *, last(split(projectRoot.absoluteFileName, '/')) AS projectName   \n",
    "         WITH * \n",
    "             ,coalesce(codeUnit.incomingDependencies, 0)          AS incomingDependencies\n",
    "             ,coalesce(codeUnit.outgoingDependencies, 0)          AS outgoingDependencies\n",
    "             ,coalesce(codeUnit.fqn, codeUnit.globalFqn, codeUnit.fileName, codeUnit.signature, codeUnit.name) AS codeUnitName\n",
    "             ,coalesce(artifactName, projectName, \"\")             AS projectName\n",
    "             ,coalesce(codeUnit.anomalyScore, 0.0)                AS anomalyScore\n",
    "             ,coalesce(codeUnit.anomalyNodeEmbeddingSHAPSum, 0.0) AS anomalyNodeEmbeddingSHAPSum\n",
    "       RETURN DISTINCT \n",
    "              codeUnitName\n",
    "             ,codeUnit.name                                                 AS shortCodeUnitName\n",
    "             ,projectName\n",
    "             ,elementId(codeUnit)                                           AS nodeElementId\n",
    "             ,incomingDependencies\n",
    "             ,outgoingDependencies\n",
    "             ,incomingDependencies + outgoingDependencies                   AS degree\n",
    "             ,codeUnit.embeddingsFastRandomProjectionTunedForClustering     AS embedding\n",
    "             ,codeUnit.centralityPageRank                                   AS pageRank\n",
    "             ,codeUnit.centralityArticleRank                                AS articleRank\n",
    "             ,codeUnit.centralityPageRank - codeUnit.centralityArticleRank  AS pageToArticleRankDifference\n",
    "             ,codeUnit.centralityBetweenness                                AS betweenness\n",
    "             ,codeUnit.communityLocalClusteringCoefficient                  AS clusteringCoefficient\n",
    "             ,1.0 - codeUnit.communityLocalClusteringCoefficient            AS inverseClusteringCoefficient\n",
    "             ,1.0 - codeUnit.clusteringHDBSCANProbability                   AS clusterApproximateOutlierScore\n",
    "             ,codeUnit.clusteringHDBSCANProbability                         AS clusterProbability\n",
    "             ,codeUnit.clusteringHDBSCANNoise                               AS clusterNoise\n",
    "             ,codeUnit.clusteringHDBSCANRadiusMax                           AS clusterRadiusMax\n",
    "             ,codeUnit.clusteringHDBSCANRadiusAverage                       AS clusterRadiusAverage\n",
    "             ,codeUnit.clusteringHDBSCANNormalizedDistanceToMedoid          AS clusterDistanceToMedoid\n",
    "             ,codeUnit.clusteringHDBSCANSize                                AS clusterSize\n",
    "             ,codeUnit.clusteringHDBSCANLabel                               AS clusterLabel\n",
    "             ,codeUnit.clusteringHDBSCANMedoid                              AS clusterMedoid\n",
    "             ,CASE WHEN anomalyScore < 0.0 THEN 0.0 ELSE anomalyScore END   AS anomalyScore\n",
    "             ,anomalyNodeEmbeddingSHAPSum * -1.0                            AS negatedAnomalyNodeEmbeddingSHAPSum\n",
    "             ,codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationX          AS embeddingVisualizationX\n",
    "             ,codeUnit.embeddingsFastRandomProjectionTunedForClusteringVisualizationY          AS embeddingVisualizationY\n",
    "        \"\"\"\n",
    "    return query_cypher_to_data_frame(query, {\"projection_node_label\": node_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7656bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_annotation_style: dict = {\n",
    "    'textcoords': 'offset points',\n",
    "    'arrowprops': dict(arrowstyle='->', color='black', alpha=0.3),\n",
    "    'fontsize': 6,\n",
    "    'backgroundcolor': 'white',\n",
    "    'bbox': dict(boxstyle='round,pad=0.4',\n",
    "                    edgecolor='silver',\n",
    "                    facecolor='whitesmoke',\n",
    "                    alpha=1\n",
    "                )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(text: str, max_length: int = 26):\n",
    "    \"\"\"\n",
    "    Truncates the input text to match the given maximum length.\n",
    "    In case it exceeds the maximum length, the last 3 characters are replaced by dots to make the truncation visible.\n",
    "    \"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    return text[:max_length - 3] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_each_with_index(\n",
    "    data: pd.DataFrame,\n",
    "    using: typing.Callable,\n",
    "    name_column: str,\n",
    "    x_position_column: str,\n",
    "    y_position_column: str,\n",
    "    value_column: str = \"\",\n",
    "    probability_column: str = \"\",\n",
    "    **kwargs\n",
    "):\n",
    "    if data.empty:\n",
    "        return\n",
    "\n",
    "    data_in_reversed_order = data.iloc[::-1]  # plot most important annotations last to overlap less important ones\n",
    "\n",
    "    annotation_function = using\n",
    "    for dataframe_index, row in data_in_reversed_order.iterrows():\n",
    "        index = typing.cast(int, dataframe_index)\n",
    "        y_offset = (index % 5) * 10\n",
    "\n",
    "        value_info = f\" ({row[value_column]:.4f})\" if value_column else \"\"\n",
    "        probability_info = f\" (p={row[probability_column]:.3f})\" if probability_column else \"\"\n",
    "\n",
    "        annotation_function(\n",
    "            **plot_annotation_style,\n",
    "            **kwargs,\n",
    "            text=f\"#{index + 1}: {truncate(row[name_column])}{value_info}{probability_info}\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5 + y_offset),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5de7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_into_center_while_preserving_masked_rows(\n",
    "        data: pd.DataFrame,\n",
    "        distances_to_center: np.ndarray,\n",
    "        mask_for_columns_to_preserve: pd.Series,\n",
    "        distance_to_center_quantile: float = 0.8,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"Zooms in\" into the input data DataFrame to focus on the data in the center.\n",
    "    The numpy array \"distances_to_center\" contains a distance for every row in the input data.\n",
    "    All rows outside the \"percentile_of_distance_to_center\" of this distance will get filtered out.\n",
    "    However, fields that are marked with true by <mask_for_columns_to_preserve> remain in the DataFrame even if they are outside the distance quantile.\n",
    "    \"\"\"\n",
    "    if data.shape[0] != distances_to_center.size:\n",
    "        raise ValueError(\"Error: The number of rows in the data need to match the length of the distances_to_center.\")\n",
    "    distance_to_center_threshold = np.quantile(distances_to_center, distance_to_center_quantile)\n",
    "    return data[(distances_to_center <= distance_to_center_threshold) | mask_for_columns_to_preserve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances_to_center(data: pd.DataFrame, x_position_column: str, y_position_column: str):\n",
    "    \"\"\"\n",
    "    Computes the 2D Euclidean distances from center for every point and returns that as an numpy array.\n",
    "    \"\"\"\n",
    "    center_x = data[x_position_column].mean()\n",
    "    center_y = data[y_position_column].mean()\n",
    "    return np.sqrt((data[x_position_column] - center_x)**2 + (data[y_position_column] - center_y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_lowest_score_columns(\n",
    "        data: pd.DataFrame,\n",
    "        score_column: str,\n",
    "        lowest_n: int,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns a DataDFrame with one unnamed column containing Boolean values for every row of the input data.\n",
    "    True means that the input data row fulfills the predicate \"score from score_column with in the top_n values\".\n",
    "    \"\"\"\n",
    "    score_threshold = data[score_column].nsmallest(lowest_n).iloc[-1]\n",
    "    return (data[score_column] <= score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_into_center(\n",
    "        data: pd.DataFrame,\n",
    "        x_position_column: str,\n",
    "        y_position_column: str,\n",
    "        percentile_of_distance_to_center: float = 0.8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"Zooms in\" into the input data DataFrame to focus on the data in the center.\n",
    "    Only rows outside within the \"percentile_of_distance_to_center\" will remain in the returned DataFrame.\n",
    "    \"\"\"\n",
    "    distances_to_center = calculate_distances_to_center(data, x_position_column, y_position_column)\n",
    "    no_exceptions_dummy_mask = pd.Series(False, index=data.index)\n",
    "    return zoom_into_center_while_preserving_masked_rows(data, distances_to_center, no_exceptions_dummy_mask, percentile_of_distance_to_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12307ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_into_center_while_preserving_lowest_scores(\n",
    "        data: pd.DataFrame,\n",
    "        x_position_column: str,\n",
    "        y_position_column: str,\n",
    "        score_column: str,\n",
    "        top_n_scores: int = 10,\n",
    "        percentile_of_distance_to_center: float = 0.8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"Zooms in\" into the input data DataFrame to focus on the data in the center.\n",
    "    Only rows outside within the \"percentile_of_distance_to_center\" will remain in the returned DataFrame.\n",
    "    Rows with scores (score_column) within the top_n_scores will remain in the DataFrame \n",
    "    even if they are further away from the center.\n",
    "    \"\"\"\n",
    "    distances_to_center = calculate_distances_to_center(data, x_position_column, y_position_column)\n",
    "    top_score_rows_mask = mask_lowest_score_columns(data, score_column, top_n_scores)\n",
    "    return zoom_into_center_while_preserving_masked_rows(data, distances_to_center, top_score_rows_mask, percentile_of_distance_to_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_into_center_while_preserving_scores_above_threshold(\n",
    "        data: pd.DataFrame,\n",
    "        x_position_column: str,\n",
    "        y_position_column: str,\n",
    "        score_column: str,\n",
    "        score_threshold: float,\n",
    "        percentile_of_distance_to_center: float = 0.8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    \"Zooms in\" into the input data DataFrame to focus on the data in the center.\n",
    "    Only rows outside within the \"percentile_of_distance_to_center\" will remain in the returned DataFrame.\n",
    "    Rows with scores (score_column) above the score_threshold will remain in the DataFrame \n",
    "    even if they are further away from the center.\n",
    "    \"\"\"\n",
    "    distances_to_center = calculate_distances_to_center(data, x_position_column, y_position_column)\n",
    "    score_above_threshold_mask = (data[score_column] >= score_threshold)\n",
    "    return zoom_into_center_while_preserving_masked_rows(data, distances_to_center, score_above_threshold_mask, percentile_of_distance_to_center)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ad7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_marker_sizes(size_values, minimum_size: int = 10, maximum_size: int = 1000, top_fraction: float = 0.1, downscale_factor: float = 0.8):\n",
    "    \"\"\"\n",
    "    Scales numeric size values to a visual range suitable for matplotlib's scatter plot.\n",
    "\n",
    "    Parameters:\n",
    "        size_values (array-like): The raw size values to scale.\n",
    "        minimum_size (float): The smallest marker area (in points^2).\n",
    "        maximum_size (float): The largest marker area (in points^2).\n",
    "        top_fraction (float or None): If set, only top values remain fully scaled, others are reduced slightly.\n",
    "        downscale_factor (float): Factor to reduce sizes below the cutoff (0 < factor < 1).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Scaled marker sizes.\n",
    "    \"\"\"\n",
    "    size_values = np.array(size_values)\n",
    "    smallest_value = size_values.min()\n",
    "    largest_value = size_values.max()\n",
    "\n",
    "    # Handle case where all values are the same\n",
    "    if largest_value == smallest_value:\n",
    "        normalized_values = np.full_like(size_values, 0.5)\n",
    "    else:\n",
    "        normalized_values = (size_values - smallest_value) / (largest_value - smallest_value)\n",
    "\n",
    "    cutoff = np.quantile(normalized_values, 1.0 - top_fraction)\n",
    "    cutoff = np.quantile(normalized_values, 1 - top_fraction)\n",
    "    below_cutoff = normalized_values < cutoff\n",
    "    normalized_values[below_cutoff] *= downscale_factor\n",
    "    \n",
    "    # Scale to desired visual size range\n",
    "    return normalized_values * (maximum_size - minimum_size) + minimum_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68aa20",
   "metadata": {},
   "source": [
    "## 1. Java Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def38db",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_features = query_data(\"Package\")\n",
    "display(java_package_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927388f",
   "metadata": {},
   "source": [
    "### 1.1 Differences between Page Rank and Article Rank\n",
    "\n",
    "A high difference between Page Rank and Article Rank can reveal nodes with imbalanced roles — e.g. utility code that is highly depended on but does not depend on much else.\n",
    "\n",
    "PageRank measures how important a node is by who depends on it (high in-degree weight) while ArticleRank measures how important a node is based on how many other nodes it links to (outgoing edges matter more).\n",
    "\n",
    "Nodes with low PageRank but high ArticleRank may be coordination-heavy, which could signal:\n",
    "- Unusual architecture\n",
    "- Utility overuse\n",
    "- Monolithic patterns\n",
    "\n",
    "These are often design smells or potential anomalies in large-scale codebases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb417d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_standard_deviation_lines(color: typing.LiteralString, mean: float, standard_deviation: float, standard_deviation_factor: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Plots vertical lines for the mean + factor times standard deviation (z-score references).\n",
    "    \"\"\"\n",
    "    # Vertical line for the standard deviation\n",
    "    positive_standard_deviation = mean + (standard_deviation_factor * standard_deviation)\n",
    "    horizontal_line_label = f'Mean + {standard_deviation_factor} x Standard Deviation: {positive_standard_deviation:.2f}' if standard_deviation_factor != 0 else f'Mean: {mean:.2f}'\n",
    "    \n",
    "    plot.axvline(positive_standard_deviation, color=color, linestyle='dashed', linewidth=1, label=horizontal_line_label)\n",
    "    \n",
    "    if standard_deviation_factor != 0:\n",
    "        negative_standard_deviation = mean - (standard_deviation_factor * standard_deviation)\n",
    "        plot.axvline(negative_standard_deviation, color=color, linestyle='dashed', linewidth=1)\n",
    "        \n",
    "    plot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_difference_between_article_and_page_rank(\n",
    "    page_ranks: pd.Series, \n",
    "    article_ranks: pd.Series,\n",
    "    short_names: pd.Series,\n",
    "    title_prefix: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the difference between Article Rank and Page Rank for Java packages.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    page_ranks : pd.Series\n",
    "        DataFrame column containing Page Rank values.\n",
    "    article_ranks : pd.Series\n",
    "        DataFrame column containing Article Rank values.\n",
    "    short_names : pd.Series\n",
    "        DataFrame column containing short names of the code units.\n",
    "    title_prefix: str\n",
    "        Text at the beginning of the title\n",
    "    \"\"\"\n",
    "    if page_ranks.empty or article_ranks.empty or short_names.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    # Calculate the difference between Article Rank and Page Rank\n",
    "    page_to_article_rank_difference = page_ranks - article_ranks\n",
    "\n",
    "    plot.figure(figsize=(10, 6))\n",
    "    plot.hist(page_to_article_rank_difference, bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "    plot.title(f\"{title_prefix} distribution of PageRank - ArticleRank differences\", pad=20)\n",
    "    plot.xlabel('Absolute difference between Page Rank and Article Rank')\n",
    "    plot.ylabel('Frequency')\n",
    "    plot.xlim(left=page_to_article_rank_difference.min(), right=page_to_article_rank_difference.max())\n",
    "    plot.yscale('log')  # Use logarithmic scale for better visibility of differences\n",
    "    plot.grid(True)\n",
    "    plot.tight_layout()\n",
    "\n",
    "    mean_difference = page_to_article_rank_difference.mean()\n",
    "    standard_deviation = page_to_article_rank_difference.std()\n",
    "    \n",
    "    # Vertical line for the mean\n",
    "    plot_standard_deviation_lines('red', mean_difference, standard_deviation, standard_deviation_factor=0)\n",
    "    # Vertical line for the standard deviation + mean (=z-score of 1)\n",
    "    plot_standard_deviation_lines('orange', mean_difference, standard_deviation, standard_deviation_factor=1)\n",
    "    # Vertical line for 2 x standard deviations + mean (=z-score of 2)\n",
    "    plot_standard_deviation_lines('green', mean_difference, standard_deviation, standard_deviation_factor=2)\n",
    "\n",
    "    def annotate_outliers(outliers: pd.DataFrame) -> None:\n",
    "        if outliers.empty:\n",
    "            return\n",
    "        for dataframe_index, row in outliers.iterrows():\n",
    "            index = typing.cast(int, dataframe_index)\n",
    "            value = row['pageToArticleRankDifference']\n",
    "            x_index_offset = - index * 10 if value > 0 else + index * 10\n",
    "            plot.annotate(\n",
    "                text=f'{row['shortName']} (PageRanking #{row['page_rank_ranking']}, ArticleRanking #{row['article_rank_ranking']})',\n",
    "                xy=(value, 1),\n",
    "                xytext=(value + x_index_offset, 60),\n",
    "                rotation=90,\n",
    "                **plot_annotation_style,\n",
    "            )\n",
    "\n",
    "    # Merge all series into a single DataFrame for easier handling\n",
    "    page_to_article_rank_dataframe = pd.DataFrame({\n",
    "        'shortName': short_names,\n",
    "        'pageRank': page_ranks,\n",
    "        'articleRank': article_ranks,\n",
    "        'pageToArticleRankDifference': page_to_article_rank_difference,\n",
    "        'page_rank_ranking': page_ranks.rank().astype(int),\n",
    "        'article_rank_ranking': article_ranks.rank().astype(int)\n",
    "    }, index=page_ranks.index)\n",
    "\n",
    "    # Annotate values above z-score of 2 with their names\n",
    "    positive_z_score_2 = mean_difference + 2 * standard_deviation\n",
    "    positive_outliers = page_to_article_rank_dataframe[page_to_article_rank_difference > positive_z_score_2].sort_values(by='pageToArticleRankDifference', ascending=False).reset_index().head(5)\n",
    "    annotate_outliers(positive_outliers)\n",
    "\n",
    "    # Annotate values below z-score of -2 with their names\n",
    "    negative_z_score_2 = mean_difference - 2 * standard_deviation\n",
    "    negative_outliers = page_to_article_rank_dataframe[page_to_article_rank_difference < negative_z_score_2].sort_values(by='pageToArticleRankDifference', ascending=True).reset_index().head(5)\n",
    "    annotate_outliers(negative_outliers)\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec76328",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_difference_between_article_and_page_rank(\n",
    "    java_package_features['pageRank'],\n",
    "    java_package_features['articleRank'],\n",
    "    java_package_features['shortCodeUnitName'],\n",
    "    title_prefix='Java Package'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec26a8",
   "metadata": {},
   "source": [
    "### 1.2 Local Clustering Coefficient\n",
    "\n",
    "The local clustering coefficient is a measure of how connected a node's neighbors are to each other.\n",
    "A high local clustering coefficient indicates that a node's neighbors are well-connected, which can suggest a tightly-knit group of related components or classes.\n",
    "A low local clustering coefficient may indicate that a node's neighbors are not well-connected, which can suggest a more loosely-coupled architecture or potential design smells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed900c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_coefficient_distribution(clustering_coefficients: pd.Series, title_prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots the distribution of clustering coefficients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clustering_coefficients : pd.Series\n",
    "        Series containing clustering coefficient values.\n",
    "    text_prefix: str\n",
    "        Text at the beginning of the title\n",
    "    \"\"\"\n",
    "    if clustering_coefficients.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    plot.figure(figsize=(10, 6))\n",
    "    plot.figure(figsize=(10, 6))\n",
    "    plot.hist(clustering_coefficients, bins=40, color='blue', alpha=0.7, edgecolor='black')\n",
    "    plot.title(f\"{title_prefix} Distribution of Clustering Coefficients\", pad=20)\n",
    "    plot.xlabel('Clustering Coefficient')\n",
    "    plot.ylabel('Frequency')\n",
    "    plot.xlim(left=clustering_coefficients.min(), right=clustering_coefficients.max())\n",
    "    # plot.yscale('log')  # Use logarithmic scale for better visibility of differences\n",
    "    plot.grid(True)\n",
    "    plot.tight_layout()\n",
    "\n",
    "    mean = clustering_coefficients.mean()\n",
    "    standard_deviation = clustering_coefficients.std()\n",
    "\n",
    "    # Vertical line for the mean\n",
    "    plot_standard_deviation_lines('red', mean, standard_deviation, standard_deviation_factor=0)\n",
    "    # Vertical line for 1 x standard deviations + mean (=z-score of 1)\n",
    "    plot_standard_deviation_lines('green', mean, standard_deviation, standard_deviation_factor=1)\n",
    "\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aff8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_coefficient_distribution(java_package_features['clusteringCoefficient'], title_prefix=\"Java Package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f46116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_coefficient_vs_page_rank(\n",
    "    clustering_coefficients: pd.Series, \n",
    "    page_ranks: pd.Series,\n",
    "    short_names: pd.Series,\n",
    "    clustering_noise: pd.Series,\n",
    "    title_prefix: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the relationship between clustering coefficients and Page Rank values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clustering_coefficients : pd.Series\n",
    "        Series containing clustering coefficient values.\n",
    "    page_ranks : pd.Series\n",
    "        Series containing Page Rank values.\n",
    "    short_names : pd.Series\n",
    "        Series containing short names of the code units.\n",
    "    clustering_noise : pd.Series\n",
    "        Series indicating whether the code unit is noise (value = 1) nor not (value = 0) from the clustering algorithm.\n",
    "    title_prefix: str\n",
    "        Text at the beginning of the title\n",
    "    \"\"\"\n",
    "    if clustering_coefficients.empty or page_ranks.empty or short_names.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    color = clustering_noise.map({0: 'blue', 1: 'gray'})\n",
    "\n",
    "    plot.figure(figsize=(10, 6))\n",
    "    plot.scatter(x=clustering_coefficients, y=page_ranks, alpha=0.7, color=color)\n",
    "    plot.title(f\"{title_prefix} Clustering Coefficient vs Page Rank\", pad=20)\n",
    "    plot.xlabel('Clustering Coefficient')\n",
    "    plot.ylabel('Page Rank')\n",
    "\n",
    "    # Add color bar: grey = noise, blue = non-noise\n",
    "    scatter_noise = plot.scatter([], [], color='lightgrey', label='Noise', alpha=0.7)\n",
    "    scatter = plot.scatter([], [], color='blue', label='Non-Noise', alpha=0.7)\n",
    "    plot.legend(handles=[scatter, scatter_noise], loc='upper right', title='Clustering')\n",
    "    \n",
    "    # Merge all series into a single DataFrame for easier handling\n",
    "    combined_data = pd.DataFrame({\n",
    "        'shortName': short_names,\n",
    "        'clusteringCoefficient': clustering_coefficients,\n",
    "        'pageRank': page_ranks,\n",
    "        'clusterNoise': clustering_noise,\n",
    "    }, index=clustering_coefficients.index)\n",
    "\n",
    "    common_column_names_for_annotations = {\n",
    "        \"name_column\": 'shortName',\n",
    "        \"x_position_column\": 'clusteringCoefficient',\n",
    "        \"y_position_column\": 'pageRank'\n",
    "    }\n",
    "\n",
    "    # Annotate points with their names. Filter out values with a page rank smaller than 1.5 standard deviations\n",
    "    mean_page_rank = page_ranks.mean()\n",
    "    standard_deviation_page_rank = page_ranks.std()\n",
    "    threshold_page_rank = mean_page_rank + 1.5 * standard_deviation_page_rank\n",
    "    significant_points = combined_data[combined_data['pageRank'] > threshold_page_rank].reset_index(drop=True).head(10)\n",
    "    annotate_each_with_index(\n",
    "        significant_points,\n",
    "        using=plot.annotate,\n",
    "        value_column='pageRank',\n",
    "        **common_column_names_for_annotations\n",
    "    )\n",
    "\n",
    "    # Annotate points with the highest clustering coefficients (top 20) and only show the lowest 5 page ranks\n",
    "    combined_data['page_rank_ranking'] = combined_data['pageRank'].rank(ascending=False).astype(int)\n",
    "    combined_data['clustering_coefficient_ranking'] = combined_data['clusteringCoefficient'].rank(ascending=False).astype(int)\n",
    "    top_clustering_coefficients = combined_data.sort_values(by='clusteringCoefficient', ascending=False).reset_index(drop=True).head(20)\n",
    "    top_clustering_coefficients = top_clustering_coefficients.sort_values(by='pageRank', ascending=True).reset_index(drop=True).head(5)\n",
    "    annotate_each_with_index(\n",
    "        top_clustering_coefficients,\n",
    "        using=plot.annotate,\n",
    "        value_column='clusteringCoefficient',\n",
    "        **common_column_names_for_annotations\n",
    "    )\n",
    "\n",
    "    #plot.yscale('log')  # Use logarithmic scale for better visibility of differences\n",
    "    plot.grid(True)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b584b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_coefficient_vs_page_rank(\n",
    "    java_package_features['clusteringCoefficient'],\n",
    "    java_package_features['pageRank'],\n",
    "    java_package_features['shortCodeUnitName'],\n",
    "    java_package_features['clusterNoise'],\n",
    "    title_prefix='Java Package'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f5e4b",
   "metadata": {},
   "source": [
    "### 1.3 HDBSCAN Clusters\n",
    "\n",
    "HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that can identify clusters of varying densities and shapes. It is particularly useful for detecting anomalies in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_visualization_cluster_diameter(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    result_diameter_column_name: str = 'clusterVisualizationDiameter',\n",
    "    cluster_label_column_name: str = \"clusterLabel\",\n",
    "    x_position_column: str = \"embeddingVisualizationX\",\n",
    "    y_position_column: str = \"embeddingVisualizationY\",\n",
    "):\n",
    "    \n",
    "    def max_pairwise_distance(points):\n",
    "        if len(points) < 2:\n",
    "            return 0.0\n",
    "        # Efficient vectorized pairwise distance computation\n",
    "        dists = np.sqrt(\n",
    "            np.sum((points[:, np.newaxis, :] - points[np.newaxis, :, :]) ** 2, axis=-1)\n",
    "        )\n",
    "        return np.max(dists)\n",
    "    \n",
    "    unique_cluster_labels = clustering_visualization_dataframe[cluster_label_column_name].unique()\n",
    "    \n",
    "    if len(unique_cluster_labels) == 0:\n",
    "        return \n",
    "\n",
    "    cluster_diameters = {}\n",
    "    for cluster_label in unique_cluster_labels:\n",
    "        if cluster_label == -1:\n",
    "            cluster_diameters[-1] = 0.0\n",
    "            continue\n",
    "        \n",
    "        cluster_nodes = clustering_visualization_dataframe[\n",
    "            clustering_visualization_dataframe[cluster_label_column_name] == cluster_label\n",
    "        ]\n",
    "        cluster_diameters[cluster_label] = max_pairwise_distance(cluster_nodes[[x_position_column, y_position_column]].to_numpy())\n",
    "\n",
    "    if cluster_diameters:\n",
    "        clustering_visualization_dataframe[result_diameter_column_name] = clustering_visualization_dataframe[cluster_label_column_name].map(cluster_diameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_visualization_cluster_diameter(java_package_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18125c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_by_criteria(\n",
    "        data: pd.DataFrame,\n",
    "        by: str,\n",
    "        ascending: bool = True,\n",
    "        cluster_count: int = 10,\n",
    "        label_column_name: str = 'clusterLabel'\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Returns the rows for the \"cluster_count\" clusters with the largest (ascending=False) or smallest(ascending=True)\n",
    "    value in the column specified with \"by\". Noise (labeled with -1) remains unfiltered.\n",
    "    \"\"\"\n",
    "    if ascending:\n",
    "        threshold = data.groupby(by=label_column_name)[by].min().nsmallest(cluster_count).iloc[-1]\n",
    "        # print(f\"Ascending threshold is {threshold} for {by}.\")\n",
    "        return data[(data[by] <= threshold) | (data[label_column_name] == -1)]\n",
    "\n",
    "    threshold = data.groupby(by=label_column_name)[by].max().nlargest(cluster_count).iloc[-1]\n",
    "    # print(f\"Descending threshold is {threshold} for {by}.\")\n",
    "    return data[(data[by] >= threshold) | (data[label_column_name] == -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    title: str,\n",
    "    main_color_map: str = \"tab20\",\n",
    "    code_unit_column_name: str = \"shortCodeUnitName\",\n",
    "    cluster_label_column_name: str = \"clusterLabel\",\n",
    "    cluster_medoid_column_name: str = \"clusterMedoid\",\n",
    "    centrality_column_name: str = \"pageRank\",\n",
    "    x_position_column: str = 'embeddingVisualizationX',\n",
    "    y_position_column: str = 'embeddingVisualizationY',\n",
    "    cluster_visualization_diameter_column = 'clusterVisualizationDiameter',\n",
    "    percentile_of_distance_to_center_for_zoom: float = 1.0 # default = 1.0 = no zoom, more = nearer to zero\n",
    ") -> None:\n",
    "\n",
    "    if clustering_visualization_dataframe.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "    \n",
    "    # Create figure and subplots\n",
    "    plot.figure(figsize=(10, 10))\n",
    "\n",
    "    # Setup columns\n",
    "    node_size_column = centrality_column_name\n",
    "\n",
    "    clustering_visualization_dataframe_zoomed = zoom_into_center(\n",
    "        clustering_visualization_dataframe,\n",
    "        x_position_column,\n",
    "        y_position_column,\n",
    "        percentile_of_distance_to_center=percentile_of_distance_to_center_for_zoom\n",
    "    )\n",
    "\n",
    "    # Add column with scaled version of \"node_size_column\" for uniform marker scaling\n",
    "    clustering_visualization_dataframe_zoomed = clustering_visualization_dataframe_zoomed.copy()\n",
    "    clustering_visualization_dataframe_zoomed.loc[:, node_size_column + '_scaled'] = scale_marker_sizes(clustering_visualization_dataframe_zoomed[node_size_column])\n",
    "\n",
    "    def get_common_plot_parameters(data: pd.DataFrame) -> dict:\n",
    "        return {\n",
    "            \"x\": data[x_position_column],\n",
    "            \"y\": data[y_position_column],\n",
    "            \"s\": data[node_size_column + '_scaled'],\n",
    "        }\n",
    "\n",
    "    # Separate HDBSCAN non-noise and noise nodes\n",
    "    node_embeddings_without_noise = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[cluster_label_column_name] != -1]\n",
    "    node_embeddings_noise_only = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[cluster_label_column_name] == -1]\n",
    "    # ------------------------------------------\n",
    "    # Subplot: HDBSCAN Clustering with KDE\n",
    "    # ------------------------------------------\n",
    "    plot.title(title, pad=20)\n",
    "\n",
    "    unique_cluster_labels = node_embeddings_without_noise[cluster_label_column_name].unique()\n",
    "    hdbscan_color_palette = seaborn.color_palette(main_color_map, len(unique_cluster_labels))\n",
    "    hdbscan_cluster_to_color = dict(zip(unique_cluster_labels, hdbscan_color_palette))\n",
    "\n",
    "    max_visualization_diameter = node_embeddings_without_noise[cluster_visualization_diameter_column].max()\n",
    "    visualization_diameter_normalization_factor = max_visualization_diameter * 2\n",
    "\n",
    "    # Plot noise points in gray\n",
    "    plot.scatter(\n",
    "        **get_common_plot_parameters(node_embeddings_noise_only),\n",
    "        color='lightgrey',\n",
    "        alpha=0.4,\n",
    "        label=\"Noise\"\n",
    "    )\n",
    "    \n",
    "    for cluster_label in unique_cluster_labels:\n",
    "        cluster_nodes = node_embeddings_without_noise[\n",
    "            node_embeddings_without_noise[cluster_label_column_name] == cluster_label\n",
    "        ]\n",
    "        # By comparing the cluster diameter to the max diameter of all clusters in the quartile,\n",
    "        # we can adjust the alpha value for the KDE plot to visualize smaller clusters more clearly.\n",
    "        # This way, larger clusters will have a lower alpha value, making them less prominent and less prone to overshadow smaller clusters.\n",
    "        cluster_diameter = cluster_nodes.iloc[0][cluster_visualization_diameter_column]\n",
    "        alpha = max((1.0 - (cluster_diameter / (visualization_diameter_normalization_factor))) * 0.45 - 0.25, 0.02)\n",
    "\n",
    "        # KDE cloud shape\n",
    "        if len(cluster_nodes) > 1 and (\n",
    "            cluster_nodes[x_position_column].std() > 0 or cluster_nodes[y_position_column].std() > 0\n",
    "        ):\n",
    "            seaborn.kdeplot(\n",
    "                x=cluster_nodes[x_position_column],\n",
    "                y=cluster_nodes[y_position_column],\n",
    "                fill=True,\n",
    "                alpha=alpha,\n",
    "                levels=2,\n",
    "                color=hdbscan_cluster_to_color[cluster_label],\n",
    "                ax=plot.gca(),  # Use current axes\n",
    "                warn_singular=False,\n",
    "            )\n",
    "\n",
    "        # Node scatter points\n",
    "        plot.scatter(\n",
    "            **get_common_plot_parameters(cluster_nodes),\n",
    "            color=hdbscan_cluster_to_color[cluster_label],\n",
    "            alpha=0.9,\n",
    "            label=f\"Cluster {cluster_label}\"\n",
    "        )\n",
    "\n",
    "        # Annotate medoids of the cluster\n",
    "        medoids = cluster_nodes[cluster_nodes[cluster_medoid_column_name] == 1]\n",
    "        for index, row in medoids.iterrows():\n",
    "            plot.annotate(\n",
    "                text=f\"{truncate(row[code_unit_column_name], 30)} (cluster {row[cluster_label_column_name]})\",\n",
    "                xy=(row[x_position_column], row[y_position_column]),\n",
    "                xytext=(5, 5),  # Offset for better visibility\n",
    "                **plot_annotation_style,\n",
    "                alpha=0.6\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daff3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_features_filtered=get_clusters_by_criteria(\n",
    "    java_package_features, by='clusterSize', ascending=False, cluster_count=20\n",
    ")\n",
    "plot_clusters(\n",
    "    clustering_visualization_dataframe=java_package_features_filtered,\n",
    "    title=\"Java Package clusters with the largest size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_features_filtered=get_clusters_by_criteria(\n",
    "    java_package_features, by='clusterRadiusMax', ascending=False, cluster_count=20\n",
    ")\n",
    "plot_clusters(\n",
    "    clustering_visualization_dataframe=java_package_features_filtered,\n",
    "    title=\"Java Package clusters with the biggest max radius\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec816783",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_features_filtered=get_clusters_by_criteria(\n",
    "    java_package_features, by='clusterRadiusAverage', ascending=False, cluster_count=20\n",
    ")\n",
    "plot_clusters(\n",
    "    clustering_visualization_dataframe=java_package_features_filtered,\n",
    "    title=\"Java Package clusters with the biggest average radius\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_probabilities(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    title: str,\n",
    "    code_unit_column: str = \"shortCodeUnitName\",\n",
    "    cluster_label_column: str = \"clusterLabel\",\n",
    "    cluster_medoid_column: str = \"clusterMedoid\",\n",
    "    cluster_size_column: str = \"clusterSize\",\n",
    "    cluster_probability_column: str = \"clusterProbability\",\n",
    "    size_column: str = \"pageRank\",\n",
    "    x_position_column: str = 'embeddingVisualizationX',\n",
    "    y_position_column: str = 'embeddingVisualizationY',\n",
    "    annotate_n_lowest_probabilities: int = 10\n",
    ") -> None:\n",
    "\n",
    "    if clustering_visualization_dataframe.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "\n",
    "    clustering_visualization_dataframe_zoomed = zoom_into_center_while_preserving_lowest_scores(\n",
    "        clustering_visualization_dataframe,\n",
    "        x_position_column,\n",
    "        y_position_column,\n",
    "        cluster_probability_column,\n",
    "        annotate_n_lowest_probabilities\n",
    "    )\n",
    "\n",
    "    # Add column with scaled version of \"node_size_column\" for uniform marker scaling\n",
    "    clustering_visualization_dataframe_zoomed = clustering_visualization_dataframe_zoomed.copy()\n",
    "    clustering_visualization_dataframe_zoomed.loc[:, size_column + '_scaled'] = scale_marker_sizes(clustering_visualization_dataframe_zoomed[size_column])\n",
    "\n",
    "    def get_common_plot_parameters(data: pd.DataFrame) -> dict:\n",
    "        return {\n",
    "            \"x\": data[x_position_column],\n",
    "            \"y\": data[y_position_column],\n",
    "            \"s\": data[size_column + '_scaled'],\n",
    "        }\n",
    "\n",
    "    cluster_noise = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[cluster_label_column] == -1]\n",
    "    cluster_non_noise = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[cluster_label_column] != -1]\n",
    "    cluster_even_labels = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[cluster_label_column] % 2 == 0]\n",
    "    cluster_odd_labels = clustering_visualization_dataframe_zoomed[clustering_visualization_dataframe_zoomed[cluster_label_column] % 2 == 1]\n",
    "\n",
    "    plot.figure(figsize=(10, 10))\n",
    "    plot.title(title, pad=20)\n",
    "\n",
    "    # Plot noise\n",
    "    plot.scatter(\n",
    "        **get_common_plot_parameters(cluster_noise),\n",
    "        color='lightgrey',\n",
    "        alpha=0.4,\n",
    "        label='Noise'\n",
    "    )\n",
    "\n",
    "    # Plot even labels\n",
    "    plot.scatter(\n",
    "        **get_common_plot_parameters(cluster_even_labels),\n",
    "        c=cluster_even_labels[cluster_probability_column],\n",
    "        vmin=0.6,\n",
    "        vmax=1.0,\n",
    "        cmap='Greens',\n",
    "        alpha=0.8,\n",
    "        label='Even Label'\n",
    "    )\n",
    "\n",
    "    # Plot odd labels\n",
    "    plot.scatter(\n",
    "        **get_common_plot_parameters(cluster_odd_labels),\n",
    "        c=cluster_odd_labels[cluster_probability_column],\n",
    "        vmin=0.6,\n",
    "        vmax=1.0,\n",
    "        cmap='Blues',\n",
    "        alpha=0.8,\n",
    "        label='Odd Label'\n",
    "    )\n",
    "\n",
    "    # Find center node of each cluster (medoid), sort them by cluster size descending and add a mean cluster probability column\n",
    "    cluster_medoids = cluster_non_noise[cluster_non_noise[cluster_medoid_column] == 1]\n",
    "    cluster_medoids_by_cluster_size = cluster_medoids.sort_values(by=cluster_size_column, ascending=False).head(20)\n",
    "    mean_probabilities = cluster_non_noise.groupby(cluster_label_column)[cluster_probability_column].mean().rename('mean_cluster_probability')\n",
    "    cluster_medoids_with_mean_probabilites = cluster_medoids_by_cluster_size.merge(mean_probabilities, on=cluster_label_column, how='left').reset_index()\n",
    "\n",
    "    # Annotate medoids of the cluster\n",
    "    for index, row in cluster_medoids_with_mean_probabilites.iterrows():\n",
    "        plot.annotate(\n",
    "            text=f\"{truncate(row[code_unit_column])} (cluster {row[cluster_label_column]}) (p={row['mean_cluster_probability']:.3f})\",\n",
    "            xy=(row[x_position_column], row[y_position_column]),\n",
    "            xytext=(5, 5),\n",
    "            alpha=0.4,\n",
    "            **plot_annotation_style\n",
    "        )\n",
    "\n",
    "    lowest_probabilities = cluster_non_noise.sort_values(by=cluster_probability_column, ascending=True).reset_index().head(annotate_n_lowest_probabilities)\n",
    "    annotate_each_with_index(\n",
    "        lowest_probabilities,\n",
    "        using=plot.annotate,\n",
    "        name_column=code_unit_column,\n",
    "        x_position_column=x_position_column,\n",
    "        y_position_column=y_position_column,\n",
    "        probability_column=cluster_probability_column,\n",
    "        color=\"red\"\n",
    "    )\n",
    "\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95071ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_probabilities(java_package_features, \"Java Package clustering probabilities (red=high uncertainty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9580ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_noise(\n",
    "    clustering_visualization_dataframe: pd.DataFrame,\n",
    "    title: str,\n",
    "    code_unit_column_name: str = \"shortCodeUnitName\",\n",
    "    cluster_label_column_name: str = \"clusterLabel\",\n",
    "    size_column_name: str = \"degree\",\n",
    "    color_column_name: str = \"pageRank\",\n",
    "    x_position_column = 'embeddingVisualizationX',\n",
    "    y_position_column = 'embeddingVisualizationY',\n",
    "    downscale_normal_sizes: float = 0.8\n",
    ") -> None:\n",
    "    if clustering_visualization_dataframe.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "\n",
    "    # Filter only noise points\n",
    "    noise_points = clustering_visualization_dataframe[clustering_visualization_dataframe[cluster_label_column_name] == -1]\n",
    "    noise_points = noise_points.sort_values(by=size_column_name, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if noise_points.empty:\n",
    "        print(\"No noise points to plot.\")\n",
    "        return\n",
    "\n",
    "    plot.figure(figsize=(10, 10))\n",
    "    plot.suptitle(title, fontsize=12)\n",
    "    plot.title(f\"red, annotation value=${color_column_name}$, size=${size_column_name}$\", fontsize=10, pad=30)\n",
    "\n",
    "    # Determine the color threshold for noise points\n",
    "    color_10th_highest_value = noise_points[color_column_name].nlargest(10).iloc[-1]  # Get the 10th largest value\n",
    "    color_90_quantile = noise_points[color_column_name].quantile(0.90)\n",
    "    color_threshold = max(color_10th_highest_value, color_90_quantile)\n",
    "\n",
    "    noise_points_zoomed = zoom_into_center_while_preserving_scores_above_threshold(\n",
    "        noise_points,\n",
    "        x_position_column,\n",
    "        y_position_column,\n",
    "        color_column_name,\n",
    "        color_threshold\n",
    "    )\n",
    "\n",
    "    # Add column with scaled version of \"node_size_column\" for uniform marker scaling\n",
    "    noise_points_zoomed = noise_points_zoomed.copy()\n",
    "    noise_points_zoomed.loc[:, size_column_name + '_scaled'] = scale_marker_sizes(noise_points_zoomed[size_column_name], downscale_factor=downscale_normal_sizes)\n",
    "\n",
    "    normal_noise_points = noise_points_zoomed[noise_points_zoomed[color_column_name] < color_threshold]\n",
    "    highlighted_noise_points = noise_points_zoomed[noise_points_zoomed[color_column_name] >= color_threshold]\n",
    "\n",
    "    def get_common_plot_parameters(data: pd.DataFrame) -> dict:\n",
    "        return {\n",
    "            \"x\": data[x_position_column],\n",
    "            \"y\": data[y_position_column],\n",
    "            \"s\": data[size_column_name + '_scaled'],\n",
    "        }\n",
    "\n",
    "    # Scatter plot for noise points\n",
    "    plot.scatter(\n",
    "        **get_common_plot_parameters(normal_noise_points),\n",
    "        color=\"lightgrey\",\n",
    "        alpha=0.5\n",
    "    )\n",
    "\n",
    "    # Scatter plot for highlighted noise points\n",
    "    plot.scatter(\n",
    "        **get_common_plot_parameters(highlighted_noise_points),\n",
    "        color=\"red\",\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Annotate the largest 10 points and all colored ones with their names\n",
    "    annotate_each_with_index(\n",
    "        data=highlighted_noise_points,\n",
    "        using=plot.annotate,\n",
    "        name_column=code_unit_column_name,\n",
    "        x_position_column=x_position_column,\n",
    "        y_position_column=y_position_column,\n",
    "        value_column=color_column_name,\n",
    "        color=\"red\"\n",
    "    )\n",
    "\n",
    "    plot.xlabel(x_position_column)\n",
    "    plot.ylabel(y_position_column)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_noise(\n",
    "    clustering_visualization_dataframe=java_package_features,\n",
    "    title=\"Java Package clustering noise points that are surprisingly central (red) or popular (size)\",\n",
    "    size_column_name='degree',\n",
    "    color_column_name='pageRank'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b2010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_noise(\n",
    "    clustering_visualization_dataframe=java_package_features,\n",
    "    title=\"Java Package clustering noise points that bridge flow (red) and are poorly integrated (size)\",\n",
    "    size_column_name='inverseClusteringCoefficient',\n",
    "    color_column_name='betweenness',\n",
    "    downscale_normal_sizes=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_noise(\n",
    "    clustering_visualization_dataframe=java_package_features,\n",
    "    title=\"Java Package clustering noise points with role inversion (size) possibly violating layering or dependency direction (red)\",\n",
    "    size_column_name='pageToArticleRankDifference',\n",
    "    color_column_name='betweenness'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682bb64",
   "metadata": {},
   "source": [
    "## 2. Java Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_features = query_data(\"Type\")\n",
    "display(java_type_features.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25370d7f",
   "metadata": {},
   "source": [
    "### 2.1 Differences between Page Rand and Article Rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_anomaly_detection_centrality_features_query = \"\"\"\n",
    "    MATCH (artifact:Java:Artifact)-[:CONTAINS]->(codeUnit:Java:Type)\n",
    "    WHERE codeUnit.incomingDependencies                        IS NOT NULL\n",
    "      AND codeUnit.outgoingDependencies                        IS NOT NULL\n",
    "      AND codeUnit.centralityArticleRank                       IS NOT NULL\n",
    "      AND codeUnit.centralityPageRank                          IS NOT NULL\n",
    "      AND codeUnit.centralityBetweenness                       IS NOT NULL\n",
    "   RETURN DISTINCT \n",
    "         codeUnit.fqn                                         AS codeUnitName\n",
    "        ,codeUnit.name                                        AS shortCodeUnitName\n",
    "        ,artifact.name                                        AS projectName\n",
    "        ,codeUnit.incomingDependencies                        AS incomingDependencies\n",
    "        ,codeUnit.outgoingDependencies                        AS outgoingDependencies\n",
    "        ,codeUnit.centralityArticleRank                       AS articleRank\n",
    "        ,codeUnit.centralityPageRank                          AS pageRank\n",
    "        ,codeUnit.centralityBetweenness                       AS betweenness\n",
    "\"\"\"\n",
    "\n",
    "java_type_anomaly_detection_centrality_features = query_cypher_to_data_frame(java_type_anomaly_detection_centrality_features_query)\n",
    "display(java_type_anomaly_detection_centrality_features.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6119f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_difference_between_article_and_page_rank(\n",
    "    java_type_features['pageRank'],\n",
    "    java_type_features['articleRank'],\n",
    "    java_type_features['shortCodeUnitName'],\n",
    "    title_prefix='Java Type'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4965c",
   "metadata": {},
   "source": [
    "### 2.2 Local Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_coefficient_distribution(java_type_features['clusteringCoefficient'], title_prefix=\"Java Package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_coefficient_vs_page_rank(\n",
    "    java_type_features['clusteringCoefficient'],\n",
    "    java_type_features['pageRank'],\n",
    "    java_type_features['shortCodeUnitName'],\n",
    "    java_type_features['clusterNoise'],\n",
    "    title_prefix='Java Type'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69256999",
   "metadata": {},
   "source": [
    "### 2.3 HDBSCAN Clusters\n",
    "\n",
    "HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that can identify clusters of varying densities and shapes. It is particularly useful for detecting anomalies in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_visualization_cluster_diameter(java_type_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_features_filtered=get_clusters_by_criteria(\n",
    "    java_type_features, by='clusterSize', ascending=False, cluster_count=20\n",
    ")\n",
    "plot_clusters(\n",
    "    clustering_visualization_dataframe=java_type_features_filtered,\n",
    "    title=\"Java Type clusters with the largest size\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881783e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_features_filtered=get_clusters_by_criteria(\n",
    "    java_type_features, by='clusterRadiusMax', ascending=False, cluster_count=20\n",
    ")\n",
    "plot_clusters(\n",
    "    clustering_visualization_dataframe=java_type_features_filtered,\n",
    "    title=\"Java Type clusters with the biggest max radius\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_features_filtered=get_clusters_by_criteria(\n",
    "    java_type_features, by='clusterRadiusAverage', ascending=False, cluster_count=20\n",
    ")\n",
    "plot_clusters(\n",
    "    clustering_visualization_dataframe=java_type_features_filtered,\n",
    "    title=\"Java Type clusters with the biggest average radius\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_probabilities(java_type_features, \"Java Type clustering probabilities (red=high uncertainty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ec20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_noise(\n",
    "    clustering_visualization_dataframe=java_type_features,\n",
    "    title=\"Java Type clustering noise points that are surprisingly central (red) or popular (size)\",\n",
    "    size_column_name='degree',\n",
    "    color_column_name='pageRank'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d888be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_noise(\n",
    "    clustering_visualization_dataframe=java_type_features,\n",
    "    title=\"Java Type clustering noise points that bridge flow (red) and are poorly integrated (size)\",\n",
    "    size_column_name='inverseClusteringCoefficient',\n",
    "    color_column_name='betweenness',\n",
    "    downscale_normal_sizes=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9921ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_noise(\n",
    "    clustering_visualization_dataframe=java_type_features,\n",
    "    title=\"Java Type clustering noise points with role inversion (size) possibly violating layering or dependency direction (red)\",\n",
    "    size_column_name='pageToArticleRankDifference',\n",
    "    color_column_name='betweenness'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19552eea",
   "metadata": {},
   "source": [
    "### 2.4 Best Pareto Frontier tradeoff feature combinations and archetypes\n",
    "\n",
    "Multi objective optimization for anomaly detection. Combining multiple metrics to identify anomalies that may not be apparent when considering each metric in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba89c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rank_column(\n",
    "    data: pd.DataFrame,\n",
    "    value_column_name: str,\n",
    "    ascending: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a ranking column to the DataFrame based on the specified value column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    value_column_name : str\n",
    "        The name of the column based on which the ranking is computed.\n",
    "    ascending : bool, optional\n",
    "        If True, ranks in ascending order (default is False for descending order).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with the new ranking column added.\n",
    "    \"\"\"\n",
    "    if value_column_name not in data.columns:\n",
    "        raise ValueError(f\"Column '{value_column_name}' does not exist in the DataFrame.\")\n",
    "    if data.empty:\n",
    "        print(\"DataFrame is empty. No ranking column added.\")\n",
    "        return data\n",
    "    if value_column_name + '_ranking' in data.columns:\n",
    "        print(f\"Ranking column '{value_column_name}_ranking' already exists. No new column added.\")\n",
    "        return data\n",
    "    data[value_column_name + '_ranking'] = data[value_column_name].rank(ascending=ascending, method='dense').astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_unit_columns = ['projectName', 'codeUnitName']\n",
    "features_to_rank = ['anomalyScore', 'degree', 'pageRank', 'articleRank', 'pageToArticleRankDifference', 'betweenness', 'negatedAnomalyNodeEmbeddingSHAPSum',\n",
    "                    'inverseClusteringCoefficient', 'clusterApproximateOutlierScore', 'clusterRadiusAverage', 'clusterSize', 'clusterDistanceToMedoid']\n",
    "\n",
    "for feature in features_to_rank:\n",
    "    java_type_features = add_rank_column(java_type_features, feature, ascending=False)\n",
    "\n",
    "# display(java_type_features.sort_values(by='anomalyScore', ascending=False)[code_unit_columns + features_to_rank].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64071a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_frontier(input_data, metrics, maximize=True):\n",
    "    \"\"\"\n",
    "    Extracts the Pareto frontier (skyline) from a DataFrame.\n",
    "\n",
    "    input_data: DataFrame\n",
    "    metrics: list of column names to consider\n",
    "    maximize: True if higher is better for all metrics\n",
    "    \"\"\"\n",
    "    data = input_data[metrics].to_numpy()\n",
    "    if not maximize:\n",
    "        data = -data  # flip sign if minimizing\n",
    "    \n",
    "    # Keep track of which rows are dominated (start with none)\n",
    "    is_dominated = np.zeros(len(data), dtype=bool)\n",
    "    for i, point in enumerate(data):\n",
    "        # Skip if already marked dominated\n",
    "        if is_dominated[i]:\n",
    "            continue\n",
    "        # Check which other rows dominate this row\n",
    "        dominates = np.all(data >= point, axis=1) & np.any(data > point, axis=1)\n",
    "        # If any row dominates this one, mark this row as dominated\n",
    "        is_dominated |= dominates\n",
    "    \n",
    "    # Keep only non-dominated rows = Pareto frontier\n",
    "    return input_data[~is_dominated].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_feature_tradeoff_code_units(\n",
    "    data: pd.DataFrame,\n",
    "    feature_names: list[str],\n",
    "    code_unit_columns: list[str] = ['projectName', 'shortCodeUnitName', 'codeUnitName'],\n",
    "    top_n: int = 10\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identifies code units that represent the best trade-offs across multiple features using the Pareto frontier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The input DataFrame containing code unit features.\n",
    "    features : list of str\n",
    "        List of feature column names to consider for the Pareto frontier.\n",
    "    code_unit_columns : list of str\n",
    "        List of columns that identify the code units (e.g., name, project) (default is ['projectName', 'codeUnitName']).\n",
    "    top_n : int, optional\n",
    "        Number of top code units to return from the Pareto frontier (default is 20).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the top code units on the Pareto frontier with their features.\n",
    "    \"\"\"\n",
    "    if data.empty:\n",
    "        print(\"DataFrame is empty. No Pareto frontier can be computed.\")\n",
    "        return data\n",
    "\n",
    "    features_rank_columns = [feature + '_ranking' for feature in feature_names]\n",
    "    selected_columns = code_unit_columns + feature_names + features_rank_columns\n",
    "    pareto_best_feature_tradeoffs = pareto_frontier(java_type_features, feature_names, maximize=False)\n",
    "    return pareto_best_feature_tradeoffs[selected_columns].head(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d270b",
   "metadata": {},
   "source": [
    "#### 2.4.0 Pareto best trade-offs of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8445365",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_best_feature_tradeoff_code_units(java_type_features, features_to_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3adf7",
   "metadata": {},
   "source": [
    "#### 2.4.1 Hub (High degree, low clustering coefficient) - Best Pareto feature trade-offs\n",
    "\n",
    "**Definition:**\n",
    "A node with unusually high **degree centrality** (many direct connections) compared to its peers, often with **low clustering coefficient** (its neighbors are not connected to each other).\n",
    "\n",
    "**In software:**\n",
    "\n",
    "* A class/package/module that is used **everywhere** → often “God classes” or utility-heavy components.\n",
    "* Can indicate **violation of modularity** or **overgeneralization** (too many responsibilities).\n",
    "\n",
    "**Implications:**\n",
    "\n",
    "* Increases **coupling**, reduces maintainability.\n",
    "* Single point of failure: refactoring or breaking changes ripple through the system.\n",
    "\n",
    "**Variants:**\n",
    "* In-degree hub (high fan-in): Many other code units depend on this one.Indicates re-use, but also high coupling. Classic sign of God Class / Utility Class (everywhere referenced).\n",
    "* Out-degree hub (high fan-out): This code unit depends on many others. Indicates broad knowledge of the system. Often suggests Feature Envy or Controller classes (too many responsibilities).\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Lanza & Marinescu, *Object-Oriented Metrics in Practice* (Springer, 2006) – “God Class” anti-pattern.\n",
    "* Barabási, *Network Science* (Cambridge, 2016) – scale-free networks, hub nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_focussed_features = ['anomalyScore', 'degree', 'inverseClusteringCoefficient']\n",
    "display(get_best_feature_tradeoff_code_units(java_type_features, hub_focussed_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfd235",
   "metadata": {},
   "source": [
    "#### 2.4.2 Bottleneck (High betweenness, low redundancy) best Pareto feature trade-offs\n",
    "\n",
    "**Definition:**\n",
    "A node with very high **betweenness centrality** – it lies on many shortest paths between other nodes.\n",
    "\n",
    "**In software:**\n",
    "\n",
    "* A package/module that acts as a **bridge between subsystems**.\n",
    "* Often an **unintended dependency concentration**: if removed, communication between modules breaks.\n",
    "\n",
    "**Implications:**\n",
    "\n",
    "* Scalability risk: changes here affect many modules.\n",
    "* Architectural smell: “concentration of control.”\n",
    "\n",
    "**References:**\n",
    "\n",
    "* MacCormack et al., *Exploring the Structure of Complex Software Designs* (Management Science, 2006) – dependency bottlenecks in software.\n",
    "* Freeman, *Centrality in Social Networks* (Social Networks, 1977) – betweenness centrality.\n",
    "* Valverde & Solé (2003): \"Hierarchical small worlds in software architecture\" → showed how real software dependency graphs often lack redundancy and thus create fragile bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_focussed_features = ['anomalyScore', 'betweenness', 'clusterApproximateOutlierScore']\n",
    "display(get_best_feature_tradeoff_code_units(java_type_features, bottleneck_focussed_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc86c29",
   "metadata": {},
   "source": [
    "#### 2.4.3 Outlier (High cluster distance, small cluster size) best Pareto feature trade-offs\n",
    "\n",
    "**Definition:**\n",
    "A node that is **structurally far away** from its assigned cluster/community (large distance to medoid, very small cluster size).\n",
    "\n",
    "**In software:**\n",
    "\n",
    "* A class/module that doesn’t fit into any architectural layer cleanly.\n",
    "* Example: a utility hidden inside a domain-specific cluster, or a feature with **no clear dependencies**.\n",
    "\n",
    "**Implications:**\n",
    "\n",
    "* Possible **code smell**: “orphan” or “misplaced class.”\n",
    "* Hard to reason about, maintain, or assign ownership.\n",
    "* Unusual dependency pattern\n",
    "* Architectural mismatch when approximate outlier score is also high\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Koschke, *Software Clustering: Extracting Structure from Source Code* (IEEE TSE, 2006).\n",
    "* Hinneburg & Keim, *Optimal Grid-Clustering* (VLDB, 1999) – cluster outliers in high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73daaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_focussed_features = ['anomalyScore', 'clusterDistanceToMedoid', 'clusterApproximateOutlierScore']\n",
    "display(get_best_feature_tradeoff_code_units(java_type_features, outlier_focussed_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df5cce",
   "metadata": {},
   "source": [
    "#### 2.4.4 Authority (High PageRank, low articleRank) best Pareto feature trade-offs\n",
    "\n",
    "**Definition:**\n",
    "A node with **high PageRank** but relatively **low ArticleRank** or similar ranking mismatch → suggests **influence disproportionate to usage context**.\n",
    "\n",
    "**In software:**\n",
    "\n",
    "* A module referenced widely but not strongly contributing back (utility libraries, framework entry points).\n",
    "* Could indicate **monopoly dependencies** (e.g., logging frameworks, base classes).\n",
    "\n",
    "**Implications:**\n",
    "\n",
    "* Central authority role can be intended (core library), but in anomaly context, it may indicate **over-centralization**.\n",
    "* A design smell: one class \"knows too much\" or others depend on it excessively.\n",
    "* Over-relied utility with few reverse connections.\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Kleinberg, *Authoritative Sources in a Hyperlinked Environment* (JACM, 1999) – HITS algorithm.\n",
    "* Page et al., *The PageRank Citation Ranking* (Stanford Tech Report, 1999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c53e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_focussed_features = ['anomalyScore', 'degree', 'pageRank', 'pageToArticleRankDifference']\n",
    "display(get_best_feature_tradeoff_code_units(java_type_features, authority_focussed_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f9732",
   "metadata": {},
   "source": [
    "#### 2.4.5 Bridge (Embedding-driven anomaly, cross-cluster) best Pareto feature trade-offs\n",
    "\n",
    "**Definition:**\n",
    "A node whose embedding or SHAP contribution comes from **latent dimensions** (e.g., PCA components) rather than raw structural metrics → meaning it connects across **otherwise unrelated clusters**.\n",
    "\n",
    "**In software:**\n",
    "\n",
    "* A class/module that integrates concepts from multiple subsystems.\n",
    "* May appear in embeddings as a “boundary object” that doesn’t belong to just one cluster.\n",
    "\n",
    "**Implications:**\n",
    "\n",
    "* Can be **legitimate integrators** (e.g., API facades) or **architecture violations** (tangled dependencies).\n",
    "* Increases coupling between modules that should be independent.\n",
    "* Connects unrelated domains, risky coupling\n",
    "\n",
    "**References:**\n",
    "\n",
    "* Conway’s Law (Conway, 1968) – bridges often mirror organizational seams.\n",
    "* Borgatti & Everett, *Models of Core/Periphery Structures* (Social Networks, 2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge_focussed_features = ['anomalyScore', 'negatedAnomalyNodeEmbeddingSHAPSum']\n",
    "display(get_best_feature_tradeoff_code_units(java_type_features, bridge_focussed_features))"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "JohT"
   }
  ],
  "code_graph_analysis_pipeline_data_validation": "ValidateAlwaysFalse",
  "kernelspec": {
   "display_name": "codegraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Anomaly Detection - Manual Exploration"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
