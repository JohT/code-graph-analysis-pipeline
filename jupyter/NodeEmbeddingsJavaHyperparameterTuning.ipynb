{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f0eabc4",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning of Java Node Embeddings\n",
    "\n",
    "This notebook demonstrates different methods for node embeddings and how to further reduce their dimensionality to be able to visualize them in a 2D plot. \n",
    "\n",
    "Node embeddings are essentially an array of floating point numbers (length = embedding dimension) that can be used as \"features\" in machine learning. These numbers approximate the relationship and similarity information of each node and can also be seen as a way to encode the topology of the graph.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "Due to dimensionality reduction some information gets lost, especially when visualizing node data in two dimensions. Nevertheless, it helps to get an intuition on what node embeddings are and how much of the similarity and neighborhood information is retained. The latter can be observed by how well nodes of the same color and therefore same community are placed together and how much bigger nodes with a high centrality score influence them. \n",
    "\n",
    "If the visualization doesn't show a somehow clear separation between the communities (colors) here are some ideas for tuning: \n",
    "- Clean the data, e.g. filter out very few nodes with extremely high degree that aren't actually that important\n",
    "- Try directed vs. undirected projections\n",
    "- Tune the embedding algorithm, e.g. use a higher dimensionality\n",
    "- Tune t-SNE that is used to reduce the node embeddings dimension to two dimensions for visualization. \n",
    "\n",
    "It could also be the case that the node embeddings are good enough and well suited the way they are despite their visualization for the down stream task like node classification or link prediction. In that case it makes sense to see how the whole pipeline performs before tuning the node embeddings in detail. \n",
    "\n",
    "## Note about data dependencies\n",
    "\n",
    "PageRank centrality and Leiden community are also fetched from the Graph and need to be calculated first.\n",
    "This makes it easier to see if the embeddings approximate the structural information of the graph in the plot.\n",
    "If these properties are missing you will only see black dots all of the same size.\n",
    "\n",
    "<br>  \n",
    "\n",
    "### References\n",
    "- [jqassistant](https://jqassistant.org)\n",
    "- [Neo4j Python Driver](https://neo4j.com/docs/api/python-driver/current)\n",
    "- [Tutorial: Applied Graph Embeddings](https://neo4j.com/developer/graph-data-science/applied-graph-embeddings)\n",
    "- [Visualizing the embeddings in 2D](https://github.com/openai/openai-cookbook/blob/main/examples/Visualizing_embeddings_in_2D.ipynb)\n",
    "- [scikit-learn TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE)\n",
    "- [AttributeError: 'list' object has no attribute 'shape'](https://bobbyhadz.com/blog/python-attributeerror-list-object-has-no-attribute-shape)\n",
    "- [Fast Random Projection (neo4j)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp)\n",
    "- [HashGNN (neo4j)](https://neo4j.com/docs/graph-data-science/2.6/machine-learning/node-embeddings/hashgnn)\n",
    "- [node2vec (neo4j)](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/node2vec) computes a vector representation of a node based on second order random walks in the graph. \n",
    "- [Complete guide to understanding Node2Vec algorithm](https://towardsdatascience.com/complete-guide-to-understanding-node2vec-algorithm-4e9a35e5d147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import contextlib\n",
    "\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import typing as typ\n",
    "import numpy as np\n",
    "from openTSNE.sklearn import TSNE\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following cell uses the build-in %html \"magic\" to override the CSS style for tables to a much smaller size.\n",
    "#This is especially needed for PDF export of tables with multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS style for smaller dataframe tables. */\n",
    ".dataframe th {\n",
    "    font-size: 8px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 8px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07319282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Colormap\n",
    "# main_color_map = 'nipy_spectral'\n",
    "main_color_map = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import __version__ as matplotlib_version\n",
    "print('matplotlib version: {}'.format(matplotlib_version))\n",
    "\n",
    "from numpy import __version__ as numpy_version\n",
    "print('numpy version: {}'.format(numpy_version))\n",
    "\n",
    "from openTSNE import __version__ as openTSNE_version\n",
    "print('openTSNE version: {}'.format(openTSNE_version))\n",
    "\n",
    "from pandas import __version__ as pandas_version\n",
    "print('pandas version: {}'.format(pandas_version))\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('scikit-learn version: {}'.format(sklearn_version))\n",
    "\n",
    "from seaborn import __version__ as seaborn_version  # type: ignore\n",
    "print('seaborn version: {}'.format(seaborn_version))\n",
    "\n",
    "from optuna import __version__ as optuna_version\n",
    "print('optuna version: {}'.format(optuna_version))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set the environment variable \"NEO4J_INITIAL_PASSWORD\" in your shell \n",
    "# before starting jupyter notebook to provide the password for the user \"neo4j\". \n",
    "# It is not recommended to hardcode the password into jupyter notebook for security reasons.\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\n",
    "    uri=\"bolt://localhost:7687\", \n",
    "    auth=(\"neo4j\", os.environ.get(\"NEO4J_INITIAL_PASSWORD\"))\n",
    ")\n",
    "driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cypher_query_from_file(filename) -> str:\n",
    "    with open(filename) as file:\n",
    "        return ' '.join(file.readlines())\n",
    "    \n",
    "\n",
    "def query_cypher_to_data_frame(filename, parameters: typ.Optional[typ.Dict[str, typ.Any]] = None):\n",
    "    records, summary, keys = driver.execute_query(query_=get_cypher_query_from_file(filename), parameters_=parameters)\n",
    "    return pd.DataFrame([r.values() for r in records], columns=keys)\n",
    "\n",
    "\n",
    "def query_cypher_to_data_frame_suppress_warnings(filename, parameters: typ.Optional[typ.Dict[str, typ.Any]] = None):\n",
    "    \"\"\"\n",
    "    Executes the Cypher query in the given file and returns the result as a pandas DataFrame.\n",
    "    This function suppresses any warnings or error messages that would normally be printed to stderr.\n",
    "    This is useful when you want to run a query without cluttering the output with warnings.\n",
    "    Parameters:\n",
    "    - filename: The name of the file containing the Cypher query.\n",
    "    - parameters: Optional dictionary of parameters to pass to the Cypher query.\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing the results of the Cypher query.\n",
    "    \"\"\"\n",
    "    import contextlib\n",
    "    with open(os.devnull, 'w') as devnull, contextlib.redirect_stderr(devnull):\n",
    "        return query_cypher_to_data_frame(filename, parameters)\n",
    "\n",
    "def query_cypher_to_data_frame_for_verbosity(verbose: bool) -> typ.Callable:\n",
    "    \"\"\"\n",
    "    Returns a function that executes a Cypher query from a file and returns the result as a pandas DataFrame.\n",
    "    If verbose is True, it returns a function that prints warnings and errors to stderr.\n",
    "    If verbose is False, it returns a function that suppresses warnings and errors.\n",
    "    Parameters:\n",
    "    - verbose: A boolean indicating whether to print warnings and errors.\n",
    "    Returns:\n",
    "    - A function that takes a filename and optional parameters, and returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return query_cypher_to_data_frame if verbose else query_cypher_to_data_frame_suppress_warnings\n",
    "\n",
    "def query_first_non_empty_cypher_to_data_frame(*filenames : str, parameters: typ.Optional[typ.Dict[str, typ.Any]] = None):\n",
    "    \"\"\"\n",
    "    Executes the Cypher queries of the given files and returns the first result that is not empty.\n",
    "    If all given file names result in empty results, the last (empty) result will be returned.\n",
    "    By additionally specifying \"limit=\" the \"LIMIT\" keyword will appended to query so that only the first results get returned.\n",
    "    \"\"\"\n",
    "    result=pd.DataFrame()\n",
    "    for filename in filenames:\n",
    "        result=query_cypher_to_data_frame(filename, parameters)\n",
    "        if not result.empty:\n",
    "            print(\"The results have been provided by the query filename: \" + filename)\n",
    "            return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batch_data_into_database(dataframe: pd.DataFrame, node_label: str, id_column: str = \"nodeElementId\", cypher_query_file: str = \"../cypher/Dependencies_Projection/Dependencies_14_Write_Batch_Data.cypher\", batch_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Writes the given dataframe to the Neo4j database using a batch write operation.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: The pandas DataFrame to write.\n",
    "    - label: The label to use for the nodes in the Neo4j database.\n",
    "    - id_column: The name of the column in the DataFrame that contains the node IDs.\n",
    "    - cypher_query_file: The file containing the Cypher query for writing the data.\n",
    "    - batch_size: The number of rows to write in each batch.\n",
    "    \"\"\"\n",
    "    def prepare_rows(dataframe):\n",
    "        rows = []\n",
    "        for _, row in dataframe.iterrows():\n",
    "            properties_without_id = row.drop(labels=[id_column]).to_dict()\n",
    "            rows.append({\n",
    "                \"nodeId\": row[id_column],\n",
    "                \"properties\": properties_without_id\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    def update_batch(transaction, rows):\n",
    "        query = get_cypher_query_from_file(cypher_query_file)\n",
    "        transaction.run(query, dependencies_projection_rows=rows, dependencies_projection_node=node_label)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for start in range(0, len(dataframe), batch_size):\n",
    "            batch_dataframe = dataframe.iloc[start:start + batch_size]\n",
    "            batch_rows = prepare_rows(batch_dataframe)\n",
    "            return session.execute_write(update_batch, batch_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO option to choose between directed and undirected projection\n",
    "\n",
    "def create_undirected_projection(parameters: dict) -> bool: \n",
    "    \"\"\"\n",
    "    Creates an undirected homogenous in-memory Graph projection for/with Neo4j Graph Data Science Plugin.\n",
    "    It returns True if there is data available for the given parameter and False otherwise.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dependencies_projection : str\n",
    "        The name prefix for the in-memory projection for dependencies. Example: \"java-package-embeddings-notebook\"\n",
    "    dependencies_projection_node : str\n",
    "        The label of the nodes that will be used for the projection. Example: \"Package\"\n",
    "    dependencies_projection_weight_property : str\n",
    "        The name of the node property that contains the dependency weight. Example: \"weight25PercentInterfaces\"\n",
    "    \"\"\"\n",
    "    \n",
    "    is_data_missing=query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_0_Check_Projectable.cypher\", parameters).empty\n",
    "    if is_data_missing: return False\n",
    "\n",
    "    query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_1_Delete_Projection.cypher\", parameters)\n",
    "    query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_2_Delete_Subgraph.cypher\", parameters)\n",
    "    query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_1_Delete_Projection.cypher\", dict(dependencies_projection=parameters[\"dependencies_projection\"] + '-cleaned-sampled'))\n",
    "    # To include the direction of the relationships use the following line to create the projection:\n",
    "    # query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_3_Create_Projection.cypher\", parameters)\n",
    "    node_count : int = 0\n",
    "    if parameters[\"dependencies_projection_node\"] == \"Type\":\n",
    "        results=query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_4c_Create_Undirected_Java_Type_Projection.cypher\", parameters)\n",
    "        node_count=results[\"nodeCount\"].values[0]\n",
    "    else:\n",
    "        query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_4_Create_Undirected_Projection.cypher\", parameters)\n",
    "        results=query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_5_Create_Subgraph.cypher\", parameters)\n",
    "        node_count=results[\"nodeCount\"].values[0]\n",
    "    \n",
    "    print(\"The number of nodes in the original projection is: \" + str(node_count))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as numpy_typing\n",
    "import numpy as np\n",
    "\n",
    "def get_projected_graph_information(projection_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the projection information for the given parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    projection_name : str\n",
    "        The name prefix for the in-memory projection for dependencies. Example: \"java-package-embeddings-notebook\"\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = dict(\n",
    "        dependencies_projection=projection_name,\n",
    "    )\n",
    "    return query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_12_Get_Projection_Statistics.cypher\", parameters)\n",
    "\n",
    "\n",
    "def get_projected_graph_node_count(projection_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of nodes in the projected graph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    projection_name : str\n",
    "        The name prefix for the in-memory projection for dependencies. Example: \"java-package-embeddings-notebook\"\n",
    "    \"\"\"\n",
    "    \n",
    "    graph_information = get_projected_graph_information(projection_name)\n",
    "    if graph_information.empty:\n",
    "        return 0\n",
    "    return graph_information[\"nodeCount\"].values[0]\n",
    "\n",
    "\n",
    "def get_all_data_without_slicing_cross_validator_for_node_count(node_count: int) -> typ.List[typ.Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Returns a list with a single tuple containing the node indices for cross-validation so that all data is used for training and testing.\n",
    "    This is useful for the case when no slicing is applied, i.e., all data is used for training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_count : int\n",
    "        The number of nodes in the projected graph.\n",
    "    \"\"\"\n",
    "    node_indices = np.arange(node_count)\n",
    "    all_data_without_slicing_cross_validator = [(node_indices, node_indices)]\n",
    "    return all_data_without_slicing_cross_validator\n",
    "\n",
    "\n",
    "def get_initial_dummy_data_for_hyperparameter_tuning(\n",
    "    node_count: int, \n",
    ") -> numpy_typing.NDArray:\n",
    "    \"\"\"\n",
    "    Returns a list with a single tuple containing the node indices as dummy data for hyperparameter tuning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node_count : int\n",
    "        The number of nodes in the projected graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    node_indices = np.arange(node_count)\n",
    "    return node_indices.reshape(-1, 1) # Reshape to fit the model's shape requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5068985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSamplingResult:\n",
    "    \"\"\"\n",
    "    A class to represent the result of a graph sampling operation.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO Make the sampling threshold configurable by environment variable \n",
    "    # The choses default is very low to favor performance over tuning quality.\n",
    "    # The reason is that experiments showed that the non-sampled Fast Random Projection provides the best results.\n",
    "    # Sampled node2vec and HashGNN results are only for comparison / experimentation. Its ok to limit their resource consumption.\n",
    "    default_graph_sampling_threshold = 256 \n",
    "    \n",
    "    # Private static (or class?) method that cant be access from the outside and that converts the parameters to the sampled graph:\n",
    "    def __parameters_for_sampled_graph(self, parameters: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Converts the parameters to the sampled graph by adapting dependencies_projection to match the name of the sampled graph.\n",
    "        \"\"\"\n",
    "        parameters_for_sampled_graph = parameters.copy()\n",
    "        parameters_for_sampled_graph[\"dependencies_projection\"] = parameters_for_sampled_graph[\"dependencies_projection\"] + '-cleaned-sampled'\n",
    "        return parameters_for_sampled_graph\n",
    "\n",
    "\n",
    "    def __init__(self, is_sampled: bool, node_count: int, parameters: dict):\n",
    "        \"\"\"\n",
    "        Initializes the GraphSamplingResult with the sampled status and node count.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        is_sampled : bool\n",
    "            Indicates whether the graph was sampled or not.\n",
    "        node_count : int\n",
    "            The number of nodes in the sampled graph (or the original in case it wasn't sampled).\n",
    "        parameters : dict\n",
    "            The updated parameters for the sampled graph or the copied original parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check if the parameters dictionary contains the key \"dependencies_projection\"\n",
    "        if \"dependencies_projection\" not in parameters:\n",
    "            raise ValueError(\"The parameters dictionary must contain the key 'dependencies_projection'.\")\n",
    "        \n",
    "        self.is_sampled = is_sampled\n",
    "        self.node_count = node_count\n",
    "        self.updated_parameters = self.__parameters_for_sampled_graph(parameters) if is_sampled else parameters\n",
    "    \n",
    "    @classmethod\n",
    "    def not_sampled(this_class, parameters: dict):\n",
    "        \"\"\"\n",
    "        Creates a GraphSamplingResult instance indicating that the graph was not sampled.\n",
    "        \"\"\"\n",
    "        node_count = get_projected_graph_node_count(parameters[\"dependencies_projection\"])\n",
    "        return this_class(False, node_count, parameters)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GraphSamplingResult(is_sampled={self.is_sampled}, node_count={self.node_count}, updated_parameters={self.updated_parameters})\"\n",
    "\n",
    "\n",
    "def sample_graph_if_size_exceeds_limit(parameters: dict, graph_sampling_threshold: int = GraphSamplingResult.default_graph_sampling_threshold) -> GraphSamplingResult:\n",
    "    \"\"\"\n",
    "    Samples the graph if the number of nodes exceeds the node count limit.\n",
    "    Sampling takes a random subset of the graph to reduce the size of the graph for further processing.\n",
    "    It returns True if the graph was sampled and False otherwise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : dict\n",
    "        dependencies_projection : str\n",
    "            The name prefix for the in-memory projection for dependencies. Example: \"java-package-embeddings-notebook\"\n",
    "    \"\"\"\n",
    "    if graph_sampling_threshold is None or graph_sampling_threshold <= 0:\n",
    "        print(f\"Graph size limit is not set: {graph_sampling_threshold}. Sampling is not performed.\")\n",
    "        return GraphSamplingResult.not_sampled(parameters)\n",
    "\n",
    "    graph_information=get_projected_graph_information(parameters[\"dependencies_projection\"])\n",
    "    node_count = graph_information[\"nodeCount\"].values[0]\n",
    "    if node_count <= graph_sampling_threshold:\n",
    "        print(f\"The number of nodes in the projection is: {node_count} and is below the limit of {graph_sampling_threshold}. Sampling is not performed.\")\n",
    "        return GraphSamplingResult.not_sampled(parameters)\n",
    "    \n",
    "    # Delete sampled graph projection if it already exists\n",
    "    query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_1_Delete_Projection.cypher\", dict(dependencies_projection=parameters[\"dependencies_projection\"] + '-cleaned-sampled-cleaned'))\n",
    "\n",
    "    sampling_parameters = dict(\n",
    "        dependencies_projection = parameters[\"dependencies_projection\"] + '-cleaned',\n",
    "        dependencies_projection_sampling_ratio = graph_sampling_threshold / node_count\n",
    "    )\n",
    "    results=query_cypher_to_data_frame(\"../cypher/Dependencies_Projection/Dependencies_13_Sample_Projected_Graph.cypher\", sampling_parameters)\n",
    "    node_count=results[\"nodeCount\"].values[0]\n",
    "    print(\"The number of nodes in the sampled projection is: \" + str(node_count))\n",
    "    \n",
    "    return GraphSamplingResult(True, node_count, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca184ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by (but rewritten): https://github.com/prathmachowksey/Hopkins-Statistic-Clustering-Tendency/blob/master/Hopkins-Statistic-Clustering-Tendency.ipynb\n",
    "def hopkins_statistic(\n",
    "    data,\n",
    "    sample_ratio: float = 0.05,\n",
    "    n_trials: int = 1,\n",
    "    random_state=None,\n",
    "    distance_metric='euclidean'\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the Hopkins statistic to assess the cluster tendency of a dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like or DataFrame): Input data matrix of shape (n_samples, n_features).\n",
    "        sample_ratio (float): Proportion of samples to draw (default: 0.05).\n",
    "        n_trials (int): Number of repeated trials for averaging (default: 1).\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "        distance_metric (str): Distance metric to use for nearest neighbors (default: 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: Mean Hopkins statistic over n_trials (range: 0 to 1).\n",
    "        \n",
    "    References:\n",
    "        Richard G. Lawson, Peter C. Jurs (1990). New index for clustering tendency and its application to chemical problems.\n",
    "        https://pubs.acs.org/doi/abs/10.1021/ci00065a010\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    if data is None:\n",
    "        return 0\n",
    "    \n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        print(\"Warning: Converting DataFrame\")\n",
    "        data = data.values\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    num_points, num_features = data.shape\n",
    "    sample_size = max(1, int(sample_ratio * num_points))\n",
    "\n",
    "    hopkins_values = []\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        # Sample points from the dataset\n",
    "        random_number_generator = np.random.default_rng(random_state)\n",
    "        sampled_indices = random_number_generator.choice(num_points, size=sample_size, replace=False)\n",
    "        real_sample = data[sampled_indices]\n",
    "\n",
    "        # Generate uniformly distributed random points within the data bounds\n",
    "        data_min = data.min(axis=0)\n",
    "        data_max = data.max(axis=0)\n",
    "\n",
    "        uniform_sample = random_number_generator.uniform(data_min, data_max, size=(sample_size, num_features))\n",
    "\n",
    "        # Fit NearestNeighbors on the full dataset\n",
    "        nearest_neighbors = NearestNeighbors(n_neighbors=2, metric=distance_metric)\n",
    "        nearest_neighbors.fit(data)\n",
    "\n",
    "        # Distance from uniform points to their nearest neighbor in real data\n",
    "        uniform_distances, _ = nearest_neighbors.kneighbors(uniform_sample)\n",
    "        uniform_nearest_distances = uniform_distances[:, 0]\n",
    "\n",
    "        # Distance from sampled real points to their second nearest neighbor (to skip self)\n",
    "        real_distances, _ = nearest_neighbors.kneighbors(real_sample)\n",
    "        real_nearest_distances = real_distances[:, 1]\n",
    "\n",
    "        # Hopkins statistic for this trial\n",
    "        total_uniform_distance = np.sum(uniform_nearest_distances)\n",
    "        total_real_distance = np.sum(real_nearest_distances)\n",
    "        \n",
    "        total_distance = total_uniform_distance + total_real_distance\n",
    "        if np.isclose(total_distance, 0.0, rtol=1e-09, atol=1e-09) or np.isnan(total_distance):\n",
    "            print(f\"Warning: Zero distance: total_uniform_distance={total_uniform_distance}, total_real_distance={total_real_distance}, data_min={min(data_min)}, data_max={max(data_max)}, sample_size={sample_size}, num_points={num_points}, num_features={num_features}\")\n",
    "            hopkins_score = 0.0\n",
    "        else:\n",
    "            hopkins_score = total_uniform_distance / total_distance\n",
    "\n",
    "        hopkins_values.append(hopkins_score)\n",
    "\n",
    "    return np.mean(hopkins_values) if n_trials > 1 else hopkins_values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "\n",
    "def get_noise_ratio(clustering_results: NDArray) -> float:\n",
    "    \"\"\"\n",
    "    Returns the ratio of noise points in the clustering results.\n",
    "    Noise points are labeled as -1 in HDBSCAN.\n",
    "    \n",
    "    Parameters:\n",
    "    - clustering_results: NDArray containing the clustering results.\n",
    "    \n",
    "    Returns:\n",
    "    - A float representing the noise ratio.\n",
    "    \"\"\"\n",
    "    return np.sum(clustering_results == -1) / len(clustering_results)\n",
    "\n",
    "def adjusted_mutual_info_score_without_noise_penalty(clustering_results: NDArray, reference_communities: NDArray) -> float:\n",
    "    from sklearn.metrics import adjusted_mutual_info_score\n",
    "    \n",
    "    mask_noise = clustering_results != -1 # Exclude noise points from the comparison\n",
    "    return float(adjusted_mutual_info_score(reference_communities[mask_noise], clustering_results[mask_noise]))\n",
    "\n",
    "def soft_ramp_limited_penalty(score, lower_threshold=0.6, upper_threshold=0.8, sharpness=2) -> float:\n",
    "    if score <= lower_threshold:\n",
    "        return 1.0  # No penalty\n",
    "    elif score >= upper_threshold:\n",
    "        return 0.0  # Full penalty\n",
    "    else:\n",
    "        # Normalize noise into [0, 1] range for ramp\n",
    "        x = (score - lower_threshold) / (upper_threshold - lower_threshold)\n",
    "        return max(0.0, 1 - x**sharpness)\n",
    "\n",
    "\n",
    "def adjusted_mutual_info_score_with_soft_ramp_noise_penalty(clustering_results: NDArray, reference_communities: NDArray, **kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Computes the adjusted mutual information score with a custom noise penalty based on a soft ramp function.\n",
    "    \n",
    "    Parameters:\n",
    "    - clustering_results: NDArray containing the clustering results.\n",
    "    - reference_communities: NDArray containing the reference communities for comparison.\n",
    "    - kwargs: Additional keyword arguments for the noise penalty function (e.g., sharpness).\n",
    "    \n",
    "    Returns:\n",
    "    - A float representing the adjusted mutual information score with noise penalty.\n",
    "    \"\"\"\n",
    "    score = adjusted_mutual_info_score_without_noise_penalty(reference_communities, clustering_results)\n",
    "    penalty = soft_ramp_limited_penalty(get_noise_ratio(clustering_results), **kwargs)\n",
    "    return float(score) * penalty\n",
    "\n",
    "#For debugging/explanation purposes\n",
    "# def plot_soft_ramp_limited_score():\n",
    "#     \"\"\"\n",
    "#     Plots the noise penalty curve for the custom soft ramp function.\n",
    "#     The curve shows how the penalty decreases as noise increases, with a sharpness parameter.\n",
    "#     \"\"\"\n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plot\n",
    "\n",
    "#     noise = np.linspace(0, 1, 200)\n",
    "#     penalty_2 = [soft_ramp_limited_score(n, sharpness=2) for n in noise]\n",
    "#     penalty_3 = [soft_ramp_limited_score(n, sharpness=3) for n in noise]\n",
    "#     penalty_4 = [soft_ramp_limited_score(n, sharpness=4) for n in noise]\n",
    "\n",
    "#     plot.plot(noise, penalty_2, label='Soft Ramp Penalty (sharpness=2)')\n",
    "#     plot.plot(noise, penalty_3, label='Soft Ramp Penalty (sharpness=3)')\n",
    "#     plot.plot(noise, penalty_4, label='Soft Ramp Penalty (sharpness=4)')\n",
    "#     plot.axvline(0.4, color='gray', linestyle='--', label='Ramp Start (0.4)')\n",
    "#     plot.axvline(0.6, color='red', linestyle='--', label='Ramp End (0.6)')\n",
    "#     plot.xlabel(\"Noise Ratio\")\n",
    "#     plot.ylabel(\"Penalty\")\n",
    "#     plot.title(\"Custom Noise Penalty Function\")\n",
    "#     plot.legend()\n",
    "#     plot.grid(True)\n",
    "#     plot.show()\n",
    "\n",
    "# plot_soft_ramp_limited_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881bacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "\n",
    "class TunedClusteringResult:\n",
    "    def __init__(self, labels: NDArray, probabilities : NDArray):\n",
    "        self.labels = labels\n",
    "        self.probabilities = probabilities\n",
    "        self.cluster_count = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        self.noise_count = np.sum(labels == -1)\n",
    "        self.noise_ratio = self.noise_count / len(labels) if len(labels) > 0 else 0\n",
    "    def __repr__(self):\n",
    "        return f\"TunedClusteringResult(cluster_count={self.cluster_count}, noise_count={self.noise_count}, noise_ratio={self.noise_ratio}, labels=[...], probabilities=[...], )\"\n",
    "\n",
    "def tuned_hierarchical_density_based_spatial_clustering(embeddings: NDArray, reference_community_ids: NDArray) -> TunedClusteringResult:\n",
    "    \"\"\"\n",
    "    Applies the optimized hierarchical density-based spatial clustering algorithm (HDBSCAN) to the given node embeddings.\n",
    "    The parameters are tuned to get results similar to the ones of the community detection algorithm.\n",
    "    The result is a list of cluster ids for each node embedding.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.cluster import HDBSCAN\n",
    "    import numpy as np\n",
    "\n",
    "    # specify parameters and distributions to sample from\n",
    "    hyper_parameter_distributions = {\n",
    "        \"min_samples\": [2, 3, 4, 5, 7, 10],\n",
    "        \"min_cluster_size\": [4, 5, 7, 10],\n",
    "        # Since the \"eom\" method is the default for HDBSCAN and it seems to work well for most cases, we use it as the default method.\n",
    "        \"cluster_selection_method\": [\"eom\"], #[\"eom\", \"leaf\"],\n",
    "        # Since \"manhattan\" seems to get selected most of the time, and has an advantage for high-dimensional data, we use it as the default metric.\n",
    "        \"metric\": [\"manhattan\"], # [\"euclidean\", \"manhattan\"], \n",
    "    }\n",
    "    \n",
    "    def adjusted_mutual_info_score_with_noise_penalty_for_community_references(community_references):\n",
    "        \"\"\"\n",
    "        Creates a custom scoring function based on the Adjusted Rand Index (ARI) that penalizes for high noise ratio in clustering.\n",
    "        Input:\n",
    "        - community_references: The true labels of the communities for the data points.\n",
    "        Output:\n",
    "        - A scoring function that can directly be used for e.g. GridSearchCV/RandomizedSearchCV and that takes an estimator and data (embeddings) and returns the ARI score with a penalty for noise ratio.\n",
    "        \"\"\"\n",
    "        def adjusted_mutual_info_scorer_with_noise_penalty(estimator, embeddings):\n",
    "            clustering_result = estimator.fit_predict(embeddings)\n",
    "            return adjusted_mutual_info_score_with_soft_ramp_noise_penalty(clustering_result, community_references)\n",
    "\n",
    "        return adjusted_mutual_info_scorer_with_noise_penalty\n",
    "\n",
    "\n",
    "    # Use custom CV that feeds all data to each fold (no slicing)\n",
    "    all_data_without_slicing_cross_validator = [(np.arange(len(embeddings)), np.arange(len(embeddings)))]\n",
    "\n",
    "    tuned_hdbscan = GridSearchCV(\n",
    "        estimator=HDBSCAN(),\n",
    "        refit=False, # Without refit, the estimator doesn't need to implement the 'predict' method. Drawback: Only the best parameters are returned, not the best model.\n",
    "        param_grid=hyper_parameter_distributions,\n",
    "        n_jobs=-1,\n",
    "        scoring=adjusted_mutual_info_score_with_noise_penalty_for_community_references(reference_community_ids),\n",
    "        cv=all_data_without_slicing_cross_validator,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    tuned_hdbscan.fit(embeddings)\n",
    "\n",
    "    #print(\"Best adjusted rand score with noise penalty:\", tuned_hdbscan.best_score_)\n",
    "    print(\"Tuned HDBSCAN parameters:\", tuned_hdbscan.best_params_)\n",
    "\n",
    "    # Run the clustering again with the best parameters\n",
    "    cluster_algorithm = HDBSCAN(**tuned_hdbscan.best_params_, n_jobs=-1, allow_single_cluster=False)\n",
    "    best_model = cluster_algorithm.fit(embeddings)\n",
    "\n",
    "    results = TunedClusteringResult(best_model.labels_, best_model.probabilities_)\n",
    "    print(f\"Number of HDBSCAN clusters (excluding noise): {results.cluster_count:.0f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def output_optuna_tuning_results(optimized_study: optuna.Study, name_of_the_optimized_algorithm: str):\n",
    "    from typing import cast\n",
    "    from optuna.importance import get_param_importances, MeanDecreaseImpurityImportanceEvaluator\n",
    "    from optuna.trial import TrialState\n",
    "\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} parameters (Optuna):\", optimized_study.best_params)\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} score with penalty :\", optimized_study.best_value)\n",
    "    print(f\"Best {name_of_the_optimized_algorithm} parameter influence:\", get_param_importances(optimized_study, evaluator=MeanDecreaseImpurityImportanceEvaluator()))\n",
    "    \n",
    "    valid_trials = [trial for trial in optimized_study.trials if trial.value is not None and trial.state == TrialState.COMPLETE]\n",
    "    top_trials = sorted(valid_trials, key=lambda t: cast(float, t.value), reverse=True)[:10]\n",
    "    for i, trial in enumerate(top_trials):\n",
    "        print(f\"Best {name_of_the_optimized_algorithm} parameter rank: {i+1}, trial: {trial.number}, Value = {trial.value:.6f}, Params: {trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93090818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import NDArray\n",
    "\n",
    "# TODO keep either this (additional optuna dependency) or the implementation above (no additional dependency but not as efficient)\n",
    "def tuned_hierarchical_density_based_spatial_clustering_optuna(embeddings: NDArray, reference_community_ids: NDArray) -> TunedClusteringResult:\n",
    "    \"\"\"\n",
    "    Applies the optimized hierarchical density-based spatial clustering algorithm (HDBSCAN) to the given node embeddings.\n",
    "    The parameters are tuned to get results similar to the ones of the community detection algorithm.\n",
    "    The result is a list of cluster ids for each node embedding.\n",
    "    \"\"\"\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.importance import get_param_importances\n",
    "    from sklearn.cluster import HDBSCAN # type: ignore\n",
    "    import numpy as np\n",
    "\n",
    "    base_clustering_parameter = dict(\n",
    "        metric='manhattan', # Turned out to be the best option in most of the initial experiments\n",
    "        allow_single_cluster=False\n",
    "    )\n",
    "\n",
    "    def objective(trial):\n",
    "        min_cluster_size = trial.suggest_int(\"min_cluster_size\", 4, 50)\n",
    "        min_samples = trial.suggest_int(\"min_samples\", 2, 30)\n",
    "\n",
    "        clusterer = HDBSCAN(\n",
    "            **base_clustering_parameter,\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples\n",
    "        )\n",
    "        labels = clusterer.fit_predict(embeddings)\n",
    "        return adjusted_mutual_info_score_with_soft_ramp_noise_penalty(labels, reference_community_ids)\n",
    "\n",
    "    # TODO create study with db?\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42), study_name=\"HDBSCAN\")#, storage=f\"sqlite:///optuna_study_node_embeddings_java.db\", load_if_exists=True)\n",
    "    \n",
    "    # Try (enqueue) two specific settings first that led to good results in initial experiments\n",
    "    study.enqueue_trial({\"min_cluster_size\": 4, \"min_samples\": 2})\n",
    "    study.enqueue_trial({\"min_cluster_size\": 5, \"min_samples\": 2})\n",
    "    \n",
    "    # Start the hyperparameter tuning\n",
    "    study.optimize(objective, n_trials=20, timeout=10)\n",
    "    output_optuna_tuning_results(study, 'HDBSCAN')\n",
    "\n",
    "    # Run the clustering again with the best parameters\n",
    "    cluster_algorithm = HDBSCAN(**base_clustering_parameter, **study.best_params, n_jobs=-1)\n",
    "    best_model = cluster_algorithm.fit(embeddings)\n",
    "\n",
    "    return TunedClusteringResult(best_model.labels_, best_model.probabilities_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as numpy_typing\n",
    "\n",
    "class CommunityComparingScores:\n",
    "    def __init__(self, adjusted_mutual_info_score: float, adjusted_rand_index: float, normalized_mutual_information: float):\n",
    "        self.adjusted_mutual_info_score = adjusted_mutual_info_score\n",
    "        self.adjusted_rand_index = adjusted_rand_index\n",
    "        self.normalized_mutual_information = normalized_mutual_information\n",
    "        self.scores = {\n",
    "            \"Adjusted Mutual Info Score\": adjusted_mutual_info_score,\n",
    "            \"Adjusted Rand Index\": adjusted_rand_index,\n",
    "            \"Normalized Mutual Information\": normalized_mutual_information\n",
    "        }\n",
    "    def __repr__(self):\n",
    "        return f\"CommunityComparingScores(adjusted_mutual_info_score={self.adjusted_mutual_info_score}, adjusted_rand_index={self.adjusted_rand_index}, normalized_mutual_information={self.normalized_mutual_information})\"\n",
    "\n",
    "def get_community_comparing_scores(cluster_labels: numpy_typing.NDArray, reference_community_ids: numpy_typing.NDArray) -> CommunityComparingScores:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the scores of the clustering algorithm compared to the community detection algorithm.\n",
    "    The scores are calculated using the adjusted rand index (ARI) and the normalized mutual information (NMI).\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "\n",
    "    # Create a mask to filter out noise points. In HDBSCAN, noise points are labeled as -1\n",
    "    mask = cluster_labels != -1\n",
    "    ami = float(adjusted_mutual_info_score(reference_community_ids[mask], cluster_labels[mask]))\n",
    "    ari = adjusted_rand_score(reference_community_ids[mask], cluster_labels[mask])\n",
    "    nmi = float(normalized_mutual_info_score(reference_community_ids[mask], cluster_labels[mask]))\n",
    "\n",
    "    return CommunityComparingScores(ami, ari, nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import pandas as pd\n",
    "\n",
    "def get_clustering_property_name(clustering_property_type: Literal['Label', 'Probability'] = 'Label', clustering_name: str = \"TunedHDBSCAN\"):\n",
    "    \"\"\"\n",
    "    Assembles the property name for clustering results.\n",
    "    This helps to have a uniform schema.\n",
    "    \"\"\"\n",
    "    return 'clustering' + clustering_name + clustering_property_type\n",
    "\n",
    "def add_clustering_results_to_embeddings(embeddings: pd.DataFrame, clustering_result: TunedClusteringResult, clustering_name: str = \"TunedHDBSCAN\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds the clustering results to the embeddings DataFrame.\n",
    "    \"\"\"\n",
    "    embeddings[get_clustering_property_name('Label', clustering_name)] = clustering_result.labels\n",
    "    embeddings[get_clustering_property_name('Probability', clustering_name)] = clustering_result.probabilities\n",
    "    return embeddings\n",
    "\n",
    "def get_clustering_results_distribution(embeddings: pd.DataFrame, clustering_name: str = \"TunedHDBSCAN\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns the clustering results distribution for the given clustering name.\n",
    "    \"\"\"\n",
    "    return embeddings.groupby(get_clustering_property_name('Label', clustering_name)).aggregate(\n",
    "        probability=(get_clustering_property_name('Probability', clustering_name), 'mean'),\n",
    "        count=('codeUnitName', 'count'),\n",
    "        communityIds=('communityId', lambda x: list(set(x))),\n",
    "        codeUnitNames=('codeUnitName', lambda x: list(set(x))),\n",
    "    ).reset_index().sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunedHierarchicalDensityBasedSpatialClusteringResult:\n",
    "    def __init__(self, embeddings: pd.DataFrame, clustering_result: TunedClusteringResult, community_comparing_scores: CommunityComparingScores, clustering_results_distribution: pd.DataFrame):\n",
    "        self.embeddings = embeddings\n",
    "        self.clustering_result = clustering_result\n",
    "        self.community_comparing_scores = community_comparing_scores\n",
    "        self.clustering_results_distribution = clustering_results_distribution\n",
    "    def __repr__(self):\n",
    "        return f\"TunedHierarchicalDensityBasedSpatialClusteringResult(embeddings={self.embeddings}, clustering_result={self.clustering_result}, community_comparing_scores={self.community_comparing_scores}, clustering_results_distribution={self.clustering_results_distribution})\"\n",
    "\n",
    "\n",
    "def add_tuned_hierarchical_density_based_spatial_clustering(embeddings: pd.DataFrame, clustering_name: str = \"TunedHDBSCAN\") -> TunedHierarchicalDensityBasedSpatialClusteringResult:\n",
    "    \"\"\"\n",
    "    Applies the tuned hierarchical density-based spatial clustering algorithm (HDBSCAN) to the given node embeddings.\n",
    "    The parameters are tuned to get results similar to the ones of the community detection algorithm.\n",
    "    The result is the input DataFrame with the clustering results added.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    # Apply the tuned HDBSCAN clustering algorithm\n",
    "    embeddings_values = np.array(embeddings.embedding.tolist())\n",
    "    community_reference_ids = np.array(embeddings.communityId.tolist())\n",
    "    \n",
    "    # TODO keep only one implementation\n",
    "    grid_search_hdbscan_start = time.time()\n",
    "    clustering_result = tuned_hierarchical_density_based_spatial_clustering(embeddings_values, community_reference_ids)\n",
    "    grid_search_hdbscan_end = time.time()\n",
    "    print(clustering_result)\n",
    "    \n",
    "    community_comparing_scores = get_community_comparing_scores(clustering_result.labels, community_reference_ids)\n",
    "    print(community_comparing_scores)\n",
    "    \n",
    "    # ----------\n",
    "\n",
    "    optuna_start = time.time()\n",
    "    clustering_result = tuned_hierarchical_density_based_spatial_clustering_optuna(embeddings_values, community_reference_ids)\n",
    "    optuna_end = time.time()\n",
    "    print(clustering_result)\n",
    "    \n",
    "    community_comparing_scores = get_community_comparing_scores(clustering_result.labels, community_reference_ids)\n",
    "    print(community_comparing_scores)\n",
    "\n",
    "    # ----------\n",
    "    print(f\"Grid Search tuning time: {grid_search_hdbscan_end - grid_search_hdbscan_start:.2f} seconds\")\n",
    "    print(f\"Optuna tuning time: {optuna_end - optuna_start:.2f} seconds\")\n",
    "    # ----------\n",
    "\n",
    "    # Add the clustering results to the embeddings DataFrame\n",
    "    embeddings = add_clustering_results_to_embeddings(embeddings, clustering_result, clustering_name)\n",
    "    \n",
    "    # Get the clustering results distribution\n",
    "    clustering_results_distribution = get_clustering_results_distribution(embeddings, clustering_name)\n",
    "    \n",
    "    # Display the clustering results distribution\n",
    "    display(clustering_results_distribution)\n",
    "    \n",
    "    return TunedHierarchicalDensityBasedSpatialClusteringResult(embeddings, clustering_result, community_comparing_scores, clustering_results_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embedding_tuning_scores = []\n",
    "\n",
    "def reset_node_embedding_tuning_scores():\n",
    "    \"\"\"\n",
    "    Resets the collected node embedding scores\n",
    "    This is useful to start a new evaluation run without old results.\n",
    "    \"\"\"\n",
    "    global node_embedding_tuning_scores\n",
    "    node_embedding_tuning_scores = []\n",
    "\n",
    "\n",
    "def add_node_embedding_tuning_scores(embedding_dimension: int,\n",
    "                                     adjusted_mutual_info_score: float,\n",
    "                                     confidence_score: float,\n",
    "                                     clustering_noise_ratio: float,\n",
    "                                     cluster_count: int):\n",
    "    \"\"\"\n",
    "    Collects node embedding scores for later analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    global node_embedding_tuning_scores\n",
    "    node_embedding_tuning_scores.append(dict(\n",
    "        embedding_dimension = embedding_dimension,\n",
    "        adjusted_mutual_info_score = adjusted_mutual_info_score,\n",
    "        confidence_score = confidence_score,\n",
    "        clustering_noise_ratio = clustering_noise_ratio,\n",
    "        cluster_count = cluster_count\n",
    "    ))\n",
    "\n",
    "\n",
    "def plot_node_embedding_tuning_scores():\n",
    "    \"\"\"\n",
    "    Plots the clustering noise ratio and cluster count against the adjusted mutual info score for the Fast Random Projection node embeddings.\n",
    "    This function uses matplotlib to create two horizontally arranged subplots:\n",
    "    - Left: clustering noise ratio vs. adjusted mutual info score\n",
    "    - Right: cluster count vs. adjusted mutual info score\n",
    "    The color of the points represents the embedding dimension.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plot\n",
    "    import pandas as pd\n",
    "\n",
    "    tuning_scores = pd.DataFrame(node_embedding_tuning_scores)\n",
    "\n",
    "    figure, axes = plot.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "    figure.subplots_adjust(wspace=0.1)\n",
    "\n",
    "    noise_ratio_plot = axes[0].scatter(\n",
    "        tuning_scores['clustering_noise_ratio'],\n",
    "        tuning_scores['adjusted_mutual_info_score'],\n",
    "        c=tuning_scores['embedding_dimension'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    axes[0].set_xlabel('Clustering Noise Ratio')\n",
    "    axes[0].set_ylabel('Adjusted Mutual Info Score')\n",
    "    axes[0].set_title('Clustering Noise Ratio vs. Adjusted Mutual Info Score')\n",
    "\n",
    "    cluster_count_plot = axes[1].scatter(\n",
    "        tuning_scores['cluster_count'],\n",
    "        tuning_scores['adjusted_mutual_info_score'],\n",
    "        c=tuning_scores['embedding_dimension'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    axes[1].set_xlabel('Cluster Count')\n",
    "    axes[1].set_title('Cluster Count vs. Adjusted Mutual Info Score')\n",
    "\n",
    "    # Place a single colorbar between the two subplots\n",
    "    colorbar = figure.colorbar(\n",
    "        cluster_count_plot,\n",
    "        ax=axes,\n",
    "        fraction=0.05,\n",
    "        aspect=30,\n",
    "        location='right',\n",
    "    )\n",
    "    colorbar.set_label('Embedding Dimension')\n",
    "\n",
    "    plot.show()\n",
    "\n",
    "\n",
    "def output_node_embedding_tuning_scores():\n",
    "    \"\"\"\n",
    "    Returns the DataFrame with the results of the Fast Random Projection node embeddings.\n",
    "    \"\"\"\n",
    "    node_embeddings_score_results_dataframe = pd.DataFrame(node_embedding_tuning_scores)\n",
    "    print(\"Min noise ratio:\", node_embeddings_score_results_dataframe.clustering_noise_ratio.min())\n",
    "    print(\"Max noise ratio:\", node_embeddings_score_results_dataframe.clustering_noise_ratio.max())\n",
    "    print(\"Min adjusted mutual info score:\", node_embeddings_score_results_dataframe.adjusted_mutual_info_score.min())\n",
    "    print(\"Max adjusted mutual info score:\", node_embeddings_score_results_dataframe.adjusted_mutual_info_score.max())\n",
    "    print(\"Min cluster count:\", node_embeddings_score_results_dataframe.cluster_count.min())\n",
    "    print(\"Max cluster count:\", node_embeddings_score_results_dataframe.cluster_count.max())\n",
    "    \n",
    "    plot_node_embedding_tuning_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbdf6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalDensityClusteringScores:\n",
    "\n",
    "    def __init__(self, embedding_dimension: int, adjusted_mutual_info_score: float, confidence_score: float, noise_ratio: float, cluster_count: int):\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.adjusted_mutual_info_score = adjusted_mutual_info_score\n",
    "        self.confidence_score = confidence_score\n",
    "        self.noise_ratio = noise_ratio\n",
    "        self.cluster_count = cluster_count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"HierarchicalDensityClusteringScores(embedding_dimension={self.embedding_dimension}, adjusted_mutual_info_score={self.adjusted_mutual_info_score}, confidence_score={self.confidence_score}, noise_ratio={self.noise_ratio}, cluster_count={self.cluster_count})\"\n",
    "    \n",
    "    def append_to_tuning_scores(self):\n",
    "        add_node_embedding_tuning_scores(\n",
    "            self.embedding_dimension,\n",
    "            self.adjusted_mutual_info_score, \n",
    "            self.confidence_score, \n",
    "            self.noise_ratio,\n",
    "            self.cluster_count\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def cluster_embeddings_with_references(cls, embedding_column: pd.Series, reference_community_id_column: pd.Series) -> 'HierarchicalDensityClusteringScores':\n",
    "        \"\"\"\n",
    "        Clusters the embeddings with the reference community ids and returns the clustering scores.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        embedding_column : pd.Series\n",
    "            The column containing the embeddings to be clustered.\n",
    "        reference_community_id_column : pd.Series\n",
    "            The column containing the reference community ids to compare the clustering results against.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        HierarchicalDensityClusteringScores\n",
    "            An instance of HierarchicalDensityClusteringScores containing the clustering scores.\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from sklearn.cluster import HDBSCAN\n",
    "        \n",
    "        hierarchical_density_based_spatial_clustering = HDBSCAN(\n",
    "            cluster_selection_method='eom',\n",
    "            metric='manhattan',\n",
    "            min_samples=2,\n",
    "            min_cluster_size=5,\n",
    "            allow_single_cluster=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        embeddings = np.array(embedding_column.tolist())\n",
    "        clustering_result = hierarchical_density_based_spatial_clustering.fit(embeddings)\n",
    "        \n",
    "        reference_community_ids = np.array(reference_community_id_column.tolist())\n",
    "        adjusted_mutual_info_score_value = adjusted_mutual_info_score_with_soft_ramp_noise_penalty(clustering_result.labels_, reference_community_ids)\n",
    "        \n",
    "        confidence_score = np.mean(clustering_result.probabilities_[clustering_result.labels_ != -1])\n",
    "        noise_count = np.sum(clustering_result.labels_ == -1)\n",
    "        noise_ratio = noise_count / len(clustering_result.labels_)\n",
    "        cluster_count = len(set(clustering_result.labels_)) - (1 if -1 in clustering_result.labels_ else 0)\n",
    "        return cls(len(embeddings[0]), adjusted_mutual_info_score_value, confidence_score, noise_ratio, cluster_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import typing as typ\n",
    "\n",
    "class DependencyProjectionParameters:\n",
    "    def __init__(self, \n",
    "                 projection_name: str = \"java-type-embeddings-notebook\",\n",
    "                 projection_node: str = \"Type\",\n",
    "                 projection_weight_property: str = \"weight\"\n",
    "                ):\n",
    "        self.projection_name = projection_name\n",
    "        self.projection_node = projection_node\n",
    "        self.projection_weight_property = projection_weight_property\n",
    "\n",
    "    @classmethod\n",
    "    def from_projection_parameters(cls, projection_parameters: dict):\n",
    "        \"\"\"\n",
    "        Creates a DependencyProjectionParameters instance from a dictionary of projection parameters.\n",
    "        The dictionary must contain the following keys:\n",
    "         - \"dependencies_projection\": The name of the projection.\n",
    "         - \"dependencies_projection_node\": The node type of the projection.\n",
    "         - \"dependencies_projection_weight_property\": The weight property of the projection.\n",
    "        \"\"\"\n",
    "        if not all(key in projection_parameters for key in [\"dependencies_projection\", \"dependencies_projection_node\", \"dependencies_projection_weight_property\"]):\n",
    "            raise ValueError(\"The projection parameters must contain the keys: 'dependencies_projection', 'dependencies_projection_node', 'dependencies_projection_weight_property'.\")\n",
    "        return cls(\n",
    "            projection_name=projection_parameters[\"dependencies_projection\"],\n",
    "            projection_node=projection_parameters[\"dependencies_projection_node\"],\n",
    "            projection_weight_property=projection_parameters[\"dependencies_projection_weight_property\"]\n",
    "        )\n",
    "\n",
    "    def get_cypher_parameters(self):\n",
    "        return {\n",
    "            \"dependencies_projection\": self.projection_name,\n",
    "            \"dependencies_projection_node\": self.projection_node,\n",
    "            \"dependencies_projection_weight_property\": self.projection_weight_property,\n",
    "        }\n",
    "    \n",
    "    def clone_with_projection_name(self, projection_name: str):\n",
    "        return DependencyProjectionParameters(\n",
    "            projection_name=projection_name,\n",
    "            projection_node=self.projection_node,\n",
    "            projection_weight_property=self.projection_weight_property\n",
    "        )\n",
    "\n",
    "def create_tuneable(class_to_create: typ.Type, verbose: bool = False) -> typ.Any:\n",
    "    if not hasattr(class_to_create, '__init__'):\n",
    "        raise ValueError(f\"The class {class_to_create.__name__} does not have an __init__ method. It cannot be created.\")\n",
    "    if not callable(class_to_create.__init__):\n",
    "        raise ValueError(f\"The class {class_to_create.__name__} has an __init__ method, but it is not callable. It cannot be created.\")\n",
    "    if not issubclass(class_to_create, BaseEstimator):\n",
    "        raise ValueError(f\"The class {class_to_create.__name__} does not inherit from BaseEstimator. It cannot be created.\")\n",
    "\n",
    "    # print(f\"Creating a tuneable estimator for the class {class_to_create.__name__}...\")\n",
    "\n",
    "    class TuneableEstimator():\n",
    "        def __init__(self):\n",
    "            self.class_to_create_ = class_to_create\n",
    "            self.verbose = verbose\n",
    "\n",
    "        def with_projection_parameters(self, projection_parameters: dict) -> typ.Any:\n",
    "            \"\"\"\n",
    "            Creates an instance of the given class (using its constructor) with projection parameters from a dict.\n",
    "            The dict must contain the following keys: \n",
    "             - \"dependencies_projection\"\n",
    "             - \"dependencies_projection_node\"\n",
    "             - \"dependencies_projection_weight_property\".\n",
    "            \"\"\"\n",
    "    \n",
    "            #print(f\"...with projection parameters: {projection_parameters}\")\n",
    "            return self.class_to_create_(\n",
    "                dependency_projection = DependencyProjectionParameters.from_projection_parameters(projection_parameters), # type: ignore\n",
    "                verbose=self.verbose # type: ignore\n",
    "            )\n",
    "    return TuneableEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TuneableFastRandomProjectionNodeEmbeddings(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Can be used with GridSearchCV or RandomizedSearchCV to tune the parameters of the Fast Random Projection node embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    cypher_file_for_read_ = \"../cypher/Node_Embeddings/Node_Embeddings_1d_Fast_Random_Projection_Tuneable_Stream.cypher\"  \n",
    "    cypher_file_for_write_ = \"../cypher/Node_Embeddings/Node_Embeddings_1e_Fast_Random_Projection_Tuneable_Write.cypher\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 dependency_projection: DependencyProjectionParameters = DependencyProjectionParameters(),\n",
    "                 verbose: bool = False,\n",
    "                 # Tuneable algorithm parameters\n",
    "                 embedding_dimension: int = 64, \n",
    "                 random_seed: int = 42,\n",
    "                 fast_random_projection_normalization_strength: float = 0.3,\n",
    "                 fast_random_projection_forth_iteration_weight: float = 1.0,\n",
    "                ):\n",
    "        self.dependency_projection = dependency_projection\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.random_seed = random_seed\n",
    "        self.fast_random_projection_normalization_strength = fast_random_projection_normalization_strength\n",
    "        self.fast_random_projection_forth_iteration_weight = fast_random_projection_forth_iteration_weight\n",
    "\n",
    "\n",
    "    def __to_embedding_parameters(self):\n",
    "        return {\n",
    "            \"dependencies_projection_embedding_dimension\": str(self.embedding_dimension),\n",
    "            \"dependencies_projection_fast_random_projection_normalization_strength\": str(self.fast_random_projection_normalization_strength),\n",
    "            \"dependencies_projection_fast_random_projection_forth_iteration_weight\": str(self.fast_random_projection_forth_iteration_weight),\n",
    "            \"dependencies_projection_embedding_random_seed\": str(self.random_seed),\n",
    "            \"dependencies_projection_write_property\": \"embeddingsFastRandomProjectionForClustering\",\n",
    "            **self.dependency_projection.get_cypher_parameters()\n",
    "        }    \n",
    "    \n",
    "\n",
    "    def __generate_embeddings(self):\n",
    "        node_embedding_parameters = self.__to_embedding_parameters()\n",
    "        if self.verbose:\n",
    "            print(\"Generating embeddings using Neo4j Graph Data Science with the following parameters: \" + str(node_embedding_parameters))\n",
    "        return query_cypher_to_data_frame_for_verbosity(self.verbose)(self.cypher_file_for_read_, parameters=node_embedding_parameters)\n",
    "\n",
    "\n",
    "    def __check_fitted(self):\n",
    "        \"\"\"\n",
    "        Checks if the model has been fitted by checking if the embeddings_ attribute exists.\n",
    "        Raises a ValueError if the model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'embeddings_') or not hasattr(self, 'clustering_scores_'):\n",
    "            raise ValueError(\"The model has not been fitted yet. Please call the fit method before.\")\n",
    "\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Fits the model by generating node embeddings and calculating the Hopkins statistic.\n",
    "        \"\"\"\n",
    "        self.embeddings_ = self.__generate_embeddings()\n",
    "        self.clustering_scores_ = HierarchicalDensityClusteringScores.cluster_embeddings_with_references(self.embeddings_.embedding, self.embeddings_.communityId).append_to_tuning_scores()\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def score(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Returns the score of the model based on the adjusted mutual info score comparing the clusters with pre calculated Leiden communities.\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.clustering_scores_.adjusted_mutual_info_score\n",
    "\n",
    "\n",
    "    def write_embeddings(self):\n",
    "        \"\"\"\n",
    "        Writes the generated embeddings to the Neo4j database.\n",
    "        This is useful for further processing or analysis of the embeddings.\n",
    "        \"\"\"\n",
    "        node_embedding_parameters = self.__to_embedding_parameters()\n",
    "        print(\"Writing embeddings to Neo4j with the following parameters: \" + str(node_embedding_parameters))\n",
    "        query_cypher_to_data_frame_for_verbosity(self.verbose)(self.cypher_file_for_write_, parameters=node_embedding_parameters)\n",
    "        return self\n",
    "\n",
    "        \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns the generated embeddings\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.embeddings_\n",
    "\n",
    "\n",
    "    def get_clustering_scores(self) -> HierarchicalDensityClusteringScores:\n",
    "        \"\"\"\n",
    "        Returns the clustering scores, which include the adjusted mutual info score, confidence score, noise ratio, and cluster count.\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.clustering_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class TuneableNode2VecNodeEmbeddings(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Can be used with GridSearchCV or RandomizedSearchCV to tune the parameters of node embeddings with node2vec.\n",
    "    \"\"\"\n",
    "\n",
    "    cypher_file_name_ = \"../cypher/Node_Embeddings/Node_Embeddings_3d_Node2Vec_Tuneable_Stream.cypher\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 dependency_projection: DependencyProjectionParameters = DependencyProjectionParameters(),\n",
    "                 verbose: bool = False,\n",
    "                 # Tuneable algorithm parameters\n",
    "                 embedding_dimension: int = 64, \n",
    "                 random_seed: int = 42,\n",
    "                 node2vec_in_out_factor: float = 1.0,\n",
    "                 node2vec_return_factor: float = 1.0,\n",
    "                 node2vec_window_size: int = 10,\n",
    "                 node2vec_walk_length: int = 80,\n",
    "                 node2vec_walks_per_node: int = 10,\n",
    "                 node2vec_iterations: int = 1,\n",
    "                 node2vec_negative_sampling_rate: int = 5,\n",
    "                 node2vec_positive_sampling_factor: float = 0.001,\n",
    "                ):\n",
    "        self.dependency_projection = dependency_projection\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.random_seed = random_seed\n",
    "        self.node2vec_in_out_factor = node2vec_in_out_factor\n",
    "        self.node2vec_return_factor = node2vec_return_factor\n",
    "        self.node2vec_window_size = node2vec_window_size\n",
    "        self.node2vec_walk_length = node2vec_walk_length\n",
    "        self.node2vec_walks_per_node = node2vec_walks_per_node\n",
    "        self.node2vec_iterations = node2vec_iterations\n",
    "        self.node2vec_negative_sampling_rate = node2vec_negative_sampling_rate\n",
    "        self.node2vec_positive_sampling_factor = node2vec_positive_sampling_factor\n",
    "\n",
    "\n",
    "    def __to_embedding_parameters(self):\n",
    "        return {\n",
    "            \"dependencies_projection_embedding_dimension\": str(self.embedding_dimension),\n",
    "            \"dependencies_projection_embedding_random_seed\": str(self.random_seed),\n",
    "            \"dependencies_projection_node2vec_in_out_factor\": str(self.node2vec_in_out_factor),\n",
    "            \"dependencies_projection_node2vec_return_factor\": str(self.node2vec_return_factor),\n",
    "            \"dependencies_projection_node2vec_window_size\": str(self.node2vec_window_size),\n",
    "            \"dependencies_projection_node2vec_walk_length\": str(self.node2vec_walk_length),\n",
    "            \"dependencies_projection_node2vec_walks_per_node\": str(self.node2vec_walks_per_node),\n",
    "            \"dependencies_projection_node2vec_iterations\": str(self.node2vec_iterations),\n",
    "            \"dependencies_projection_node2vec_negative_sampling_rate\": str(self.node2vec_negative_sampling_rate),\n",
    "            \"dependencies_projection_node2vec_positive_sampling_factor\": str(self.node2vec_positive_sampling_factor),\n",
    "            **self.dependency_projection.get_cypher_parameters()\n",
    "        }    \n",
    "    \n",
    "\n",
    "    def __generate_embeddings(self):\n",
    "        node_embedding_parameters = self.__to_embedding_parameters()\n",
    "        if self.verbose:\n",
    "            print(\"Generating embeddings using Neo4j Graph Data Science with the following parameters: \" + str(node_embedding_parameters))\n",
    "        return query_cypher_to_data_frame_for_verbosity(self.verbose)(self.cypher_file_name_, parameters=node_embedding_parameters)\n",
    "\n",
    "\n",
    "    def __check_fitted(self):\n",
    "        \"\"\"\n",
    "        Checks if the model has been fitted by checking if the embeddings_ attribute exists.\n",
    "        Raises a ValueError if the model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'embeddings_') or not hasattr(self, 'clustering_scores_'):\n",
    "            raise ValueError(\"The model has not been fitted yet. Please call the fit method before.\")\n",
    "\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Fits the model by generating node embeddings and calculating the Hopkins statistic.\n",
    "        \"\"\"\n",
    "        self.embeddings_ = self.__generate_embeddings()\n",
    "        self.clustering_scores_ = HierarchicalDensityClusteringScores.cluster_embeddings_with_references(self.embeddings_.embedding, self.embeddings_.communityId).append_to_tuning_scores()\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def refit_with_projection(self, projection_name: str):\n",
    "        \"\"\"\n",
    "        Re-fits the model for the given projection name.\n",
    "        This is useful for tuning the model with different projections (sampled/original).\n",
    "        \"\"\"\n",
    "        if projection_name == self.dependency_projection.projection_name:\n",
    "            print(f\"Projection name '{projection_name}' is the same as the current one. No re-fitting needed.\")\n",
    "            return self\n",
    "        \n",
    "        self.dependency_projection = self.dependency_projection.clone_with_projection_name(projection_name)\n",
    "        print(f\"Re-fitting the model with the following parameters: \" + str(self.__to_embedding_parameters()))\n",
    "        return self.fit()\n",
    "\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Returns the score of the model based on the adjusted mutual info score comparing the clusters with pre calculated Leiden communities.\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.clustering_scores_.adjusted_mutual_info_score\n",
    "    \n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns the generated embeddings\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.embeddings_\n",
    "\n",
    "\n",
    "    def get_clustering_scores(self) -> HierarchicalDensityClusteringScores:\n",
    "        \"\"\"\n",
    "        Returns the clustering scores, which include the adjusted mutual info score, confidence score, noise ratio, and cluster count.\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.clustering_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651bede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class TuneableHashGNNNodeEmbeddings(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Can be used with GridSearchCV or RandomizedSearchCV to tune the parameters of node embeddings with HashGNN.\n",
    "    \"\"\"\n",
    "\n",
    "    cypher_file_name_ = \"../cypher/Node_Embeddings/Node_Embeddings_2d_Hash_GNN_Tuneable_Stream.cypher\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 dependency_projection: DependencyProjectionParameters = DependencyProjectionParameters(),\n",
    "                 verbose: bool = False,\n",
    "                 # Tuneable algorithm parameters\n",
    "                 embedding_dimension: int = 64, \n",
    "                 random_seed: int = 42,\n",
    "                 hashgnn_iterations: int = 2,\n",
    "                 hashgnn_density_level: int = 2,\n",
    "                 hashgnn_neighbor_influence: float = 1.0,\n",
    "                 hashgnn_dimension_multiplier: int = 2,\n",
    "                ):\n",
    "        self.dependency_projection = dependency_projection\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.random_seed = random_seed\n",
    "        self.hashgnn_iterations = hashgnn_iterations\n",
    "        self.hashgnn_density_level = hashgnn_density_level\n",
    "        self.hashgnn_neighbor_influence = hashgnn_neighbor_influence\n",
    "        self.hashgnn_dimension_multiplier = hashgnn_dimension_multiplier\n",
    "\n",
    "\n",
    "    def __to_embedding_parameters(self):\n",
    "        return {\n",
    "            \"dependencies_projection_embedding_dimension\": str(self.embedding_dimension),\n",
    "            \"dependencies_projection_embedding_random_seed\": str(self.random_seed),\n",
    "            \"dependencies_projection_hashgnn_iterations\": str(self.hashgnn_iterations),\n",
    "            \"dependencies_projection_hashgnn_density_level\": str(self.hashgnn_density_level),\n",
    "            \"dependencies_projection_hashgnn_neighbor_influence\": str(self.hashgnn_neighbor_influence),\n",
    "            \"dependencies_projection_hashgnn_dimension_multiplier\": str(self.hashgnn_dimension_multiplier),\n",
    "            **self.dependency_projection.get_cypher_parameters()\n",
    "        }    \n",
    "    \n",
    "\n",
    "    def __generate_embeddings(self):\n",
    "        node_embedding_parameters = self.__to_embedding_parameters()\n",
    "        if self.verbose:\n",
    "            print(\"Generating embeddings using Neo4j Graph Data Science with the following parameters: \" + str(node_embedding_parameters))\n",
    "        return query_cypher_to_data_frame_for_verbosity(self.verbose)(self.cypher_file_name_, parameters=node_embedding_parameters)\n",
    "\n",
    "\n",
    "    def __check_fitted(self):\n",
    "        \"\"\"\n",
    "        Checks if the model has been fitted by checking if the embeddings_ attribute exists.\n",
    "        Raises a ValueError if the model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'embeddings_') or not hasattr(self, 'clustering_scores_'):\n",
    "            raise ValueError(\"The model has not been fitted yet. Please call the fit method before.\")\n",
    "\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Fits the model by generating node embeddings and calculating the Hopkins statistic.\n",
    "        \"\"\"\n",
    "        self.embeddings_ = self.__generate_embeddings()\n",
    "        self.clustering_scores_ = HierarchicalDensityClusteringScores.cluster_embeddings_with_references(self.embeddings_.embedding, self.embeddings_.communityId).append_to_tuning_scores()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def refit_with_projection(self, projection_name: str):\n",
    "        \"\"\"\n",
    "        Re-fits the model for the given projection name.\n",
    "        This is useful for tuning the model with different projections (sampled/original).\n",
    "        \"\"\"\n",
    "        if projection_name == self.dependency_projection.projection_name:\n",
    "            print(f\"Projection name '{projection_name}' is the same as the current one. No re-fitting needed.\")\n",
    "            return self\n",
    "        \n",
    "        self.dependency_projection = self.dependency_projection.clone_with_projection_name(projection_name)\n",
    "        print(f\"Re-fitting the model with the following parameters: \" + str(self.__to_embedding_parameters()))\n",
    "        return self.fit()\n",
    "\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Returns the score of the model based on the adjusted mutual info score comparing the clusters with pre calculated Leiden communities.\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.clustering_scores_.adjusted_mutual_info_score\n",
    "    \n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\"\n",
    "        Returns the generated embeddings\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.embeddings_\n",
    "    \n",
    "\n",
    "    def get_clustering_scores(self) -> HierarchicalDensityClusteringScores:\n",
    "        \"\"\"\n",
    "        Returns the clustering scores, which include the adjusted mutual info score, confidence score, noise ratio, and cluster count.\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return self.clustering_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30064d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class TuneableLeidenCommunityDetection(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Can be used with GridSearchCV or RandomizedSearchCV to tune the parameters of the Leiden community detection algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    cypher_file_for_statistics_ = \"../cypher/Community_Detection/Community_Detection_2b_Leiden_Tuneable_Statistics.cypher\"  \n",
    "    cypher_file_for_write_ = \"../cypher/Community_Detection/Community_Detection_2d_Leiden_Tuneable_Write.cypher\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 dependency_projection: DependencyProjectionParameters = DependencyProjectionParameters(),\n",
    "                 verbose: bool = False,\n",
    "                 # Tuneable algorithm parameters\n",
    "                 gamma: float = 1.0,\n",
    "                 theta: float = 0.001,\n",
    "                 max_levels: int = 10,\n",
    "                ):\n",
    "        self.dependency_projection = dependency_projection\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.max_levels = max_levels\n",
    "\n",
    "\n",
    "    def __to_algorithm_parameters(self):\n",
    "        return {\n",
    "            \"dependencies_leiden_gamma\": str(self.gamma),\n",
    "            \"dependencies_leiden_theta\": str(self.theta),\n",
    "            \"dependencies_leiden_max_levels\": str(self.max_levels),\n",
    "            \"dependencies_projection_write_property\": \"communityLeidenIdTuned\",\n",
    "            **self.dependency_projection.get_cypher_parameters()\n",
    "        }    \n",
    "    \n",
    "\n",
    "    def __run_algorithm(self):\n",
    "        algorithm_parameters = self.__to_algorithm_parameters()\n",
    "        if self.verbose:\n",
    "            print(\"Calculating Leiden communities using Neo4j Graph Data Science with the following parameters: \" + str(algorithm_parameters))\n",
    "        return query_cypher_to_data_frame_for_verbosity(self.verbose)(self.cypher_file_for_statistics_, parameters=algorithm_parameters)\n",
    "\n",
    "\n",
    "    def __check_fitted(self):\n",
    "        \"\"\"\n",
    "        Checks if the model has been fitted by checking if the embeddings_ attribute exists.\n",
    "        Raises a ValueError if the model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'community_statistics_'):\n",
    "            raise ValueError(\"The model has not been fitted yet. Please call the fit method before.\")\n",
    "\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        Fits the model by calculating Leiden communities and their statistics.\n",
    "        \"\"\"\n",
    "        self.community_statistics_ = self.__run_algorithm()\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def score(self, X=None, y=None):\n",
    "        \"\"\"\n",
    "        The returned score is high for community detection results with high modularity and high community count.\n",
    "        A penalty assures that a modularity lower than 0.3 (*1) will result in a score of zero (\"worst\").\n",
    "        The community count is normalized by dividing it through the number of nodes in the projected Graph.\n",
    "        To give the relative community count more weight, it is multiplied by 100. \n",
    "        \n",
    "        (*1) Mane, Prachita; Shanbhag, Sunanda; Kamath, Tanmayee; Mackey, Patrick; and Springer, John, \n",
    "        \"Analysis of Community Detection Algorithms for Large Scale Cyber Networks\" (2016)\n",
    "        \"\"\"\n",
    "        soft_ramped_modularity = 1.0 - soft_ramp_limited_penalty(self.get_modularity(), 0.30, 0.35, sharpness=1)\n",
    "        score = float(self.get_community_count() * 100) / float(self.get_node_count_()) * soft_ramped_modularity\n",
    "        # - For debugging purposes:\n",
    "        # print(f\"Score {score:.4f}= community count {self.get_community_count()} x soft_ramped {soft_ramped_modularity:.4f} modularity {self.get_modularity():.04f}\")\n",
    "        return score\n",
    "\n",
    "\n",
    "    def write_communities(self):\n",
    "        \"\"\"\n",
    "        Writes the calculated communities to the Neo4j database.\n",
    "        This is useful for further processing or analysis.\n",
    "        \"\"\"\n",
    "        algorithm_parameters = self.__to_algorithm_parameters()\n",
    "        print(\"Writing communities to Neo4j with the following parameters: \" + str(algorithm_parameters))\n",
    "        query_cypher_to_data_frame_for_verbosity(self.verbose)(self.cypher_file_for_write_, parameters=algorithm_parameters)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_modularity(self) -> float:\n",
    "        \"\"\"\n",
    "        Returns the modularity (global/overall) of the community statistics\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return float(self.community_statistics_['modularity'].iloc[0])\n",
    "    \n",
    "    def get_community_count(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of detected communities\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return int(self.community_statistics_['communityCount'].iloc[0])\n",
    "    \n",
    "    def get_node_count_(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of nodes in the projected Graph\n",
    "        \"\"\"\n",
    "        self.__check_fitted()\n",
    "        return int(self.community_statistics_['nodeCount'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_hyperparameter_tuning_results(cv_results):\n",
    "    \"\"\"\n",
    "    Plots the results of the hyperparameter tuning from GridSearchCV.\n",
    "    Uses the first parameter (alphabetically) as the horizontal axis and each of the other parameters as vertical axes.\n",
    "    The mean test score is plotted against the parameter values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results : dict\n",
    "        The cv_results_ attribute from a fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plot\n",
    "    import pandas as pd\n",
    "    \n",
    "    tuning_statistics = pd.DataFrame(cv_results)\n",
    "\n",
    "    # Extract parameter names\n",
    "    parameter_names = list(tuning_statistics['params'][0].keys())\n",
    "\n",
    "    # Create subplots for the first parameter (horizontal) and each other parameter (vertical)\n",
    "    row_parameter = parameter_names[0]\n",
    "\n",
    "    # filter out the first parameter (name) from parameter_names to get the other parameters as list\n",
    "    other_parameters = [name for name in parameter_names if name != row_parameter]\n",
    "    unique_row_parameter_values = sorted(tuning_statistics['param_' + row_parameter].unique())\n",
    "    row_count = len(other_parameters)\n",
    "    column_count = len(unique_row_parameter_values)\n",
    "\n",
    "    import matplotlib.pyplot as plot\n",
    "\n",
    "    figure, axes = plot.subplots(row_count, column_count, figsize=(6 * column_count, 5 * row_count))#, sharey='row')\n",
    "    if row_count == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    if column_count == 1:\n",
    "        axes = np.expand_dims(axes, axis=1)\n",
    "\n",
    "    for column_index, row_parameter_value in enumerate(unique_row_parameter_values):\n",
    "        subset = tuning_statistics[tuning_statistics['param_' + row_parameter] == row_parameter_value]\n",
    "        for row_index, parameter_name in enumerate(other_parameters):\n",
    "            axis = axes[row_index, column_index]\n",
    "            x = subset['param_' + parameter_name]\n",
    "            y = subset['mean_test_score']\n",
    "            axis.plot(x, y, marker='o', linestyle='-')\n",
    "            axis.set_title(f\"{row_parameter}: {row_parameter_value}\\n{parameter_name}\", fontsize=12)\n",
    "            axis.set_xlabel(parameter_name)\n",
    "            if column_index == 0:\n",
    "                axis.set_ylabel(\"Mean Test Score\")\n",
    "            axis.grid(True)\n",
    "\n",
    "    figure.suptitle(f'GridSearchCV Hyperparameter Tuning Results by {row_parameter}', fontsize=16)\n",
    "    plot.tight_layout(rect=(0.0, 0.03, 1.0, 0.95))\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_importance_from_grid_search(raw_tuning_results):\n",
    "    \"\"\"\n",
    "    Plots the importance of each hyperparameter based on how much variance in the score it explains.\n",
    "    The parameter with the highest variance in mean_test_score across its values is considered most important.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results : dict\n",
    "        The cv_results_ attribute from a fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plot\n",
    "    import pandas as pd\n",
    "\n",
    "    tuning_results = pd.DataFrame(raw_tuning_results)\n",
    "    parameter_columns = [column for column in tuning_results.columns if column.startswith('param_')]\n",
    "\n",
    "    # Calculate variance in mean_test_score for each parameter\n",
    "    importances = {}\n",
    "    for parameter in parameter_columns:\n",
    "        grouped = tuning_results.groupby(parameter)['mean_test_score'].mean()\n",
    "        importances[parameter.replace('param_', '')] = grouped.var()\n",
    "\n",
    "    # Sort parameters by importance\n",
    "    sorted_importances = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Plot as horizontal bars\n",
    "    plot.figure(figsize=(10, 2))\n",
    "    plot.barh(\n",
    "        [parameter_name for parameter_name, _ in reversed(sorted_importances)],\n",
    "        [parameter_variance for _, parameter_variance in reversed(sorted_importances)]\n",
    "    )\n",
    "    plot.xlabel('Variance in Mean Test Score')\n",
    "    plot.ylabel('Parameter')\n",
    "    plot.xscale('log')  # Use logarithmic scale for better visibility\n",
    "    plot.yticks(fontsize=8)\n",
    "    plot.xticks(fontsize=8, rotation=45)\n",
    "    plot.title('Parameter Importance (higher = more influence on score)')\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_scores(raw_tuning_results):\n",
    "    \"\"\"\n",
    "    Plots the scores from GridSearchCV results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results : dict\n",
    "        The cv_results_ attribute from a fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plot\n",
    "    import pandas as pd\n",
    "\n",
    "    results = pd.DataFrame(raw_tuning_results)\n",
    "    plot.figure(figsize=(10, 4))\n",
    "    plot.plot(results['mean_test_score'], label='Score', marker='o')\n",
    "    plot.xlabel('Parameter Combination Index')\n",
    "    plot.ylabel('Score')\n",
    "    plot.title('Grid Search Scores')\n",
    "    plot.legend()\n",
    "    plot.grid(True)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_timings(raw_tuning_results):\n",
    "    \"\"\"\n",
    "    Plots the fit times from GridSearchCV results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results : dict\n",
    "        The cv_results_ attribute from a fitted GridSearchCV object.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plot\n",
    "    import pandas as pd\n",
    "\n",
    "    results = pd.DataFrame(raw_tuning_results)\n",
    "    plot.figure(figsize=(10, 4))\n",
    "    plot.plot(results['mean_fit_time'], label='Mean Fit Time (s)', marker='o')\n",
    "    plot.xlabel('Parameter Combination Index')\n",
    "    plot.ylabel('Time (seconds)')\n",
    "    plot.title('Grid Search Timings')\n",
    "    plot.legend()\n",
    "    plot.grid(True)\n",
    "    plot.tight_layout()\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_top10_parameters(raw_tuning_results):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Convert cv_results_ to DataFrame and sort by mean_test_score descending\n",
    "    tuning_results = pd.DataFrame(raw_tuning_results)\n",
    "    parameter_columns = [column for column in tuning_results.columns if column.startswith('param_')]\n",
    "\n",
    "    top10 = tuning_results.sort_values(by=\"mean_test_score\", ascending=False).head(10)\n",
    "\n",
    "    # Display only the parameter columns and the score\n",
    "    print(top10[[\"mean_test_score\", *parameter_columns]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1df20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tuning_details(tuning_results, title: str = ''):\n",
    "    \"\"\"\n",
    "    Outputs the tuning details of the GridSearchCV results.\n",
    "    Prints the best parameters, best score, and the number of evaluated parameter combinations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tuning_results_ : GridSearchCV or dict\n",
    "        The fitted GridSearchCV object or its cv_results_ attribute.\n",
    "    \"\"\"\n",
    "    embeddings_array = np.array(tuning_results.best_estimator_.get_embeddings().embedding.tolist())\n",
    "    \n",
    "    print(title + \" - Best Parameters:\", tuning_results.best_params_)\n",
    "    print(title + \" - Best Score:\", tuning_results.best_score_)\n",
    "    print(title + \" - Evaluated Combinations:\", len(tuning_results.cv_results_['params']))\n",
    "    print(title + \" - Hopkins Statistic:\", hopkins_statistic(embeddings_array))\n",
    "    print(title + \" -\", tuning_results.best_estimator_.get_clustering_scores())\n",
    "\n",
    "    plot_grid_search_hyperparameter_tuning_results(tuning_results.cv_results_)\n",
    "    plot_parameter_importance_from_grid_search(tuning_results.cv_results_)\n",
    "    plot_grid_search_scores(tuning_results.cv_results_)\n",
    "    plot_grid_search_timings(tuning_results.cv_results_)\n",
    "    list_top10_parameters(tuning_results.cv_results_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddingsCreationResult:\n",
    "    def __init__(self, embeddings: pd.DataFrame, is_sampled_graph: bool = False):\n",
    "        self.embeddings = embeddings\n",
    "        self.is_sampled_graph = is_sampled_graph\n",
    "    def __repr__(self):\n",
    "        return f\"NodeEmbeddingsCreationResult(embeddings={self.embeddings}, is_sampled_graph={self.is_sampled_graph})\"\n",
    "\n",
    "# Feature ideas\n",
    "# TODO deprecated?\n",
    "# TODO option to choose between directed and undirected projection\n",
    "# TODO run a community detection algorithm co-located in here when \"communityId\" is missing\n",
    "# TODO run a centrality algorithm co-located in here when \"centrality\" score is missing\n",
    "# TODO this function suffers from excessive parameters. Modularize it into smaller functions\n",
    "def create_node_embeddings(cypher_file_name: str, parameters: dict, ignore_existing: bool = True, create_graph_projection: bool = True, graph_sampling_threshold: int = GraphSamplingResult.default_graph_sampling_threshold) -> NodeEmbeddingsCreationResult:\n",
    "    \"\"\"\n",
    "    Creates an in-memory Graph projection by calling \"create_undirected_projection\", \n",
    "    runs the cypher Query given as cypherFileName parameter to calculate and stream the node embeddings\n",
    "    and returns a DataFrame with the results.\n",
    "    \n",
    "    cypher_file_name\n",
    "    ----------\n",
    "    Name of the file containing the Cypher query that executes node embeddings procedure.\n",
    "\n",
    "    parameters\n",
    "    ----------\n",
    "    dependencies_projection : str\n",
    "        The name prefix for the in-memory projection for dependencies. Example: \"java-package-embeddings-notebook\"\n",
    "    dependencies_projection_node : str\n",
    "        The label of the nodes that will be used for the projection. Example: \"Package\"\n",
    "    dependencies_projection_weight_property : str\n",
    "        The name of the node property that contains the dependency weight. Example: \"weight25PercentInterfaces\"\n",
    "    dependencies_projection_embedding_dimension : str\n",
    "        The number of the dimensions and therefore size of the resulting array of floating point numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    if create_graph_projection:\n",
    "        print(\"Create projection\")\n",
    "        is_data_available=create_undirected_projection(parameters)\n",
    "    \n",
    "        if not is_data_available:\n",
    "            print(\"No projected data for node embeddings calculation available\")\n",
    "            empty_result = pd.DataFrame(columns=[\"codeUnitName\", 'projectName', 'nodeElementId', 'communityId', 'centrality', 'embedding'])\n",
    "            return NodeEmbeddingsCreationResult(empty_result)\n",
    "    else:\n",
    "        print(\"Skip projection creation\")\n",
    "        \n",
    "    # Check if the graph has to be sampled because of its size\n",
    "    sampling_result=sample_graph_if_size_exceeds_limit(parameters, graph_sampling_threshold)\n",
    "    \n",
    "    node_embeddings_parameters = parameters.copy()\n",
    "    if ignore_existing:\n",
    "        embeddings = query_cypher_to_data_frame(cypher_file_name, parameters=sampling_result.updated_parameters)\n",
    "    else:    \n",
    "        existing_embeddings_query_filename=\"../cypher/Node_Embeddings/Node_Embeddings_0a_Query_Calculated.cypher\"\n",
    "        embeddings = query_first_non_empty_cypher_to_data_frame(existing_embeddings_query_filename, cypher_file_name, parameters=node_embeddings_parameters)\n",
    "    \n",
    "    display(embeddings.head()) # Display the first entries of the table\n",
    "    hopkins_statistic_value = hopkins_statistic(np.array(embeddings.embedding.tolist()))\n",
    "    print(f\"Hopkins statistic value: {hopkins_statistic_value}\")\n",
    "    \n",
    "    return NodeEmbeddingsCreationResult(embeddings, sampling_result.is_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec6a9b",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "The following function takes the original node embeddings with a higher dimensionality, e.g. 64 floating point numbers, and reduces them into a two dimensional array for visualization. \n",
    "\n",
    "> It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "\n",
    "(see https://opentsne.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720aebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_node_embeddings_for_2d_visualization_tsne(embeddings: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduces the dimensionality of the node embeddings (e.g. 64 floating point numbers in an array)\n",
    "    to two dimensions for 2D visualization.\n",
    "    see https://opentsne.readthedocs.io\n",
    "    \"\"\"\n",
    "\n",
    "    if embeddings.empty: \n",
    "        print(\"No projected data for node embeddings dimensionality reduction available\")\n",
    "        return embeddings\n",
    "    \n",
    "    # Calling the fit_transform method just with a list doesn't seem to work (anymore?). \n",
    "    # It leads to an error with the following message: 'list' object has no attribute 'shape'\n",
    "    # This can be solved by converting the list to a numpy array using np.array(..).\n",
    "    # See https://bobbyhadz.com/blog/python-attributeerror-list-object-has-no-attribute-shape\n",
    "    embeddings_as_numpy_array = np.array(embeddings.embedding.to_list())\n",
    "\n",
    "    # Use t-distributed stochastic neighbor embedding (t-SNE) to reduce the dimensionality \n",
    "    # of the previously calculated node embeddings to 2 dimensions for visualization\n",
    "    t_distributed_stochastic_neighbor_embedding = TSNE(n_components=2, verbose=False, random_state=47)\n",
    "    two_dimension_node_embeddings = t_distributed_stochastic_neighbor_embedding.fit_transform(embeddings_as_numpy_array)\n",
    "    # display(two_dimension_node_embeddings.shape) # Display the shape of the t-SNE result\n",
    "\n",
    "    # Create a new DataFrame with the results of the 2 dimensional node embeddings\n",
    "    # and the code unit and artifact name of the query above as preparation for the plot\n",
    "    embeddings['embeddingVisualizationX'] = [value[0] for value in two_dimension_node_embeddings]\n",
    "    embeddings['embeddingVisualizationY'] = [value[1] for value in two_dimension_node_embeddings]\n",
    "\n",
    "    # display(embeddings.head(10)) # Display the first line of the results\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b83c2",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "The following function takes the original node embeddings with a higher dimensionality, e.g. 64 floating point numbers, and reduces them into a two dimensional array for visualization using UMAP.\n",
    "\n",
    "> UMAP is a non-linear dimensionality reduction technique that preserves both local and global structure of the data, making it well-suited for visualizing high-dimensional embeddings in 2D.\n",
    "\n",
    "(see https://umap-learn.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "def prepare_node_embeddings_for_2d_visualization_umap(embeddings: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduces the dimensionality of the node embeddings (e.g. 64 floating point numbers in an array)\n",
    "    to two dimensions for 2D visualization using UMAP.\n",
    "    see https://umap-learn.readthedocs.io\n",
    "    \"\"\"\n",
    "\n",
    "    if embeddings.empty: \n",
    "        print(\"No projected data for node embeddings dimensionality reduction available\")\n",
    "        return embeddings\n",
    "\n",
    "    # Convert the list of embeddings to a numpy array\n",
    "    embeddings_as_numpy_array = np.array(embeddings.embedding.to_list())\n",
    "\n",
    "    # Use UMAP to reduce the dimensionality to 2D for visualization\n",
    "    # umap_reducer = umap.UMAP(min_dist=0.3, n_neighbors=15, n_components=2, metric='manhattan', random_state=47)\n",
    "    umap_reducer = umap.UMAP(n_components=2, min_dist=0.3, random_state=47)\n",
    "    two_dimension_node_embeddings = umap_reducer.fit_transform(embeddings_as_numpy_array)\n",
    "\n",
    "    # Add the 2D coordinates to the DataFrame\n",
    "    embeddings['embeddingUMAPVisualizationX'] = two_dimension_node_embeddings[:, 0]\n",
    "    embeddings['embeddingUMAPVisualizationY'] = two_dimension_node_embeddings[:, 1]\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_node_embeddings_for_2d_visualization(embeddings: pd.DataFrame) -> pd.DataFrame:\n",
    "    embeddings = prepare_node_embeddings_for_2d_visualization_tsne(embeddings)\n",
    "    embeddings = prepare_node_embeddings_for_2d_visualization_umap(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO delete if not used anymore\n",
    "def plot_2d_node_embeddings_old(node_embeddings_for_visualization: pd.DataFrame, title: str, clustering_name: str = \"TunedHDBSCAN\", main_color_map: str = \"tab20\") -> None:\n",
    "    if node_embeddings_for_visualization.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "    \n",
    "    figure, (top, bottom) = plot.subplots(nrows=2, ncols=1, figsize=(8, 10))\n",
    "    figure.suptitle(title)\n",
    "    figure.subplots_adjust(top=0.92, left=0.01, right=0.99, bottom=0.01, hspace=0.2)\n",
    "\n",
    "    node_embeddings_non_noise_cluster = node_embeddings_for_visualization[node_embeddings_for_visualization[get_clustering_property_name('Label', clustering_name)] != -1]\n",
    "    node_embeddings_noise_cluster = node_embeddings_for_visualization[node_embeddings_for_visualization[get_clustering_property_name('Label', clustering_name)] == -1]\n",
    "\n",
    "    # Print the graph communities as a reference in the top plot\n",
    "    top.set_title(\"Leiden Community Detection\")\n",
    "    top.scatter(\n",
    "        x=node_embeddings_for_visualization.embeddingVisualizationX,\n",
    "        y=node_embeddings_for_visualization.embeddingVisualizationY,\n",
    "        s=node_embeddings_for_visualization.centrality * 300,\n",
    "        c=node_embeddings_for_visualization.communityId,\n",
    "        cmap=main_color_map,\n",
    "    )\n",
    "\n",
    "    # Print the clustering results based on the node embeddings in the bottom plot\n",
    "    bottom.set_title(\"HDBSCAN Clustering\")\n",
    "    bottom.scatter(\n",
    "        x=node_embeddings_non_noise_cluster.embeddingVisualizationX,\n",
    "        y=node_embeddings_non_noise_cluster.embeddingVisualizationY,\n",
    "        s=node_embeddings_non_noise_cluster.centrality * 300,\n",
    "        c=node_embeddings_non_noise_cluster[get_clustering_property_name('Label', clustering_name)],\n",
    "        cmap=main_color_map,\n",
    "    )\n",
    "    bottom.scatter(\n",
    "        x=node_embeddings_noise_cluster.embeddingVisualizationX,\n",
    "        y=node_embeddings_noise_cluster.embeddingVisualizationY,\n",
    "        s=node_embeddings_noise_cluster.centrality * 300,\n",
    "        c='lightgrey'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80968112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn\n",
    "import numpy as np\n",
    "\n",
    "def plot_2d_node_embeddings(\n",
    "    node_embeddings_for_visualization: pd.DataFrame,\n",
    "    title: str,\n",
    "    clustering_name: str = \"TunedHDBSCAN\",\n",
    "    main_color_map: str = \"tab20\",\n",
    "    x_position_column = 'embeddingVisualizationX',\n",
    "    y_position_column = 'embeddingVisualizationY'\n",
    ") -> None:\n",
    "    if node_embeddings_for_visualization.empty:\n",
    "        print(\"No projected data to plot available\")\n",
    "        return\n",
    "\n",
    "    # Create figure and subplots\n",
    "    figure, (leiden_subplot, hdbscan_subplot) = plot.subplots(nrows=2, ncols=1, figsize=(10, 12))\n",
    "    figure.suptitle(title)\n",
    "    figure.subplots_adjust(top=0.94, left=0.05, right=0.95, bottom=0.04, hspace=0.25)\n",
    "\n",
    "    # Setup columns\n",
    "    cluster_label_column_name = get_clustering_property_name('Label', clustering_name)\n",
    "    node_size_column = 'centrality'\n",
    "\n",
    "    # Separate HDBSCAN non-noise and noise nodes\n",
    "    node_embeddings_without_noise = node_embeddings_for_visualization[node_embeddings_for_visualization[cluster_label_column_name] != -1]\n",
    "    node_embeddings_noise_only = node_embeddings_for_visualization[node_embeddings_for_visualization[cluster_label_column_name] == -1]\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Top subplot: Leiden Communities with KDE\n",
    "    # ------------------------------------------\n",
    "    leiden_subplot.set_title(\"Leiden Community Detection\")\n",
    "\n",
    "    unique_community_ids = node_embeddings_for_visualization[\"communityId\"].unique()\n",
    "    leiden_color_palette = seaborn.color_palette(main_color_map, len(unique_community_ids))\n",
    "    leiden_community_to_color = dict(zip(unique_community_ids, leiden_color_palette))\n",
    "\n",
    "    for community_id in unique_community_ids:\n",
    "        community_nodes = node_embeddings_for_visualization[\n",
    "            node_embeddings_for_visualization[\"communityId\"] == community_id\n",
    "        ]\n",
    "\n",
    "        # KDE cloud shape\n",
    "        seaborn.kdeplot(\n",
    "            x=community_nodes[x_position_column],\n",
    "            y=community_nodes[y_position_column],\n",
    "            fill=True,\n",
    "            alpha=0.12,\n",
    "            levels=3,\n",
    "            color=leiden_community_to_color[community_id],\n",
    "            ax=leiden_subplot,\n",
    "        )\n",
    "\n",
    "        # Node scatter points\n",
    "        leiden_subplot.scatter(\n",
    "            x=community_nodes[x_position_column],\n",
    "            y=community_nodes[y_position_column],\n",
    "            s=community_nodes[node_size_column] * 300,\n",
    "            color=leiden_community_to_color[community_id],\n",
    "            alpha=0.7,\n",
    "            label=f\"Community {community_id}\"\n",
    "        )\n",
    "\n",
    "    leiden_subplot.legend(title=\"Leiden Communities\", loc=\"best\", prop={'size': 6})\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Bottom subplot: HDBSCAN Clustering with KDE\n",
    "    # ------------------------------------------\n",
    "    hdbscan_subplot.set_title(\"HDBSCAN Clustering\")\n",
    "\n",
    "    unique_cluster_labels = node_embeddings_without_noise[cluster_label_column_name].unique()\n",
    "    hdbscan_color_palette = seaborn.color_palette(main_color_map, len(unique_cluster_labels))\n",
    "    hdbscan_cluster_to_color = dict(zip(unique_cluster_labels, hdbscan_color_palette))\n",
    "\n",
    "    for cluster_label in unique_cluster_labels:\n",
    "        cluster_nodes = node_embeddings_without_noise[\n",
    "            node_embeddings_without_noise[cluster_label_column_name] == cluster_label\n",
    "        ]\n",
    "\n",
    "        # KDE cloud shape\n",
    "        seaborn.kdeplot(\n",
    "            x=cluster_nodes[x_position_column],\n",
    "            y=cluster_nodes[y_position_column],\n",
    "            fill=True,\n",
    "            alpha=0.05,\n",
    "            levels=2,\n",
    "            color=hdbscan_cluster_to_color[cluster_label],\n",
    "            ax=hdbscan_subplot,\n",
    "            # linewidths=0\n",
    "        )\n",
    "\n",
    "        # Node scatter points\n",
    "        hdbscan_subplot.scatter(\n",
    "            x=cluster_nodes[x_position_column],\n",
    "            y=cluster_nodes[y_position_column],\n",
    "            s=cluster_nodes[node_size_column] * 300,\n",
    "            color=hdbscan_cluster_to_color[cluster_label],\n",
    "            alpha=0.9,\n",
    "            label=f\"Cluster {cluster_label}\"\n",
    "        )\n",
    "\n",
    "    # Plot noise points in gray\n",
    "    hdbscan_subplot.scatter(\n",
    "        x=node_embeddings_noise_only[x_position_column],\n",
    "        y=node_embeddings_noise_only[y_position_column],\n",
    "        s=node_embeddings_noise_only[node_size_column] * 300,\n",
    "        color='lightgrey',\n",
    "        alpha=0.4,\n",
    "        label=\"Noise\"\n",
    "    )\n",
    "\n",
    "    hdbscan_subplot.legend(title=\"HDBSCAN Clusters\", loc=\"best\", prop={'size': 6})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68aa20",
   "metadata": {},
   "source": [
    "## 1. Java Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6cac2",
   "metadata": {},
   "source": [
    "### 1.1 Create Graph Projection\n",
    "\n",
    "To be able to run Graph algorithms efficiently and to focus on specific parts of the Graph, e.g. dependencies between code units, an in-memory \"projection\" is created containing the selected part of the Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_package_projection_parameters={\n",
    "    \"dependencies_projection\": \"java-package-embeddings-notebook\",\n",
    "    \"dependencies_projection_node\": \"Package\",\n",
    "    \"dependencies_projection_weight_property\": \"weight25PercentInterfaces\",\n",
    "}\n",
    "# Create a undirected graph projection for the Java Package nodes\n",
    "java_package_data_available = create_undirected_projection(java_package_projection_parameters)\n",
    "if java_package_data_available:\n",
    "    # Sample the graph (take a smaller subgraph of it) if it exceeds the size limit\n",
    "    # The updated parameters and node_count contain the original values if no sampling was necessary\n",
    "    java_package_sampling_result = sample_graph_if_size_exceeds_limit(java_package_projection_parameters)\n",
    "    java_package_sampled_projection_parameters = java_package_sampling_result.updated_parameters\n",
    "    java_package_node_count = java_package_sampling_result.node_count\n",
    "else:\n",
    "    print(\"No projected data for Java Package node embeddings calculation available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ea9fe",
   "metadata": {},
   "source": [
    "### 1.2 Use Leiden Community Detection Algorithm results as reference\n",
    "\n",
    "Before we create node embeddings, we will run the Leiden Community detection algorithm to get modularity optimized community ids that we will use later as a \"gold standard\" to tune the results of the node embedding clustering. \n",
    "\n",
    "The intuition/idea behind that is that we then get clusters in the vector space (node embeddings) that are close to each other (manhattan distance), when the represented code units are also coupled together. Density based clustering works of course differently and leads to different insights about the structural features of the code units so that it will (and also should) not match the Leiden communities perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_leiden_community_detection_algorithm(projection_parameters: dict) -> TuneableLeidenCommunityDetection:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "\n",
    "    def objective(trial):\n",
    "        # Here we intentionally use the original projection parameters, not the sampled ones,\n",
    "        # since the sampling is not necessary for Fast Random Projection embeddings.\n",
    "        tuneable_leiden_community_detection = create_tuneable(TuneableLeidenCommunityDetection).with_projection_parameters(projection_parameters)\n",
    "        # Suggest values for each hyperparameter\n",
    "        tuneable_leiden_community_detection.set_params(\n",
    "            gamma=trial.suggest_float(\"gamma\", low=0.7, high=1.3, step=0.01),\n",
    "            theta = trial.suggest_float(\"theta\", 0.0001, 0.01, log=True),\n",
    "            # Fixed max_levels = 10 (default) since experiments showed only minor differences in the results\n",
    "            # max_levels = trial.suggest_int(\"max_levels\", 8, 12)\n",
    "        )\n",
    "        tuneable_leiden_community_detection.fit()\n",
    "        return tuneable_leiden_community_detection.score()\n",
    "\n",
    "    # TODO create study with db?\n",
    "    study_name = \"LeidenCommunityDetection4Java\" + projection_parameters[\"dependencies_projection_node\"]\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42), study_name=study_name)#, storage=f\"sqlite:///optuna_study_node_embeddings_java.db\", load_if_exists=True)\n",
    "    \n",
    "    # Start the hyperparameter tuning\n",
    "    study.optimize(objective, n_trials=20, timeout=20)\n",
    "    output_optuna_tuning_results(study, 'Leiden Community Detection')\n",
    "\n",
    "    # Try (enqueue) specific settings first that led to good results in initial experiments\n",
    "    study.enqueue_trial({'gamma': 1.0, 'theta': 0.001, 'max_levels': 10}) # default values\n",
    "    study.enqueue_trial({'gamma': 1.14, 'theta': 0.001, 'max_levels': 10})\n",
    " \n",
    "    # Run the node embeddings algorithm again again with the best parameters\n",
    "    tuned_leiden_community_detection = create_tuneable(TuneableLeidenCommunityDetection).with_projection_parameters(projection_parameters)\n",
    "    tuned_leiden_community_detection.set_params(**study.best_params)\n",
    "    tuned_leiden_community_detection.fit()\n",
    "\n",
    "    print(\"Best Leiden Community Detection Modularity\", tuned_leiden_community_detection.get_modularity())\n",
    "    print(\"Best Leiden Community Detection Community Count\", tuned_leiden_community_detection.get_community_count())\n",
    " \n",
    "    return tuned_leiden_community_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48962d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if java_package_node_count > 0:\n",
    "    tuned_leiden_community_detection = get_tuned_leiden_community_detection_algorithm(java_package_projection_parameters)\n",
    "    tuned_leiden_community_detection.write_communities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d474706",
   "metadata": {},
   "source": [
    "### 1.3 Generate Node Embeddings using Fast Random Projection (Fast RP) for Java Packages\n",
    "\n",
    "[Fast Random Projection](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp) is used to reduce the dimensionality of the node feature space while preserving most of the distance information. Nodes with similar neighborhood result in node embedding with similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b27778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Random Projection tuned with Optuna\n",
    "\n",
    "def get_tuned_fast_random_projection_node_embeddings(projection_parameters: dict) -> TuneableFastRandomProjectionNodeEmbeddings:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "\n",
    "    def objective(trial):\n",
    "        # Here we intentionally use the original projection parameters, not the sampled ones,\n",
    "        # since the sampling is not necessary for Fast Random Projection embeddings.\n",
    "        tuneable_fast_random_projection = create_tuneable(TuneableFastRandomProjectionNodeEmbeddings).with_projection_parameters(projection_parameters)\n",
    "        # Suggest values for each hyperparameter\n",
    "        tuneable_fast_random_projection.set_params(\n",
    "            embedding_dimension=trial.suggest_categorical(\"embedding_dimension\", [64, 128, 256]),\n",
    "            fast_random_projection_normalization_strength=trial.suggest_float(\"fast_random_projection_normalization_strength\", low=-1.0, high=1.0, step=0.1),\n",
    "            fast_random_projection_forth_iteration_weight=trial.suggest_float(\"fast_random_projection_forth_iteration_weight\", low=0.0, high=2.0, step=0.1),\n",
    "        )\n",
    "        tuneable_fast_random_projection.fit()\n",
    "        return tuneable_fast_random_projection.score()\n",
    "\n",
    "    # TODO create study with db?\n",
    "    study_name = \"FastRandomProjection4Java\" + projection_parameters[\"dependencies_projection_node\"]\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42), study_name=study_name)#, storage=f\"sqlite:///optuna_study_node_embeddings_java.db\", load_if_exists=True)\n",
    "    \n",
    "    # Try (enqueue) specific settings first that led to good results in initial experiments\n",
    "    study.enqueue_trial({'embedding_dimension': 128, 'fast_random_projection_forth_iteration_weight': 0.5, 'fast_random_projection_normalization_strength': 0.3})\n",
    "    study.enqueue_trial({'embedding_dimension': 128, 'fast_random_projection_forth_iteration_weight': 1.0, 'fast_random_projection_normalization_strength': 0.5})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'fast_random_projection_forth_iteration_weight': 0.5, 'fast_random_projection_normalization_strength': 0.3})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'fast_random_projection_forth_iteration_weight': 1.0, 'fast_random_projection_normalization_strength': 0.3})\n",
    "    \n",
    "    # Start the hyperparameter tuning\n",
    "    study.optimize(objective, n_trials=80, timeout=40)\n",
    "    output_optuna_tuning_results(study, 'Fast Random Projection (FastRP)')\n",
    "\n",
    "    # Run the node embeddings algorithm again again with the best parameters\n",
    "    tuned_fast_random_projection = create_tuneable(TuneableFastRandomProjectionNodeEmbeddings).with_projection_parameters(projection_parameters)\n",
    "    tuned_fast_random_projection.set_params(**study.best_params)\n",
    "    return tuned_fast_random_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc72914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Keep solution (either Optuna or classic)\n",
    "if java_package_node_count > 0:\n",
    "    tuned_fast_random_projection = get_tuned_fast_random_projection_node_embeddings(java_package_projection_parameters)\n",
    "    # TODO Write the results back into the Neo4j database\n",
    "    #tuned_fast_random_projection.best_estimator_.write_embeddings()\n",
    "    embeddings = tuned_fast_random_projection.fit().get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "# ------\n",
    "tuneable_fast_random_projection_parameter_grid = {\n",
    "    \"embedding_dimension\": [64, 128, 256],\n",
    "    \"random_seed\": [42], # Fixed random seed since experiments showed only minor differences in the results\n",
    "    \"fast_random_projection_normalization_strength\": [-0.9, -0.5, -0.4, -0.3, -0.2, 0.0, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"fast_random_projection_forth_iteration_weight\": [0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Here we intentionally use the original projection parameters, not the sampled ones,\n",
    "# since the sampling is not necessary for Fast Random Projection embeddings.\n",
    "tuneable_fast_random_projection = create_tuneable(TuneableFastRandomProjectionNodeEmbeddings).with_projection_parameters(java_package_projection_parameters)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameter_tuning = GridSearchCV(\n",
    "    estimator=tuneable_fast_random_projection,\n",
    "    param_grid=tuneable_fast_random_projection_parameter_grid,\n",
    "    cv=get_all_data_without_slicing_cross_validator_for_node_count(java_package_node_count),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if java_package_node_count > 0:\n",
    "    reset_node_embedding_tuning_scores()\n",
    "    tuned_fast_random_projection = hyperparameter_tuning.fit(get_initial_dummy_data_for_hyperparameter_tuning(java_package_node_count))\n",
    "    output_tuning_details(tuned_fast_random_projection, 'Tuned Fast Random Projection for Java Packages')\n",
    "    output_node_embedding_tuning_scores()\n",
    "\n",
    "    embeddings = tuned_fast_random_projection.best_estimator_.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    # Write the results back into the Neo4j database\n",
    "    tuned_fast_random_projection.best_estimator_.write_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d8bca1",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction with t-distributed stochastic neighbor embedding (t-SNE)\n",
    "\n",
    "This step takes the original node embeddings with a higher dimensionality, e.g. 64 floating point numbers, and reduces them into a two dimensional array for visualization. For more details look up the function declaration for \"prepare_node_embeddings_for_2d_visualization\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031abacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if java_package_data_available:\n",
    "    node_embeddings_for_visualization = prepare_node_embeddings_for_2d_visualization(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908c47f",
   "metadata": {},
   "source": [
    "#### Visualization of the node embeddings reduced to two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if java_package_data_available:\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Package positioned by their dependency relationships (FastRP node embeddings + t-SNE)\"\n",
    "    )\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Package positioned by their dependency relationships (FastRP node embeddings + UMAP)\",\n",
    "        x_position_column='embeddingUMAPVisualizationX',\n",
    "        y_position_column='embeddingUMAPVisualizationY'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e67fe",
   "metadata": {},
   "source": [
    "#### Write the results (clustering, 2d embedding for visualization) back into the Neo4j database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0908046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_write = pd.DataFrame(data = {\n",
    "    'nodeElementId': embeddings[\"nodeElementId\"],\n",
    "    'clusteringHDBSCANLabel': embeddings[get_clustering_property_name('Label')],\n",
    "    'clusteringHDBSCANProbability': embeddings[get_clustering_property_name('Probability')],\n",
    "    'embeddingFastRandomProjectionVisualizationX': embeddings[\"embeddingVisualizationX\"],\n",
    "    'embeddingFastRandomProjectionVisualizationY': embeddings[\"embeddingVisualizationY\"],\n",
    "    })\n",
    "write_batch_data_into_database(data_to_write, 'Package')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b690b9a7",
   "metadata": {},
   "source": [
    "### 1.4 Node Embeddings for Java Packages using HashGNN\n",
    "\n",
    "[HashGNN](https://neo4j.com/docs/graph-data-science/2.6/machine-learning/node-embeddings/hashgnn) resembles Graph Neural Networks (GNN) but does not include a model or require training. It combines ideas of GNNs and fast randomized algorithms. For more details see [HashGNN](https://neo4j.com/docs/graph-data-science/2.6/machine-learning/node-embeddings/hashgnn). In this section we combine all previously separately explained steps into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_hashgnn_node_embeddings(projection_parameters: dict) -> TuneableHashGNNNodeEmbeddings:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.importance import get_param_importances\n",
    "\n",
    "    def objective(trial):\n",
    "        tuneable_hashgnn = create_tuneable(TuneableHashGNNNodeEmbeddings).with_projection_parameters(projection_parameters)\n",
    "        # Suggest values for each hyperparameter\n",
    "        tuneable_hashgnn.set_params(\n",
    "            embedding_dimension=trial.suggest_categorical(\"embedding_dimension\", [64, 128, 256]),\n",
    "            hashgnn_density_level=trial.suggest_categorical(\"hashgnn_density_level\", [1, 2]),\n",
    "            hashgnn_dimension_multiplier=trial.suggest_categorical(\"hashgnn_dimension_multiplier\", [1, 2]),\n",
    "            hashgnn_iterations=trial.suggest_categorical(\"hashgnn_iterations\", [2, 4]),\n",
    "            hashgnn_neighbor_influence=trial.suggest_categorical(\"hashgnn_neighbor_influence\", [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0, 2.0, 5.0, 10.0]),\n",
    "            random_seed=42, #trial.suggest_categorical(\"random_seed\", [42, 2025]),\n",
    "        )\n",
    "        tuneable_hashgnn.fit()\n",
    "        return tuneable_hashgnn.score()\n",
    "\n",
    "    # TODO create study with db?\n",
    "    study_name =  \"HashGNN4Java\" + projection_parameters[\"dependencies_projection_node\"]\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42), study_name=study_name)#, storage=f\"sqlite:///optuna_study_node_embeddings_java.db\", load_if_exists=True)\n",
    "    # Try (enqueue) specific settings first which led to good results in initial experiments\n",
    "    study.enqueue_trial({'embedding_dimension': 128, 'hashgnn_density_level': 2, 'hashgnn_dimension_multiplier': 1, 'hashgnn_iterations': 2, 'hashgnn_neighbor_influence': 1.0})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'hashgnn_density_level': 2, 'hashgnn_dimension_multiplier': 1, 'hashgnn_iterations': 2, 'hashgnn_neighbor_influence': 0.7})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'hashgnn_density_level': 2, 'hashgnn_dimension_multiplier': 1, 'hashgnn_iterations': 4, 'hashgnn_neighbor_influence': 1.0})\n",
    "    # Start the hyperparameter tuning\n",
    "    study.optimize(objective, n_trials=80, timeout=40)\n",
    "    output_optuna_tuning_results(study, 'HashGNN')\n",
    "\n",
    "    print(\"Best HashGNN parameters (Optuna):\", study.best_params)\n",
    "    print(\"Best HashGNN adjusted mutual info score with noise penalty:\", study.best_value)\n",
    "    print(\"Best HashGNN parameter influence:\", get_param_importances(study))\n",
    "\n",
    "    # Run the node embeddings algorithm again again with the best parameters\n",
    "    tuned_hashgnn = create_tuneable(TuneableHashGNNNodeEmbeddings).with_projection_parameters(projection_parameters)\n",
    "    tuned_hashgnn.set_params(**study.best_params)\n",
    "    return tuned_hashgnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19426811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Keep one solution (Optuna vs. GridSearch) \n",
    "if java_package_node_count > 0:\n",
    "    tuned_hashgnn = get_tuned_hashgnn_node_embeddings(java_package_sampled_projection_parameters)\n",
    "\n",
    "    if java_package_sampling_result.is_sampled:\n",
    "        tuned_hashgnn.refit_with_projection(java_package_projection_parameters[\"dependencies_projection\"])\n",
    "    else:\n",
    "        tuned_hashgnn.fit()\n",
    "        \n",
    "    embeddings = tuned_hashgnn.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    node_embeddings_for_visualization = prepare_node_embeddings_for_2d_visualization(embeddings)\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization,\n",
    "        \"Java Packages positioned by their dependency relationships (HashGNN + t-SNE)\"\n",
    "    )\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Packages positioned by their dependency relationships (HashGNN + UMAP)\",\n",
    "        x_position_column='embeddingUMAPVisualizationX',\n",
    "        y_position_column='embeddingUMAPVisualizationY'\n",
    "    )\n",
    "# -------\n",
    "tuneable_hashgnn_parameter_grid = {\n",
    "    \"embedding_dimension\": [64, 128, 256],\n",
    "    # \"random_seed\": [42, 2023], # Fixed random seed since experiments showed only minor differences in the results\n",
    "    \"hashgnn_iterations\": [2, 4],\n",
    "    \"hashgnn_density_level\": [1, 2],\n",
    "    \"hashgnn_neighbor_influence\": [0.7, 1.0, 5.0, 10.0], #  [0.1, 0.7, 1.0, 5.0, 10.0],\n",
    "    \"hashgnn_dimension_multiplier\": [1, 2],\n",
    "}\n",
    "\n",
    "tuneable_hashgnn = create_tuneable(TuneableHashGNNNodeEmbeddings).with_projection_parameters(java_package_sampled_projection_parameters)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameter_tuning = GridSearchCV(\n",
    "    estimator=tuneable_hashgnn,\n",
    "    param_grid=tuneable_hashgnn_parameter_grid,\n",
    "    cv=get_all_data_without_slicing_cross_validator_for_node_count(java_package_node_count),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if java_package_node_count > 0:\n",
    "    reset_node_embedding_tuning_scores()\n",
    "    tuned_hashgnn = hyperparameter_tuning.fit(get_initial_dummy_data_for_hyperparameter_tuning(java_package_node_count))\n",
    "    output_tuning_details(tuned_hashgnn, 'Tuned HashGNN for Java Packages')\n",
    "    output_node_embedding_tuning_scores()\n",
    "\n",
    "    if java_package_sampling_result.is_sampled:\n",
    "        tuned_hashgnn.best_estimator_.refit_with_projection(java_package_projection_parameters[\"dependencies_projection\"])\n",
    "\n",
    "    embeddings = tuned_hashgnn.best_estimator_.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    plot_2d_node_embeddings(\n",
    "        prepare_node_embeddings_for_2d_visualization(embeddings),\n",
    "        \"Java Packages positioned by their dependency relationships (HashGNN + t-SNE)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d88b4",
   "metadata": {},
   "source": [
    "### 1.5 Node Embeddings for Java Packages using node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1248226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_node2vec_node_embeddings(projection_parameters: dict) -> TuneableNode2VecNodeEmbeddings:\n",
    "    from typing import cast\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "    from optuna.importance import get_param_importances\n",
    "\n",
    "    def objective(trial):\n",
    "        tuneable_nod2vec = create_tuneable(TuneableNode2VecNodeEmbeddings).with_projection_parameters(projection_parameters)\n",
    "        # Suggest values for each hyperparameter\n",
    "        tuneable_nod2vec.set_params(\n",
    "            embedding_dimension=trial.suggest_categorical(\"embedding_dimension\", [32, 64, 128, 256]),\n",
    "            node2vec_in_out_factor=trial.suggest_float(\"node2vec_in_out_factor\", low=0.25, high=2.0, step=0.25),\n",
    "            node2vec_return_factor=trial.suggest_float(\"node2vec_return_factor\", low=0.25, high=2.5, step=0.25),\n",
    "            node2vec_window_size=trial.suggest_categorical(\"node2vec_window_size\", [5, 10]),\n",
    "        )\n",
    "        tuneable_nod2vec.fit()\n",
    "        return tuneable_nod2vec.score()\n",
    "\n",
    "    # TODO create study with db?\n",
    "    study_name = \"Node2Vec4Java\" + projection_parameters[\"dependencies_projection_node\"]\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42), study_name=study_name)#, storage=f\"sqlite:///optuna_study_node_embeddings_java.db\", load_if_exists=True)\n",
    "    # Try (enqueue) specific settings first which led to good results in local experiments\n",
    "    study.enqueue_trial({'embedding_dimension': 32, 'node2vec_in_out_factor': 1.25, 'node2vec_return_factor': 1.5, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 32, 'node2vec_in_out_factor': 1.25, 'node2vec_return_factor': 1.75, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 32, 'node2vec_in_out_factor': 1.75, 'node2vec_return_factor': 1.5, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 64, 'node2vec_in_out_factor': 0.5, 'node2vec_return_factor': 2.0, 'node2vec_window_size': 5})\n",
    "    study.enqueue_trial({'embedding_dimension': 64, 'node2vec_in_out_factor': 0.75, 'node2vec_return_factor': 0.75, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 64, 'node2vec_in_out_factor': 0.75, 'node2vec_return_factor': 2.5, 'node2vec_window_size': 5})\n",
    "    study.enqueue_trial({'embedding_dimension': 64, 'node2vec_in_out_factor': 1.0, 'node2vec_return_factor': 1.0, 'node2vec_window_size': 5})\n",
    "    study.enqueue_trial({'embedding_dimension': 64, 'node2vec_in_out_factor': 1.25, 'node2vec_return_factor': 1.5, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 128, 'node2vec_in_out_factor': 0.5, 'node2vec_return_factor': 2.0, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 128, 'node2vec_in_out_factor': 0.5, 'node2vec_return_factor': 2.25, 'node2vec_window_size': 5})\n",
    "    study.enqueue_trial({'embedding_dimension': 128, 'node2vec_in_out_factor': 1.25, 'node2vec_return_factor': 1.75, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'node2vec_in_out_factor': 0.5, 'node2vec_return_factor': 1.75, 'node2vec_window_size': 5})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'node2vec_in_out_factor': 0.5, 'node2vec_return_factor': 2.0, 'node2vec_window_size': 5})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'node2vec_in_out_factor': 1.25, 'node2vec_return_factor': 1.5, 'node2vec_window_size': 10})\n",
    "    study.enqueue_trial({'embedding_dimension': 256, 'node2vec_in_out_factor': 1.25, 'node2vec_return_factor': 1.75, 'node2vec_window_size': 10})\n",
    "    # Start the hyperparameter tuning\n",
    "    study.optimize(objective, n_trials=80, timeout=40)\n",
    "    output_optuna_tuning_results(study, 'node2vec')\n",
    "\n",
    "    # Run the node embeddings algorithm again again with the best parameters\n",
    "    tuned_node2vec = create_tuneable(TuneableNode2VecNodeEmbeddings).with_projection_parameters(projection_parameters)\n",
    "    tuned_node2vec.set_params(**study.best_params)\n",
    "    return tuned_node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Keep one solution (Optuna vs. GridSearch) \n",
    "if java_package_node_count > 0:\n",
    "    tuned_node2vec = get_tuned_node2vec_node_embeddings(java_package_sampled_projection_parameters)\n",
    "    \n",
    "    if java_package_sampling_result.is_sampled:\n",
    "        tuned_node2vec.refit_with_projection(java_package_projection_parameters[\"dependencies_projection\"])\n",
    "    else:\n",
    "        tuned_node2vec.fit()\n",
    "    \n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(tuned_node2vec.get_embeddings()).embeddings\n",
    "    display(embeddings.head())\n",
    "    \n",
    "    node_embeddings_for_visualization = prepare_node_embeddings_for_2d_visualization(embeddings)\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization,\n",
    "        \"Java Packages positioned by their dependency relationships (node2vec + t-SNE)\"\n",
    "    )\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Packages positioned by their dependency relationships (node2vec + UMAP)\",\n",
    "        x_position_column='embeddingUMAPVisualizationX',\n",
    "        y_position_column='embeddingUMAPVisualizationY'\n",
    "    )\n",
    "# -------\n",
    "    \n",
    "tuneable_node2vec_parameter_grid = {\n",
    "    \"embedding_dimension\": [32, 64, 128], # 256 rarely improves the results, but increases the computation time\n",
    "    \"node2vec_in_out_factor\": [0.25, 0.5, 1.0, 2.0], # [0.25, 0.5, 1.0, 2.0, 4.0]\n",
    "    \"node2vec_return_factor\": [0.25, 0.5, 1.0, 2.0, 4.0], # [0.25, 0.5, 1.0, 2.0, 4.0]\n",
    "    \"node2vec_negative_sampling_rate\": [5, 10],\n",
    "    # \"node2vec_window_size\": [5, 10],\n",
    "    # \"random_seed\": [42], # Fixed random seed since experiments showed only minor differences in the results\n",
    "    # \"node2vec_walk_length\": [80], # [40, 80, 160],\n",
    "    # \"node2vec_walks_per_node\": [10], # [5, 10],\n",
    "    # \"node2vec_iterations\": [1],\n",
    "    # \"node2vec_positive_sampling_factor\": [0.001],\n",
    "}\n",
    "\n",
    "tuneable_node2vec = create_tuneable(TuneableNode2VecNodeEmbeddings).with_projection_parameters(java_package_sampled_projection_parameters)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameter_tuning = GridSearchCV(\n",
    "    estimator=tuneable_node2vec,\n",
    "    param_grid=tuneable_node2vec_parameter_grid,\n",
    "    cv=get_all_data_without_slicing_cross_validator_for_node_count(java_package_node_count),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if java_package_node_count > 0:\n",
    "    reset_node_embedding_tuning_scores()\n",
    "    tuned_nod2vec = hyperparameter_tuning.fit(get_initial_dummy_data_for_hyperparameter_tuning(java_package_node_count))\n",
    "    output_tuning_details(tuned_nod2vec, 'Tuned node2vec for Java Packages')\n",
    "    output_node_embedding_tuning_scores()\n",
    "\n",
    "    if java_package_sampling_result.is_sampled:\n",
    "        tuned_nod2vec.best_estimator_.refit_with_projection(java_package_projection_parameters[\"dependencies_projection\"])\n",
    "\n",
    "    embeddings = tuned_nod2vec.best_estimator_.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    plot_2d_node_embeddings(\n",
    "        prepare_node_embeddings_for_2d_visualization(embeddings),\n",
    "        \"Java Packages positioned by their dependency relationships (node2vec + t-SNE)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5682bb64",
   "metadata": {},
   "source": [
    "## 2. Java Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25370d7f",
   "metadata": {},
   "source": [
    "### 2.1 Create Graph Projection\n",
    "\n",
    "To be able to run Graph algorithms efficiently and to focus on specific parts of the Graph, e.g. dependencies between code units, an in-memory \"projection\" is created containing the selected part of the Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_type_projection_parameters={\n",
    "    \"dependencies_projection\": \"java-type-embeddings-notebook\",\n",
    "    \"dependencies_projection_node\": \"Type\",\n",
    "    \"dependencies_projection_weight_property\": \"weight\",\n",
    "}\n",
    "# Create a undirected graph projection for the Java Type nodes\n",
    "java_type_data_available = create_undirected_projection(java_type_projection_parameters)\n",
    "if java_type_data_available:\n",
    "    # Sample the graph (take a smaller subgraph of it) if it exceeds the size limit\n",
    "    # The updated parameters and node_count contain the original values if no sampling was necessary\n",
    "    java_type_sampling_result = sample_graph_if_size_exceeds_limit(java_type_projection_parameters)\n",
    "    java_type_sampled_projection_parameters = java_type_sampling_result.updated_parameters\n",
    "    java_type_node_count = java_type_sampling_result.node_count\n",
    "else:    \n",
    "    print(\"No projected data for Java Type node embeddings calculation available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e55ae",
   "metadata": {},
   "source": [
    "### 2.2 Use Leiden Community Detection Algorithm results as reference\n",
    "\n",
    "Before we create node embeddings, we will run the Leiden Community detection algorithm to get modularity optimized community ids that we will use later as a \"gold standard\" to tune the results of the node embedding clustering. \n",
    "\n",
    "The intuition/idea behind that is that we then get clusters in the vector space (node embeddings) that are close to each other (manhattan distance), when the represented code units are also coupled together. Density based clustering works of course differently and leads to different insights about the structural features of the code units so that it will (and also should) not match the Leiden communities perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516052c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if java_type_node_count > 0:\n",
    "    tuned_leiden_community_detection = get_tuned_leiden_community_detection_algorithm(java_type_projection_parameters)\n",
    "    tuned_leiden_community_detection.write_communities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8acd30",
   "metadata": {},
   "source": [
    "### 2.3 Node Embeddings for Java Types using Fast Random Projection (Fast RP)\n",
    "\n",
    "[Fast Random Projection](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/fastrp) is used to reduce the dimensionality of the node feature space while preserving most of the distance information. Nodes with similar neighborhood result in node embedding with similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Keep solution (either Optuna or classic)\n",
    "if java_type_node_count > 0:\n",
    "    tuned_fast_random_projection = get_tuned_fast_random_projection_node_embeddings(java_type_projection_parameters)\n",
    "    # TODO Write the results back into the Neo4j database\n",
    "    #tuned_fast_random_projection.best_estimator_.write_embeddings()\n",
    "    embeddings = tuned_fast_random_projection.fit().get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "tuneable_fast_random_projection_parameter_grid = {\n",
    "    \"embedding_dimension\": [64, 128, 256],\n",
    "    \"random_seed\": [42], # Fixed random seed since experiments showed only minor differences in the results\n",
    "    \"fast_random_projection_normalization_strength\": [-0.9, -0.5, -0.4, -0.3, -0.2, 0.0, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"fast_random_projection_forth_iteration_weight\": [0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Here we intentionally use the original projection parameters, not the sampled ones,\n",
    "# since the sampling is not necessary for Fast Random Projection embeddings.\n",
    "tuneable_fast_random_projection = create_tuneable(TuneableFastRandomProjectionNodeEmbeddings).with_projection_parameters(java_type_projection_parameters)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameter_tuning = GridSearchCV(\n",
    "    estimator=tuneable_fast_random_projection,\n",
    "    param_grid=tuneable_fast_random_projection_parameter_grid,\n",
    "    cv=get_all_data_without_slicing_cross_validator_for_node_count(java_type_node_count),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if java_type_node_count > 0:\n",
    "    reset_node_embedding_tuning_scores() # Reset the DataFrame to store the results\n",
    "    tuned_fast_random_projection = hyperparameter_tuning.fit(get_initial_dummy_data_for_hyperparameter_tuning(java_type_node_count))\n",
    "    output_tuning_details(tuned_fast_random_projection, 'Tuned Fast Random Projection for Java Types')\n",
    "    output_node_embedding_tuning_scores()\n",
    "\n",
    "    embeddings = tuned_fast_random_projection.best_estimator_.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    # Write the results back into the Neo4j database\n",
    "    tuned_fast_random_projection.best_estimator_.write_embeddings()\n",
    "\n",
    "    node_embeddings_for_visualization = prepare_node_embeddings_for_2d_visualization(embeddings)\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Types positioned by their dependency relationships (Fast Random Projection + t-SNE)\"\n",
    "    )\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Types positioned by their dependency relationships (Fast Random Projection + UMAP)\",\n",
    "        x_position_column='embeddingUMAPVisualizationX',\n",
    "        y_position_column='embeddingUMAPVisualizationY'\n",
    "    )\n",
    "\n",
    "    data_to_write = pd.DataFrame(data = {\n",
    "        'nodeElementId': embeddings[\"nodeElementId\"],\n",
    "        'clusteringHDBSCANLabel': embeddings[get_clustering_property_name('Label')],\n",
    "        'clusteringHDBSCANProbability': embeddings[get_clustering_property_name('Probability')],\n",
    "        'embeddingFastRandomProjectionVisualizationX': embeddings[\"embeddingVisualizationX\"],\n",
    "        'embeddingFastRandomProjectionVisualizationY': embeddings[\"embeddingVisualizationY\"],\n",
    "    })\n",
    "    write_batch_data_into_database(data_to_write, 'Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5c1dd",
   "metadata": {},
   "source": [
    "### 2.4 Node Embeddings for Java Types using HashGNN\n",
    "\n",
    "[HashGNN](https://neo4j.com/docs/graph-data-science/2.6/machine-learning/node-embeddings/hashgnn) resembles Graph Neural Networks (GNN) but does not include a model or require training. It combines ideas of GNNs and fast randomized algorithms. For more details see [HashGNN](https://neo4j.com/docs/graph-data-science/2.6/machine-learning/node-embeddings/hashgnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c3a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Keep one solution (Optuna vs. GridSearch) \n",
    "if java_type_node_count > 0:\n",
    "    tuned_hashgnn = get_tuned_hashgnn_node_embeddings(java_type_sampled_projection_parameters)\n",
    "\n",
    "    if java_type_sampling_result.is_sampled:\n",
    "        tuned_hashgnn.refit_with_projection(java_type_projection_parameters[\"dependencies_projection\"])\n",
    "    else:\n",
    "        tuned_hashgnn.fit()\n",
    "        \n",
    "    embeddings = tuned_hashgnn.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    " \n",
    "    node_embeddings_for_visualization = prepare_node_embeddings_for_2d_visualization(embeddings)\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization,\n",
    "        \"Java Types positioned by their dependency relationships (HashGNN + t-SNE)\"\n",
    "    )\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Types positioned by their dependency relationships (HashGNN + UMAP)\",\n",
    "        x_position_column='embeddingUMAPVisualizationX',\n",
    "        y_position_column='embeddingUMAPVisualizationY'\n",
    "    )\n",
    "\n",
    "# -------\n",
    "\n",
    "tuneable_hashgnn_parameter_grid = {\n",
    "    \"embedding_dimension\": [64, 128, 256],\n",
    "    # \"random_seed\": [42, 2023], # Fixed random seed since experiments showed only minor differences in the results\n",
    "    \"hashgnn_iterations\": [2, 4],\n",
    "    \"hashgnn_density_level\": [1, 2],\n",
    "    \"hashgnn_neighbor_influence\": [0.7, 1.0, 5.0, 10.0], #  [0.1, 0.7, 1.0, 5.0, 10.0],\n",
    "    \"hashgnn_dimension_multiplier\": [1, 2],\n",
    "}\n",
    "\n",
    "tuneable_hashgnn = create_tuneable(TuneableHashGNNNodeEmbeddings).with_projection_parameters(java_type_sampled_projection_parameters)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameter_tuning = GridSearchCV(\n",
    "    estimator=tuneable_hashgnn,\n",
    "    param_grid=tuneable_hashgnn_parameter_grid,\n",
    "    cv=get_all_data_without_slicing_cross_validator_for_node_count(java_type_node_count),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if java_type_node_count > 0:\n",
    "    reset_node_embedding_tuning_scores() # Reset the DataFrame to store the results\n",
    "    tuned_hashgnn = hyperparameter_tuning.fit(get_initial_dummy_data_for_hyperparameter_tuning(java_type_node_count))\n",
    "    output_tuning_details(tuned_hashgnn, 'Tuned HashGNN for Java Types')\n",
    "    output_node_embedding_tuning_scores()\n",
    "\n",
    "    if java_type_sampling_result.is_sampled:\n",
    "        tuned_hashgnn.best_estimator_.refit_with_projection(java_type_projection_parameters[\"dependencies_projection\"])\n",
    "\n",
    "    embeddings = tuned_hashgnn.best_estimator_.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    plot_2d_node_embeddings(\n",
    "        prepare_node_embeddings_for_2d_visualization(embeddings),\n",
    "        \"Java Types positioned by their dependency relationships (HashGNN + t-SNE)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73874003",
   "metadata": {},
   "source": [
    "### 2.5 Node Embeddings for Java Types using node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Keep one solution (Optuna vs. GridSearch) \n",
    "if java_type_node_count > 0:\n",
    "    tuned_node2vec = get_tuned_node2vec_node_embeddings(java_type_sampled_projection_parameters)\n",
    "    \n",
    "    if java_package_sampling_result.is_sampled:\n",
    "        tuned_node2vec.refit_with_projection(java_type_projection_parameters[\"dependencies_projection\"])\n",
    "    else:\n",
    "        tuned_node2vec.fit()\n",
    "    \n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(tuned_node2vec.get_embeddings()).embeddings\n",
    "    display(embeddings.head())\n",
    "    \n",
    "    node_embeddings_for_visualization = prepare_node_embeddings_for_2d_visualization(embeddings)\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization,\n",
    "        \"Java Types positioned by their dependency relationships (node2vec + t-SNE)\"\n",
    "    )\n",
    "    plot_2d_node_embeddings(\n",
    "        node_embeddings_for_visualization, \n",
    "        \"Java Types positioned by their dependency relationships (node2vec + UMAP)\",\n",
    "        x_position_column='embeddingUMAPVisualizationX',\n",
    "        y_position_column='embeddingUMAPVisualizationY'\n",
    "    )\n",
    "# -------\n",
    "tuneable_node2vec_parameter_grid = {\n",
    "    \"embedding_dimension\": [32, 64, 128], # 256 rarely improves the results, but increases the computation time\n",
    "    \"node2vec_in_out_factor\": [0.25, 0.5, 1.0, 2.0], # [0.25, 0.5, 1.0, 2.0, 4.0]\n",
    "    \"node2vec_return_factor\": [0.25, 0.5, 1.0, 2.0, 4.0], # [0.25, 0.5, 1.0, 2.0, 4.0]\n",
    "    # \"node2vec_negative_sampling_rate\": [5, 10],\n",
    "    # \"node2vec_window_size\": [5, 10],\n",
    "    # \"random_seed\": [42], # Fixed random seed since experiments showed only minor differences in the results\n",
    "    # \"node2vec_walk_length\": [80], # [40, 80, 160],\n",
    "    # \"node2vec_walks_per_node\": [10], # [5, 10],\n",
    "    # \"node2vec_iterations\": [1],\n",
    "    # \"node2vec_positive_sampling_factor\": [0.001],\n",
    "}\n",
    "\n",
    "tuneable_node2vec = create_tuneable(TuneableNode2VecNodeEmbeddings).with_projection_parameters(java_type_sampled_projection_parameters)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameter_tuning = GridSearchCV(\n",
    "    estimator=tuneable_node2vec,\n",
    "    param_grid=tuneable_node2vec_parameter_grid,\n",
    "    cv=get_all_data_without_slicing_cross_validator_for_node_count(java_type_node_count),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if java_type_node_count > 0:\n",
    "    reset_node_embedding_tuning_scores()\n",
    "    tuned_node2vec = hyperparameter_tuning.fit(get_initial_dummy_data_for_hyperparameter_tuning(java_type_node_count))\n",
    "    output_tuning_details(tuned_node2vec, 'Tuned node2vec for Java Types')\n",
    "    output_node_embedding_tuning_scores()\n",
    "\n",
    "    if java_type_sampling_result.is_sampled:\n",
    "        tuned_node2vec.best_estimator_.refit_with_projection(java_type_projection_parameters[\"dependencies_projection\"])\n",
    "\n",
    "    embeddings = tuned_node2vec.best_estimator_.get_embeddings()\n",
    "    embeddings = add_tuned_hierarchical_density_based_spatial_clustering(embeddings).embeddings\n",
    "    display(embeddings.head())\n",
    "\n",
    "    plot_2d_node_embeddings(\n",
    "        prepare_node_embeddings_for_2d_visualization(embeddings),\n",
    "        \"Java Types positioned by their dependency relationships (node2vec + t-SNE)\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "JohT"
   }
  ],
  "code_graph_analysis_pipeline_data_validation": "ValidateAlwaysFalse",
  "kernelspec": {
   "display_name": "codegraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "title": "Hyperparameter tuning of Java Node Embeddings"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
